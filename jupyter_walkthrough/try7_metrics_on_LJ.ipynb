{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52849f8a-3f7a-4d29-960e-6ddeeb96ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd823e7d-a3d0-4330-b7f4-dfc50daa2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ff88f2-f025-4852-b305-e868f17c3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a656afc-e101-4cc2-8f25-a70accc729ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2236ed1e-551e-4d48-9833-267e9586b945",
   "metadata": {},
   "source": [
    "# MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39f15be-2f35-4912-acde-028a682e9be7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def antidiag_indices(offset, min_i=0, max_i=None, min_j=0, max_j=None):\n",
    "    \"\"\"\n",
    "    for a (3, 4) matrix with min_i=1, max_i=3, min_j=1, max_j=4, outputs\n",
    "    offset=2 (1, 1),\n",
    "    offset=3 (2, 1), (1, 2)\n",
    "    offset=4 (2, 2), (1, 3)\n",
    "    offset=5 (2, 3)\n",
    "    constraints:\n",
    "        i + j = offset\n",
    "        min_j <= j < max_j\n",
    "        min_i <= offset - j < max_i\n",
    "    \"\"\"\n",
    "    if max_i is None:\n",
    "        max_i = offset + 1\n",
    "    if max_j is None:\n",
    "        max_j = offset + 1\n",
    "    min_j = max(min_j, offset - max_i + 1, 0)\n",
    "    max_j = min(max_j, offset - min_i + 1, offset + 1)\n",
    "    j = torch.arange(min_j, max_j)\n",
    "    i = offset - j\n",
    "    return torch.stack([i, j])\n",
    "\n",
    "def batch_dynamic_time_warping(distance, shapes=None):\n",
    "    \"\"\"full batched DTW without any constraints\n",
    "    distance:  (batchsize, max_M, max_N) matrix\n",
    "    shapes: (batchsize,) vector specifying (M, N) for each entry\n",
    "    \"\"\"\n",
    "    # ptr: 0=left, 1=up-left, 2=up\n",
    "    ptr2dij = {0: (0, -1), 1: (-1, -1), 2: (-1, 0)}\n",
    "\n",
    "    bsz, m, n = distance.size()\n",
    "    cumdist = torch.zeros_like(distance)\n",
    "    backptr = torch.zeros_like(distance).type(torch.int32) - 1\n",
    "\n",
    "    # initialize\n",
    "    cumdist[:, 0, :] = distance[:, 0, :].cumsum(dim=-1)\n",
    "    cumdist[:, :, 0] = distance[:, :, 0].cumsum(dim=-1)\n",
    "    backptr[:, 0, :] = 0\n",
    "    backptr[:, :, 0] = 2\n",
    "\n",
    "    # DP with optimized anti-diagonal parallelization, O(M+N) steps\n",
    "    for offset in range(2, m + n - 1):\n",
    "        ind = antidiag_indices(offset, 1, m, 1, n)\n",
    "        c = torch.stack(\n",
    "            [cumdist[:, ind[0], ind[1] - 1], cumdist[:, ind[0] - 1, ind[1] - 1],\n",
    "             cumdist[:, ind[0] - 1, ind[1]], ],\n",
    "            dim=2\n",
    "        )\n",
    "        v, b = c.min(axis=-1)\n",
    "        backptr[:, ind[0], ind[1]] = b.int()\n",
    "        cumdist[:, ind[0], ind[1]] = v + distance[:, ind[0], ind[1]]\n",
    "\n",
    "    # backtrace\n",
    "    pathmap = torch.zeros_like(backptr)\n",
    "    for b in range(bsz):\n",
    "        i = m - 1 if shapes is None else (shapes[b][0] - 1).item()\n",
    "        j = n - 1 if shapes is None else (shapes[b][1] - 1).item()\n",
    "        dtwpath = [(i, j)]\n",
    "        while (i != 0 or j != 0) and len(dtwpath) < 10000:\n",
    "            assert (i >= 0 and j >= 0)\n",
    "            di, dj = ptr2dij[backptr[b, i, j].item()]\n",
    "            i, j = i + di, j + dj\n",
    "            dtwpath.append((i, j))\n",
    "        dtwpath = dtwpath[::-1]\n",
    "        indices = torch.from_numpy(np.array(dtwpath))\n",
    "        pathmap[b, indices[:, 0], indices[:, 1]] = 1\n",
    "\n",
    "    return cumdist, backptr, pathmap\n",
    "\n",
    "\n",
    "def compute_l2_dist(x1, x2):\n",
    "    \"\"\"compute an (m, n) L2 distance matrix from (m, d) and (n, d) matrices\"\"\"\n",
    "    return torch.cdist(x1.unsqueeze(0), x2.unsqueeze(0), p=2).squeeze(0).pow(2)\n",
    "\n",
    "\n",
    "def compute_rms_dist(x1, x2):\n",
    "    l2_dist = compute_l2_dist(x1, x2)\n",
    "    return (l2_dist / x1.size(1)).pow(0.5)\n",
    "\n",
    "\n",
    "def get_divisor(pathmap, normalize_type):\n",
    "    if normalize_type is None:\n",
    "        return 1\n",
    "    elif normalize_type == \"len1\":\n",
    "        return pathmap.size(0)\n",
    "    elif normalize_type == \"len2\":\n",
    "        return pathmap.size(1)\n",
    "    elif normalize_type == \"path\":\n",
    "        return pathmap.sum().item()\n",
    "    else:\n",
    "        raise ValueError(f\"normalize_type {normalize_type} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3631599-d8f3-4e24-b402-b85aa1146b04",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def batch_mel_cepstral_distortion(\n",
    "        y1, y2, sr, normalize_type=\"path\", mfcc_fn=None\n",
    "):\n",
    "    # https://huggingface.co/spaces/OFA-Sys/OFA-Generic_Interface/blob/main/fairseq/fairseq/tasks/text_to_speech.py\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/2011.03568.pdf\n",
    "    The root mean squared error computed on 13-dimensional MFCC using DTW for\n",
    "    alignment. MFCC features are computed from an 80-channel log-mel\n",
    "    spectrogram using a 50ms Hann window and hop of 12.5ms.\n",
    "    y1: list of waveforms\n",
    "    y2: list of waveforms\n",
    "    sr: sampling rate\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        import torchaudio\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install torchaudio: pip install torchaudio\")\n",
    "\n",
    "    if mfcc_fn is None or mfcc_fn.sample_rate != sr:\n",
    "        melkwargs = {\n",
    "            \"n_fft\": int(0.05 * sr), \"win_length\": int(0.05 * sr),\n",
    "            \"hop_length\": int(0.0125 * sr), \"f_min\": 20,\n",
    "            \"n_mels\": 80, \"window_fn\": torch.hann_window\n",
    "        }\n",
    "        mfcc_fn = torchaudio.transforms.MFCC(\n",
    "            sr, n_mfcc=13, log_mels=True, melkwargs=melkwargs\n",
    "        ).to(y1[0].device)\n",
    "    return batch_compute_distortion(\n",
    "        y1, y2, sr, lambda y: mfcc_fn(y).transpose(-1, -2), compute_rms_dist,\n",
    "        normalize_type\n",
    "    )\n",
    "\n",
    "    \n",
    "def batch_compute_distortion(y1, y2, sr, feat_fn, dist_fn, normalize_type):\n",
    "    d, s, x1, x2 = [], [], [], []\n",
    "    for cur_y1, cur_y2 in zip(y1, y2):\n",
    "        assert (cur_y1.ndim == 1 and cur_y2.ndim == 1)\n",
    "        cur_x1 = feat_fn(cur_y1)\n",
    "        cur_x2 = feat_fn(cur_y2)\n",
    "        x1.append(cur_x1)\n",
    "        x2.append(cur_x2)\n",
    "\n",
    "        cur_d = dist_fn(cur_x1, cur_x2)\n",
    "        d.append(cur_d)\n",
    "        s.append(d[-1].size())\n",
    "    max_m = max(ss[0] for ss in s)\n",
    "    max_n = max(ss[1] for ss in s)\n",
    "    d = torch.stack(\n",
    "        [F.pad(dd, (0, max_n - dd.size(1), 0, max_m - dd.size(0))) for dd in d]\n",
    "    )\n",
    "    s = torch.LongTensor(s).to(d.device)\n",
    "    cumdists, backptrs, pathmaps = batch_dynamic_time_warping(d, s)\n",
    "\n",
    "    rets = []\n",
    "    itr = zip(s, x1, x2, d, cumdists, backptrs, pathmaps)\n",
    "    for (m, n), cur_x1, cur_x2, dist, cumdist, backptr, pathmap in itr:\n",
    "        cumdist = cumdist[:m, :n]\n",
    "        backptr = backptr[:m, :n]\n",
    "        pathmap = pathmap[:m, :n]\n",
    "        divisor = get_divisor(pathmap, normalize_type)\n",
    "\n",
    "        distortion = cumdist[-1, -1] / divisor\n",
    "        ret = distortion, (cur_x1, cur_x2, dist, cumdist, backptr, pathmap)\n",
    "        rets.append(ret)\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d1252db-2fa4-4b06-84b5-a0b30f6d33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\LJSpeech\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a97c40-8d30-413f-ace1-9d1f6a4738b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7aeff694-5759-4b95-9679-5320ae0d1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_folder = r'.\\output\\0629lj\\result\\LJSpeech\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0629lj\\result\\LJSpeech\\wav\\synthesized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d935259-5f7b-4656-bc23-f5e986ffd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "MCDs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8eeadfd-a833-405f-8b41-1bad462378ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75865d84d8e241c4b27113bef64a7296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(ref_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    results = batch_mel_cepstral_distortion(\n",
    "            [wav_1],\n",
    "            [wav_2],\n",
    "            sr=sr,\n",
    "    )\n",
    "    MCDs.append(results[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69e90771-b1fb-49ce-9fdf-c6f063706296",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD': MCDs,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f43e17c8-dc02-4831-ad5a-360955066471",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_LJ_recon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ab287a6-2d08-41e7-ae79-4695cb90950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD mean on LJSpeech: 1.684244155883789\n",
      "MCD std on LJSpeech: 0.253206342458725\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCD mean on LJSpeech: {torch.tensor(MCDs).mean()}\")\n",
    "print(f\"MCD std on LJSpeech: {torch.tensor(MCDs).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee34f73-31ee-420e-8fa5-2821b481f3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6692cb30-a937-4b47-8645-b6431cf9e5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c624fa0d-6dd1-46c8-afb2-a1af5190df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyworld as pw\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09d613-7fb6-48bf-aa76-dcfcf8b101b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564d3b0-56af-47e8-85f7-73c80deaa7bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def world_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    f0min: int = 40,\n",
    "    f0max: int = 800,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 13,\n",
    "    mcep_alpha: float = 0.41,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract World-based acoustic features.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Minimum f0 value (default=40).\n",
    "        f0 (int): Maximum f0 value (default=800).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "        ndarray: F0 sequence (N,).\n",
    "\n",
    "    \"\"\"\n",
    "    # extract features\n",
    "    x = x.astype(np.float64)\n",
    "    # f0, time_axis = pw.harvest(\n",
    "    #     x,\n",
    "    #     fs,\n",
    "    #     f0_floor=f0min,\n",
    "    #     f0_ceil=f0max,\n",
    "    #     frame_period=n_shift / fs * 1000,\n",
    "    # )\n",
    "    pitch, t = pw.dio(\n",
    "        x,\n",
    "        fs,\n",
    "         frame_period=n_shift / fs * 1000,\n",
    "        )\n",
    "    sp = pw.cheaptrick(x, f0, time_axis, fs, fft_size=n_fft)\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "    mcep = pysptk.sp2mc(sp, 13, mcep_alpha)\n",
    "\n",
    "    return mcep, f0\n",
    "\n",
    "\n",
    "def _get_basename(path: str) -> str:\n",
    "    return os.path.splitext(os.path.split(path)[-1])[0]\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b90cbf4-8369-4d90-8eb6-f0449c732588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d50bfc57-09d5-4417-a68e-d223dd7d3a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f26a7e9-d8ad-4cf3-ae9d-851bab7ee952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import fnmatch\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pysptk\n",
    "import pyworld as pw\n",
    "import soundfile as sf\n",
    "from fastdtw import fastdtw\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be56e85-8515-42c9-a69e-cf3071621ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_folder = r'.\\output\\0629lj\\result\\LJSpeech\\mel\\gt'\n",
    "# syn_folder = r'.\\output\\0629lj\\result\\LJSpeech\\mel\\syn'\n",
    "ref_folder = r'.\\output\\0629lj\\result\\LJSpeech\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0629lj\\result\\LJSpeech\\wav\\synthesized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f7e1fd-a3fa-464d-9e72-0e71914f4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\LJSpeech\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e93180-c058-43b0-a25c-47a2c566f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 691) (80, 691)\n"
     ]
    }
   ],
   "source": [
    "uid = val_uids[0]\n",
    "# gt_mcep = torch.load(op.join(ref_folder, f\"{uid}.pt\")).numpy()\n",
    "# gen_mcep = torch.load(op.join(syn_folder, f\"{uid}.pt\")).numpy()\n",
    "# print(gt_mcep.shape, gen_mcep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dabe88ab-89ad-4368-9c1f-7bb1b23d392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "melkwargs = {\n",
    "    \"n_fft\": int(0.05 * sr), \"win_length\": int(0.05 * sr),\n",
    "    \"hop_length\": int(0.0125 * sr), \"f_min\": 20,\n",
    "    \"n_mels\": 80, \"window_fn\": torch.hann_window\n",
    "}\n",
    "mfcc_fn = torchaudio.transforms.MFCC(\n",
    "    sr, n_mfcc=13, log_mels=True, melkwargs=melkwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb59f740-f310-48ff-bf13-27248cac26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_1, sr = torchaudio.load(op.join(ref_folder, f\"{uid}.wav\"))\n",
    "wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "wav_1 = wav_1.squeeze()\n",
    "wav_2 = wav_2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08086678-f9a8-4a70-bd55-32b644eb2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "mel_2 = mfcc_fn(wav_2).T.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8295255-6559-4089-9c9c-7c167439e8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((644, 13), (644, 13))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_1.shape, mel_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cef7f51c-dad0-4c1b-a7c6-98efd7d32414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTW\n",
    "_, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "twf = np.array(path).T\n",
    "mel_2 = mel_2[twf[0]]\n",
    "mel_1 = mel_1[twf[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42cc2e98-11bc-41bc-952b-2d3a4d9eec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "mel_1 = mel_1[:, 1:]\n",
    "mel_2 = mel_2[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7b9e580-c945-4006-8d79-65114576afef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(656, 12)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d282b318-4d5e-417e-8019-76ce68778f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2226214"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e5d678f-45af-48b4-9ce7-d56d552180d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "MCDs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e2c8b64-045d-4d0e-8409-2aac73ed618a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdb7e28c9c445b3a403fb525211fca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(ref_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6545f4bf-13e0-45c5-82f2-6155f5054821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lj_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD': MCDs,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b3d2eb7-69be-497e-abe1-2d4ea99da360",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_LJ.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fed4190-b544-47f9-a517-4904597a1945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD mean on LJSpeech: 5.480271339416504\n",
      "MCD std on LJSpeech: 0.7322819232940674\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCD mean on LJSpeech: {torch.tensor(MCDs).mean()}\")\n",
    "print(f\"MCD std on LJSpeech: {torch.tensor(MCDs).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675a4a3-6720-4c78-804a-e54a6dd83c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa0f15-6f57-4685-a2c2-fe12504aee92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b91b64a-1af4-4852-9ee9-8c9e65895f25",
   "metadata": {},
   "source": [
    "# log f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eed6885b-dca3-4160-b9c4-13e5b9b1b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/espnet/espnet/blob/3e0dad524d62ccd45e067e9b36049f2214ea972a/egs2/TEMPLATE/asr1/pyscripts/utils/evaluate_f0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a063634f-0e88-4215-a8c4-a52d652faf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    f0min: int = 40,\n",
    "    f0max: int = 800,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract World-based acoustic features.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Minimum f0 value (default=40).\n",
    "        f0 (int): Maximum f0 value (default=800).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "        ndarray: F0 sequence (N,).\n",
    "\n",
    "    \"\"\"\n",
    "    # extract features\n",
    "    x = x.astype(np.float64)\n",
    "    f0, time_axis = pw.harvest(\n",
    "        x,\n",
    "        fs,\n",
    "        f0_floor=f0min,\n",
    "        f0_ceil=f0max,\n",
    "        frame_period=n_shift / fs * 1000,\n",
    "    )\n",
    "    sp = pw.cheaptrick(x, f0, time_axis, fs, fft_size=n_fft)\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "    mcep = pysptk.sp2mc(sp, mcep_dim, mcep_alpha)\n",
    "\n",
    "    return mcep, f0\n",
    "\n",
    "\n",
    "def _get_basename(path: str) -> str:\n",
    "    return os.path.splitext(os.path.split(path)[-1])[0]\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5124ff9-e3ac-4fae-b17d-4eca2e57f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "logf0_rmses = []\n",
    "logf0_corrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89e67d34-8556-48b3-9731-aa482b5b4ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb626924f1a4ee1af72792cdc587d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(ref_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    fs = gen_fs\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep, gen_f0 = world_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep, gt_f0 = world_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_f0_dtw = gen_f0[twf[0]]\n",
    "    gt_f0_dtw = gt_f0[twf[1]]\n",
    "    \n",
    "    # Get voiced part\n",
    "    nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "    gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs])\n",
    "    gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs])\n",
    "\n",
    "    # log F0 RMSE\n",
    "    log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "    # print(f\"{uid} {log_f0_rmse:.4f}\")\n",
    "\n",
    "    # log F0 corr\n",
    "    log_f0_corr = np.corrcoef(gen_f0_dtw_voiced, gt_f0_dtw_voiced)[0][1]\n",
    "    # print(f\"{uid} {log_f0_corr:.4f}\")\n",
    "\n",
    "    logf0_rmses.append(log_f0_rmse)\n",
    "    logf0_corrs.append(log_f0_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b49241c0-5cd4-4005-84e1-e7363f3de004",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_logf0_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'logf0_rmse': logf0_rmses,\n",
    "    'logf0_corr': logf0_corrs,\n",
    "})\n",
    "lj_logf0_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\logF0_LJ.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "847a54ef-28bb-4bfe-bda9-b18ff2defca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logf0_rmse mean on LJSpeech: 0.2128476233348171\n",
      "logf0_rmse std on LJSpeech: 0.07755068307229493\n",
      "logf0_corr mean on LJSpeech: 0.7322283435658924\n",
      "logf0_corr std on LJSpeech: 0.16015679184393214\n"
     ]
    }
   ],
   "source": [
    "print(f\"logf0_rmse mean on LJSpeech: {torch.tensor(logf0_rmses).mean()}\")\n",
    "print(f\"logf0_rmse std on LJSpeech: {torch.tensor(logf0_rmses).std()}\")\n",
    "\n",
    "print(f\"logf0_corr mean on LJSpeech: {torch.tensor(logf0_corrs).mean()}\")\n",
    "print(f\"logf0_corr std on LJSpeech: {torch.tensor(logf0_corrs).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e62f3-d4fe-4473-8846-f914fa86edbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ba29e-4894-4d09-b625-aaadee92c129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cc9c5-5fbd-4124-b04a-d8fdcec10283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c1628-1d09-4348-9176-ab0992c2e8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a532e80-b88b-4a49-9ce6-5694c969cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(\n",
    "    file_list: List[str],\n",
    "    gt_file_list: List[str],\n",
    "    args: argparse.Namespace,\n",
    "    f0_rmse_dict: Dict[str, float],\n",
    "):\n",
    "    \"\"\"Calculate log-F0 RMSE.\"\"\"\n",
    "    for i, gen_path in enumerate(file_list):\n",
    "        corresponding_list = list(\n",
    "            filter(lambda gt_path: _get_basename(gt_path) in gen_path, gt_file_list)\n",
    "        )\n",
    "        assert len(corresponding_list) == 1\n",
    "        gt_path = corresponding_list[0]\n",
    "        gt_basename = _get_basename(gt_path)\n",
    "\n",
    "        # load wav file as int16\n",
    "        gen_x, gen_fs = sf.read(gen_path, dtype=\"int16\")\n",
    "        gt_x, gt_fs = sf.read(gt_path, dtype=\"int16\")\n",
    "\n",
    "        fs = gen_fs\n",
    "        if gen_fs != gt_fs:\n",
    "            gt_x = librosa.resample(gt_x.astype(np.float), gt_fs, gen_fs)\n",
    "\n",
    "        # extract ground truth and converted features\n",
    "        gen_mcep, gen_f0 = world_extract(\n",
    "            x=gen_x,\n",
    "            fs=fs,\n",
    "            f0min=args.f0min,\n",
    "            f0max=args.f0max,\n",
    "            n_fft=args.n_fft,\n",
    "            n_shift=args.n_shift,\n",
    "            mcep_dim=args.mcep_dim,\n",
    "            mcep_alpha=args.mcep_alpha,\n",
    "        )\n",
    "        gt_mcep, gt_f0 = world_extract(\n",
    "            x=gt_x,\n",
    "            fs=fs,\n",
    "            f0min=args.f0min,\n",
    "            f0max=args.f0max,\n",
    "            n_fft=args.n_fft,\n",
    "            n_shift=args.n_shift,\n",
    "            mcep_dim=args.mcep_dim,\n",
    "            mcep_alpha=args.mcep_alpha,\n",
    "        )\n",
    "\n",
    "        # DTW\n",
    "        _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "        twf = np.array(path).T\n",
    "        gen_f0_dtw = gen_f0[twf[0]]\n",
    "        gt_f0_dtw = gt_f0[twf[1]]\n",
    "\n",
    "        # Get voiced part\n",
    "        nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "        gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs])\n",
    "        gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs])\n",
    "\n",
    "        # log F0 RMSE\n",
    "        log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "        logging.info(f\"{gt_basename} {log_f0_rmse:.4f}\")\n",
    "        f0_rmse_dict[gt_basename] = log_f0_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8b953-8353-4b5a-a6ca-4e9d50aa0004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58418a9-6f71-4a04-9386-ff73e4812994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd601e1-9997-4bf8-a7d2-35bbccd2bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "            # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(gen_path, dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(gt_path, dtype=\"int16\")\n",
    "    wav_1, sr = torchaudio.load(op.join(ref_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b5e72-1c05-4ffe-badc-4fa6d976cd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383162e8-739d-4089-a61b-2d8c9275b3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f3d9e-3ad3-4450-84eb-792d63fd2234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e73c4-2f9b-44a8-b99a-7e4b858ec577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58258d37-f659-4919-876e-903928d47f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00f2f7-6acd-4782-8194-65ab1cb7c8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306d670-5663-4f67-82b0-5708cbe5e3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2954dc-d7ac-4221-9bf5-013291175a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c97c23-dfda-4090-b689-978223e4746f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce66c2-cc79-4a30-9d5e-f758233627d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eece796-b1bb-43f2-9141-cad78f5daa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c6144-80e8-48ad-87dc-7c3f3d6c29ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609899a-9cf6-4b2a-a22d-a0171bce7d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edeb8af-b998-4c4e-95e6-48a3a646ab6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa4091-753b-496b-a79b-50c9af5a245e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ca22a-9e89-4b2e-8e72-2dfb95e4ac0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
