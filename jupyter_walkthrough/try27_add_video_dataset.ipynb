{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2b6d9d-a422-4a92-afdb-acc69cc536ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf3640b-af8b-45bf-b7d8-6c5748a7494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d569299-475b-46f6-a674-6a0508a13e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils.model import get_model, get_vocoder, get_param_num, vocoder_infer\n",
    "from utils.tools import to_device, log, synth_one_sample, expand, plot_mel\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset, VideoDataset\n",
    "# from utils.auto_tqdm import tqdm\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717a3fa2-b24b-4f81-a898-bc1d2f020ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor.preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42608486-13ca-454a-8115-2e8be2b62b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e632ec81-f499-42e5-8485-20c98cfdec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/Ego4D_final_v6/0719a_preprocess.yaml -m ./config/Ego4D_final_v6/0719a_model.yaml -t ./config/Ego4D_final_v6/0719a_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3e1f494-d535-4906-865c-4569110a9d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(restore_step=0, preprocess_config='./config/Ego4D_final_v6/0719a_preprocess.yaml', model_config='./config/Ego4D_final_v6/0719a_model.yaml', train_config='./config/Ego4D_final_v6/0719a_train.yaml')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef8b47ee-32ed-4e2b-bef8-98389c0d6799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video count: 194\n"
     ]
    }
   ],
   "source": [
    "config = yaml.load(open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader)\n",
    "preprocessor = Preprocessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6769362b-e07b-48e4-bc6f-08c29066099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor.build_from_trainval_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efb6a1b-1917-4659-8511-828cc8ce7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e14fc26c-59db-45b9-a6b8-9e7d5f996797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs\n",
    "\n",
    "# Get dataset\n",
    "dataset = VideoDataset(\n",
    "    \"train.txt\", 'train', preprocess_config, train_config, sort=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a1a2f40-25f7-4b46-be01-08283b4d4218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27292"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4026cfa-eca2-46e6-ad2c-fd39eb9e2fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60a3839b-63be-4459-837b-c5de1a0ce820",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "group_size = 4  # Set this larger than 1 to enable sorting in Dataset\n",
    "assert batch_size * group_size < len(dataset)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size * group_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ecf3e49-00d1-44a8-8df1-4de251c63d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return (\n",
    "#     ids,\n",
    "#     raw_texts,\n",
    "#     speakers,\n",
    "#     texts,\n",
    "#     text_lens,\n",
    "#     max(text_lens),\n",
    "#     mels,\n",
    "#     mel_lens,\n",
    "#     max(mel_lens),\n",
    "#     pitches,\n",
    "#     energies,\n",
    "#     durations,\n",
    "#     speaker_embeddings,\n",
    "#     video_embeddings,\n",
    "#     vid_lens,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33bacfe7-92bf-4bf4-8fe6-d32095818abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# for data in loader:\n",
    "#     i += 1\n",
    "#     print(len(data))\n",
    "#     if i>= 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e3795d-07af-46a3-897f-a2a4d975a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in data:\n",
    "#     print(item[10].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfc2a429-e1b3-4acf-a706-39286a8758df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using speaker embeddings.\n",
      "=> Using VarianceAdaptorWithSpeaker.\n",
      "Successfully loaded from ./output/LibriTTS/LibriTTS_800000.pth.tar\n",
      "Number of FastSpeech2 Parameters: 35165553\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)\n",
    "\n",
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16849e4c-ebf2-49d8-ae0a-fbd3478af6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "step = args.restore_step + 1\n",
    "epoch = 1\n",
    "grad_acc_step = train_config[\"optimizer\"][\"grad_acc_step\"]\n",
    "grad_clip_thresh = train_config[\"optimizer\"][\"grad_clip_thresh\"]\n",
    "total_step = train_config[\"step\"][\"total_step\"]\n",
    "log_step = train_config[\"step\"][\"log_step\"]\n",
    "save_step = train_config[\"step\"][\"save_step\"]\n",
    "synth_step = train_config[\"step\"][\"synth_step\"]\n",
    "val_step = train_config[\"step\"][\"val_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6dd53-e0f2-497c-bee4-e104dfee4451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec3b46b8-6baf-4227-89dc-01a212454dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel_lens:  tensor([1099,  618,  474,  412,  305,  778,  302,  300,  325,  299,  246,  378,\n",
      "         319,  206,  184,  301,  138,  105,  259,  117,  212,  156,  226,  239],\n",
      "       device='cuda:0', dtype=torch.int32)\n",
      "mel_masks.shape:  torch.Size([24, 1099])\n",
      "mel_masks.dtype:  torch.bool\n",
      "texts.shape:  torch.Size([24, 134])\n",
      "=> Retrieved text embeddings shape:  torch.Size([24, 134, 256])\n",
      "=> Mask shape:  torch.Size([24, 134])\n",
      "attn output shape: torch.Size([24, 134, 256])\n",
      "1 torch.Size([24, 134, 256])\n",
      "cnn output shape: torch.Size([24, 134, 256])\n",
      "attn output shape: torch.Size([24, 134, 256])\n",
      "1 torch.Size([24, 134, 256])\n",
      "cnn output shape: torch.Size([24, 134, 256])\n",
      "attn output shape: torch.Size([24, 134, 256])\n",
      "1 torch.Size([24, 134, 256])\n",
      "cnn output shape: torch.Size([24, 134, 256])\n",
      "attn output shape: torch.Size([24, 134, 256])\n",
      "1 torch.Size([24, 134, 256])\n",
      "cnn output shape: torch.Size([24, 134, 256])\n",
      "attn output shape: torch.Size([24, 1000, 256])\n",
      "1 torch.Size([24, 1000, 256])\n",
      "cnn output shape: torch.Size([24, 1000, 256])\n",
      "attn output shape: torch.Size([24, 1000, 256])\n",
      "1 torch.Size([24, 1000, 256])\n",
      "cnn output shape: torch.Size([24, 1000, 256])\n",
      "attn output shape: torch.Size([24, 1000, 256])\n",
      "1 torch.Size([24, 1000, 256])\n",
      "cnn output shape: torch.Size([24, 1000, 256])\n",
      "attn output shape: torch.Size([24, 1000, 256])\n",
      "1 torch.Size([24, 1000, 256])\n",
      "cnn output shape: torch.Size([24, 1000, 256])\n",
      "attn output shape: torch.Size([24, 1000, 256])\n",
      "1 torch.Size([24, 1000, 256])\n",
      "cnn output shape: torch.Size([24, 1000, 256])\n",
      "attn output shape: torch.Size([24, 1000, 256])\n",
      "1 torch.Size([24, 1000, 256])\n",
      "cnn output shape: torch.Size([24, 1000, 256])\n"
     ]
    }
   ],
   "source": [
    "for batchs in loader:\n",
    "    for batch in batchs:\n",
    "        batch = to_device(batch, device)\n",
    "        temp_batch = (batch[2:])\n",
    "        output = model(*(batch[2:]))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af8d98-00ea-42cf-958b-78925ecee8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393cb1f-4e87-47fb-a62c-acb68c812e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0799a-fd93-4eaf-a24e-2827ba55cf11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060ea87-3e3e-4089-bf9f-7fa703d30102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb6c28-b513-408c-89cc-502fdd12a7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbeb1c5-2b22-430d-9560-4a175b704607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05eeca3-1763-403d-b4f6-32a35cee024a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
