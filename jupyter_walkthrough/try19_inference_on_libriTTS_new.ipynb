{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d108efa7-b89e-48ce-96c4-fffa0b09f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "031ecbb7-24f6-44fc-8597-e9b7e4893f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f0c7bb-69cd-486c-b773-8a1dad061b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\LibriTTS\\val_ming.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bff5764-f134-498a-bff2-2b21dcf2cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(val_uids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96649929-694e-41da-897b-8f6885ada414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1d622e-5cd9-4e71-86ae-8f2d75152046",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_duration_folder = './preprocessed_data/LibriTTS/duration/val'\n",
    "val_energy_folder = './preprocessed_data/LibriTTS/energy/val'\n",
    "val_mel_folder = './preprocessed_data/LibriTTS/mel/val'\n",
    "val_pitch_folder = './preprocessed_data/LibriTTS/pitch/val'\n",
    "\n",
    "os.makedirs(val_duration_folder, exist_ok=True)\n",
    "os.makedirs(val_energy_folder, exist_ok=True)\n",
    "os.makedirs(val_mel_folder, exist_ok=True)\n",
    "os.makedirs(val_pitch_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e78044b-a820-4fd4-b798-0c92b19a7b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f24066646b44c3a83072827c3e0917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417052be68d84fc4a47dc0b1aa898973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd2bae3bcdc42e081d8a2d7d757582a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1441f2a2f40b4efaac80b60749042e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for uid in tqdm(val_uids):\n",
    "    speaker_id = uid.split('_')[0]\n",
    "    src = rf\".\\preprocessed_data\\LibriTTS\\duration\\{speaker_id}-duration-{uid}.npy\"\n",
    "    dst = f\"{val_duration_folder}\"\n",
    "    shutil.copy(src, dst)\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    speaker_id = uid.split('_')[0]\n",
    "    src = rf\".\\preprocessed_data\\LibriTTS\\energy\\{speaker_id}-energy-{uid}.npy\"\n",
    "    dst = f\"{val_energy_folder}\"\n",
    "    shutil.copy(src, dst)\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    speaker_id = uid.split('_')[0]\n",
    "    src = rf\".\\preprocessed_data\\LibriTTS\\mel\\{speaker_id}-mel-{uid}.npy\"\n",
    "    dst = f\"{val_mel_folder}\"\n",
    "    shutil.copy(src, dst)\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    speaker_id = uid.split('_')[0]\n",
    "    src = rf\".\\preprocessed_data\\LibriTTS\\pitch\\{speaker_id}-pitch-{uid}.npy\"\n",
    "    dst = f\"{val_pitch_folder}\"\n",
    "    shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57f76f-e1b8-4abd-a84c-db3a84d5c3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c251ba24-d8c5-4705-adbe-c7022e497527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33c00c-515b-4aa8-afe6-b1328ac61d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efb524-1fc2-452a-9c02-3d2d3b0d36da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a093d-fa94-430b-80bd-5f4ddd7f1e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad15148-eb36-4d4e-ab44-17a802919c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model, get_vocoder, get_param_num, vocoder_infer\n",
    "from utils.tools import to_device, log, synth_one_sample, expand, plot_mel\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset\n",
    "# from utils.auto_tqdm import tqdm\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc3c529-80f7-4e34-b976-21c1d93e33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "545b3063-4478-42c3-be88-39fbb783c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/LibriTTS/0714lb_preprocess.yaml -m ./config/LibriTTS/0714lb_model.yaml -t ./config/LibriTTS/0714lb_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ebbb43-143c-4a27-b813-218be8926818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(restore_step=0, preprocess_config='./config/LibriTTS/0714lb_preprocess.yaml', model_config='./config/LibriTTS/0714lb_model.yaml', train_config='./config/LibriTTS/0714lb_train.yaml')\n",
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "pprint(args)\n",
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)\n",
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5b50fe1-d92d-4c33-8309-00ccb553b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/LibriTTS/LibriTTS_800000.pth.tar'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c46ac996-d739-4c0b-9d4d-ad4450254965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Not using speaker embeddings.\n",
      "False\n",
      "None\n",
      "Number of FastSpeech2 Parameters: 35391169\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c89fbf98-3f36-4ec9-ab4d-fb47e9b76197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)\n",
    "step = args.restore_step + 1\n",
    "model.eval()\n",
    "print()\n",
    "dataset = Dataset(\n",
    "    \"val_ming.txt\", 'val', preprocess_config, train_config, sort=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48011986-8d5c-4ecd-b3bd-2b98c9bcd168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d77552-26b4-4df4-a012-46d60457b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea11159-4811-46ec-8407-3db28dd7f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "# Get loss function\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d5ea10e-a0b8-4223-8a6c-ffeb0480a4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/0714lb/result/LibriTTS\\\\plots'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(train_config['path']['result_path'], 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1c3897b-6a98-468c-b72f-9c9d3c23579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_plot_path = os.path.join(train_config['path']['result_path'], 'plot')\n",
    "output_mel_syn_path = os.path.join(train_config['path']['result_path'], 'mel', 'syn')\n",
    "output_mel_gt_path = os.path.join(train_config['path']['result_path'], 'mel', 'gt')\n",
    "output_wav_syn_path = os.path.join(train_config['path']['result_path'], 'wav', 'synthesized')\n",
    "output_wav_rec_path = os.path.join(train_config['path']['result_path'], 'wav', 'reconstructed')\n",
    "os.makedirs(output_plot_path, exist_ok=True)\n",
    "os.makedirs(output_mel_syn_path, exist_ok=True)\n",
    "os.makedirs(output_mel_gt_path, exist_ok=True)\n",
    "os.makedirs(output_wav_syn_path, exist_ok=True)\n",
    "os.makedirs(output_wav_rec_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0141e0c5-53be-4ce7-a83a-ec071b5e366b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77e57cdc6444579880d8d86c4b9f15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batchs in tqdm(loader):\n",
    "    for targets in batchs:\n",
    "        targets = to_device(targets, device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(*(targets[2:]))\n",
    "        basenames = targets[0]\n",
    "        for i in range(len(predictions[0])):\n",
    "            basename = basenames[i]\n",
    "            src_len = predictions[8][i].item()\n",
    "            mel_len = predictions[9][i].item()\n",
    "            mel_prediction = predictions[1][i, :mel_len].detach().transpose(0, 1)\n",
    "            mel_target = targets[6][i, :mel_len].detach().transpose(0, 1)\n",
    "\n",
    "            torch.save(mel_prediction.cpu(), os.path.join(output_mel_syn_path, f\"{basename}.pt\"))\n",
    "            torch.save(mel_target.cpu(), os.path.join(output_mel_gt_path, f\"{basename}.pt\"))\n",
    "            \n",
    "            \n",
    "            duration = predictions[5][i, :src_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n",
    "                pitch = predictions[2][i, :src_len].detach().cpu().numpy()\n",
    "                pitch = expand(pitch, duration)\n",
    "            else:\n",
    "                pitch = predictions[2][i, :mel_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"] == \"phoneme_level\":\n",
    "                energy = predictions[3][i, :src_len].detach().cpu().numpy()\n",
    "                energy = expand(energy, duration)\n",
    "            else:\n",
    "                energy = predictions[3][i, :mel_len].detach().cpu().numpy()\n",
    "\n",
    "            with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \n",
    "                                   \"stats.json\")) as f:\n",
    "                stats = json.load(f)\n",
    "                stats = stats[\"pitch\"] + stats[\"energy\"][:2]\n",
    "                                       \n",
    "            fig = plot_mel(\n",
    "                [\n",
    "                    (mel_prediction.cpu().numpy(), pitch, energy),\n",
    "                    (mel_target.cpu().numpy(), pitch, energy),\n",
    "                ],\n",
    "                stats,\n",
    "                [\"Synthetized Spectrogram\", \"Ground-Truth Spectrogram\"],\n",
    "            )\n",
    "            ### TODO: change to svg\n",
    "            plt.savefig(os.path.join(output_plot_path, f\"{basename}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # from .model import vocoder_infer\n",
    "\n",
    "        mel_predictions = predictions[1].transpose(1, 2)\n",
    "        mel_targets = targets[6].transpose(1, 2)\n",
    "        \n",
    "        lengths = predictions[9] * preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "        wav_predictions = vocoder_infer(\n",
    "            mel_predictions, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "        )\n",
    "        wav_targets = vocoder_infer(\n",
    "        mel_targets, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "    )\n",
    "    \n",
    "        sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "        for wav, basename in zip(wav_predictions, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_syn_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "        for wav, basename in zip(wav_targets, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_rec_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "\n",
    "        \n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59445b0-bbdf-4525-b52d-744eaf9e2f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce20e7-0d6b-403d-9711-225074a9c643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bb314-0cea-4eca-95a0-cd6d5e88738b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c5a68-a92c-47a2-9497-1efced3a001a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d094b-7b0f-43e3-8481-a49187bb1fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ce17e-20bf-4398-87ac-d24d198f9897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f80651b8-c66b-4cdf-8f55-f75df5908e42",
   "metadata": {},
   "source": [
    "# MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b1b3d5-13a0-4ae1-9e4f-6a7029beed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import os.path as op\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import argparse\n",
    "import fnmatch\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import librosa\n",
    "import pysptk\n",
    "import pyworld as pw\n",
    "import soundfile as sf\n",
    "from fastdtw import fastdtw\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8dcab65-9532-4b9b-898e-53f5b3a62ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\LibriTTS\\val_ming.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66757f3c-3452-42ad-8966-18e9b565d23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a45c8559-f619-4a27-b225-6bd89e0b9cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5400_3587_000037_000001'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_uids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e302d58-fac0-472e-9cf7-3af700ff6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_folder = r'.\\output\\0714lb\\result\\LibriTTS\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0714lb\\result\\LibriTTS\\wav\\synthesized'\n",
    "gt_folder = r'.\\Data\\LibriTTS_val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9346f1e-f0fc-4597-8c10-7b126bec21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = 22050\n",
    "# melkwargs = {\n",
    "#     \"n_fft\": int(0.05 * sr), \"win_length\": int(0.05 * sr),\n",
    "#     \"hop_length\": int(0.0125 * sr), \"f_min\": 20,\n",
    "#     \"n_mels\": 80, \"window_fn\": torch.hann_window\n",
    "# }\n",
    "# mfcc_fn = torchaudio.transforms.MFCC(\n",
    "#     sr, n_mfcc=13, log_mels=True, melkwargs=melkwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ede7f6-8b99-48cd-82e5-f3099594757a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# uids = []\n",
    "# MCDs_recon = []\n",
    "# for uid in tqdm(val_uids):\n",
    "#     uids.append(uid)\n",
    "#     wav_1, sr = torchaudio.load(op.join(recon_folder, f\"{uid}.wav\"))\n",
    "#     wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "#     wav_1 = wav_1.squeeze()\n",
    "#     wav_2 = wav_2.squeeze()\n",
    "#     mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "#     mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "#     # DTW\n",
    "#     _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "#     twf = np.array(path).T\n",
    "#     mel_2 = mel_2[twf[0]]\n",
    "#     mel_1 = mel_1[twf[1]]\n",
    "#     # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "#     mel_1 = mel_1[:, 1:]\n",
    "#     mel_2 = mel_2[:, 1:]\n",
    "#     result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "#     MCDs_recon.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ee62c7-cef7-42cc-8fbc-0d6057858ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ae94e4ddd54854a0ebc3d3fef1a38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 39s\n",
      "Wall time: 7min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def sptk_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    "    is_padding: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract SPTK-based mel-cepstrum.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Sampling rate\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "        is_padding (bool): Whether to pad the end of signal (default=False).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "\n",
    "    \"\"\"\n",
    "    # perform padding\n",
    "    if is_padding:\n",
    "        n_pad = n_fft - (len(x) - n_fft) % n_shift\n",
    "        x = np.pad(x, (0, n_pad), \"reflect\")\n",
    "\n",
    "    # get number of frames\n",
    "    n_frame = (len(x) - n_fft) // n_shift + 1\n",
    "\n",
    "    # get window function\n",
    "    win = pysptk.sptk.hamming(n_fft)\n",
    "\n",
    "    # check mcep and alpha\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "\n",
    "    # calculate spectrogram\n",
    "    mcep = [\n",
    "        pysptk.mcep(\n",
    "            x[n_shift * i : n_shift * i + n_fft] * win,\n",
    "            mcep_dim,\n",
    "            mcep_alpha,\n",
    "            eps=1e-6,\n",
    "            etype=1,\n",
    "        )\n",
    "        for i in range(n_frame)\n",
    "    ]\n",
    "\n",
    "    return np.stack(mcep)\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uids = []\n",
    "MCDs_recon = []\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    gen_x, gen_fs = sf.read(op.join(recon_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(gt_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    # print(gen_fs, gt_fs)\n",
    "    fs = gen_fs\n",
    "    if gen_fs != gt_fs:\n",
    "        gt_x = librosa.resample(gt_x.astype(float), orig_sr=gt_fs, target_sr=gen_fs)\n",
    "\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep = sptk_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        n_fft=512,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep = sptk_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        n_fft=512,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_mcep_dtw = gen_mcep[twf[0]]\n",
    "    gt_mcep_dtw = gt_mcep[twf[1]]\n",
    "\n",
    "    # MCD\n",
    "    diff2sum = np.sum((gen_mcep_dtw - gt_mcep_dtw) ** 2, 1)\n",
    "    mcd = np.mean(10.0 / np.log(10.0) * np.sqrt(2 * diff2sum), 0)\n",
    "    result = mcd\n",
    "    MCDs_recon.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26300ab-11fc-4268-82e7-b50f8645a4cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# uids = []\n",
    "# MCDs_gt = []\n",
    "# for uid in tqdm(val_uids):\n",
    "#     uids.append(uid)\n",
    "#     wav_1, sr = torchaudio.load(op.join(gt_folder, f\"{uid}.wav\"))\n",
    "#     wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "#     wav_1 = wav_1.squeeze()\n",
    "#     wav_2 = wav_2.squeeze()\n",
    "#     mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "#     mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "#     # DTW\n",
    "#     _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "#     twf = np.array(path).T\n",
    "#     mel_2 = mel_2[twf[0]]\n",
    "#     mel_1 = mel_1[twf[1]]\n",
    "#     # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "#     mel_1 = mel_1[:, 1:]\n",
    "#     mel_2 = mel_2[:, 1:]\n",
    "#     result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "#     MCDs_gt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70438c49-a88b-4ce3-8542-b4e0a7863004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cbc5d312ad4ca8be47bafcbce5e6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 24s\n",
      "Wall time: 7min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def sptk_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    "    is_padding: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract SPTK-based mel-cepstrum.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Sampling rate\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "        is_padding (bool): Whether to pad the end of signal (default=False).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "\n",
    "    \"\"\"\n",
    "    # perform padding\n",
    "    if is_padding:\n",
    "        n_pad = n_fft - (len(x) - n_fft) % n_shift\n",
    "        x = np.pad(x, (0, n_pad), \"reflect\")\n",
    "\n",
    "    # get number of frames\n",
    "    n_frame = (len(x) - n_fft) // n_shift + 1\n",
    "\n",
    "    # get window function\n",
    "    win = pysptk.sptk.hamming(n_fft)\n",
    "\n",
    "    # check mcep and alpha\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "\n",
    "    # calculate spectrogram\n",
    "    mcep = [\n",
    "        pysptk.mcep(\n",
    "            x[n_shift * i : n_shift * i + n_fft] * win,\n",
    "            mcep_dim,\n",
    "            mcep_alpha,\n",
    "            eps=1e-6,\n",
    "            etype=1,\n",
    "        )\n",
    "        for i in range(n_frame)\n",
    "    ]\n",
    "\n",
    "    return np.stack(mcep)\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uids = []\n",
    "MCDs_gt = []\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(gt_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    # print(gen_fs, gt_fs)\n",
    "    fs = gen_fs\n",
    "    if gen_fs != gt_fs:\n",
    "        gt_x = librosa.resample(gt_x.astype(float), orig_sr=gt_fs, target_sr=gen_fs)\n",
    "\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep = sptk_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        n_fft=512,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep = sptk_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        n_fft=512,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_mcep_dtw = gen_mcep[twf[0]]\n",
    "    gt_mcep_dtw = gt_mcep[twf[1]]\n",
    "\n",
    "    # MCD\n",
    "    diff2sum = np.sum((gen_mcep_dtw - gt_mcep_dtw) ** 2, 1)\n",
    "    mcd = np.mean(10.0 / np.log(10.0) * np.sqrt(2 * diff2sum), 0)\n",
    "    result = mcd\n",
    "    MCDs_gt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3a427e-d273-4d00-9382-36adbf7f87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD_recon': MCDs_recon,\n",
    "    'MCD_gt': MCDs_gt,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b24fda89-ed25-4ef6-a9e4-ee32b4b9a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_0714lb_800k_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ff9b21-5a50-4a7d-b7f5-cac60b40bbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCDs_recon mean on 0714lb_800k: 5.648897674732002\n",
      "MCDs_recon std on 0714lb_800k: 1.6735133209184978\n",
      "MCDs_gt mean on 0714lb_800k: 8.619400217271503\n",
      "MCDs_gt std on 0714lb_800k: 1.7607610658468404\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCDs_recon mean on 0714lb_800k: {torch.tensor(MCDs_recon).mean()}\")\n",
    "print(f\"MCDs_recon std on 0714lb_800k: {torch.tensor(MCDs_recon).std()}\")\n",
    "\n",
    "print(f\"MCDs_gt mean on 0714lb_800k: {torch.tensor(MCDs_gt).mean()}\")\n",
    "print(f\"MCDs_gt std on 0714lb_800k: {torch.tensor(MCDs_gt).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dce2f55-d549-4eea-8346-050b626baf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>MCD_recon</th>\n",
       "      <th>MCD_gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>7117_86732_000004_000002</td>\n",
       "      <td>3.079804</td>\n",
       "      <td>6.036933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2618_138042_000045_000002</td>\n",
       "      <td>3.121507</td>\n",
       "      <td>6.718762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2512_157242_000069_000000</td>\n",
       "      <td>3.131460</td>\n",
       "      <td>7.538399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>1445_139404_000007_000004</td>\n",
       "      <td>3.156019</td>\n",
       "      <td>6.967686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>4054_11077_000004_000004</td>\n",
       "      <td>3.166626</td>\n",
       "      <td>7.140275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           uid  MCD_recon    MCD_gt\n",
       "394   7117_86732_000004_000002   3.079804  6.036933\n",
       "262  2618_138042_000045_000002   3.121507  6.718762\n",
       "69   2512_157242_000069_000000   3.131460  7.538399\n",
       "506  1445_139404_000007_000004   3.156019  6.967686\n",
       "101   4054_11077_000004_000004   3.166626  7.140275"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_mcd_df.nsmallest(5, 'MCD_recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1857b4b2-6109-4e44-9f08-08106edbb584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>MCD_recon</th>\n",
       "      <th>MCD_gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>369_125883_000004_000001</td>\n",
       "      <td>15.447646</td>\n",
       "      <td>17.432105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2004_147173_000029_000003</td>\n",
       "      <td>13.154249</td>\n",
       "      <td>11.902160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>209_157830_000013_000003</td>\n",
       "      <td>12.544040</td>\n",
       "      <td>11.344743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>954_130627_000038_000001</td>\n",
       "      <td>12.527503</td>\n",
       "      <td>14.996965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>369_125883_000053_000000</td>\n",
       "      <td>12.090939</td>\n",
       "      <td>17.993798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           uid  MCD_recon     MCD_gt\n",
       "178   369_125883_000004_000001  15.447646  17.432105\n",
       "421  2004_147173_000029_000003  13.154249  11.902160\n",
       "74    209_157830_000013_000003  12.544040  11.344743\n",
       "7     954_130627_000038_000001  12.527503  14.996965\n",
       "125   369_125883_000053_000000  12.090939  17.993798"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_mcd_df.nlargest(5, 'MCD_recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418a137-8aad-443e-a875-2e904bd83628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e339371-1573-4cac-b33c-df3ba9972507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f188582-f5c5-4128-91b5-3a340e6bc6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb80e8-0ed0-4175-ab28-66cb79bd18b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8c179-414b-435a-88ae-962d0a3faa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "872faa45-aefc-428a-ac0d-0f454d00199d",
   "metadata": {},
   "source": [
    "# log f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b25db30f-8eb3-4046-94a6-9a2d0a74a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/espnet/espnet/blob/3e0dad524d62ccd45e067e9b36049f2214ea972a/egs2/TEMPLATE/asr1/pyscripts/utils/evaluate_f0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9568b1f-da42-457b-b9ba-b5a3fda0dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    f0min: int = 40,\n",
    "    f0max: int = 800,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract World-based acoustic features.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Minimum f0 value (default=40).\n",
    "        f0 (int): Maximum f0 value (default=800).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "        ndarray: F0 sequence (N,).\n",
    "\n",
    "    \"\"\"\n",
    "    # extract features\n",
    "    x = x.astype(np.float64)\n",
    "    f0, time_axis = pw.harvest(\n",
    "        x,\n",
    "        fs,\n",
    "        f0_floor=f0min,\n",
    "        f0_ceil=f0max,\n",
    "        frame_period=n_shift / fs * 1000,\n",
    "    )\n",
    "    sp = pw.cheaptrick(x, f0, time_axis, fs, fft_size=n_fft)\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "    mcep = pysptk.sp2mc(sp, mcep_dim, mcep_alpha)\n",
    "\n",
    "    return mcep, f0\n",
    "\n",
    "\n",
    "def _get_basename(path: str) -> str:\n",
    "    return os.path.splitext(os.path.split(path)[-1])[0]\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09bf0f32-e496-4c01-af00-a569f942c03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e45dc874f94ee79c1a8aa55d5af45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2846: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    }
   ],
   "source": [
    "uids = []\n",
    "logf0_rmses_recon = []\n",
    "logf0_corrs_recon = []\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(recon_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    fs = gen_fs\n",
    "    if gen_fs != gt_fs:\n",
    "        gt_x = librosa.resample(gt_x.astype(float), orig_sr=gt_fs, target_sr=gen_fs)\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep, gen_f0 = world_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep, gt_f0 = world_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_f0_dtw = gen_f0[twf[0]]\n",
    "    gt_f0_dtw = gt_f0[twf[1]]\n",
    "    \n",
    "    # Get voiced part\n",
    "    nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "    gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs])\n",
    "    gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs])\n",
    "\n",
    "    # log F0 RMSE\n",
    "    log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "    # print(f\"{uid} {log_f0_rmse:.4f}\")\n",
    "\n",
    "    # log F0 corr\n",
    "    log_f0_corr = np.corrcoef(gen_f0_dtw_voiced, gt_f0_dtw_voiced)[0][1]\n",
    "    # print(f\"{uid} {log_f0_corr:.4f}\")\n",
    "\n",
    "    logf0_rmses_recon.append(log_f0_rmse)\n",
    "    logf0_corrs_recon.append(log_f0_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ffddca8-6521-4129-913e-05284f76f587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1accac67c8064d6ca731c5d3165b449a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uids = []\n",
    "logf0_rmses_gt = []\n",
    "logf0_corrs_gt = []\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(gt_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    fs = gen_fs\n",
    "    if gen_fs != gt_fs:\n",
    "        gt_x = librosa.resample(gt_x.astype(float), orig_sr=gt_fs, target_sr=gen_fs)\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep, gen_f0 = world_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep, gt_f0 = world_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_f0_dtw = gen_f0[twf[0]]\n",
    "    gt_f0_dtw = gt_f0[twf[1]]\n",
    "    \n",
    "    # Get voiced part\n",
    "    nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "    gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs])\n",
    "    gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs])\n",
    "\n",
    "    # log F0 RMSE\n",
    "    log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "    # print(f\"{uid} {log_f0_rmse:.4f}\")\n",
    "\n",
    "    # log F0 corr\n",
    "    log_f0_corr = np.corrcoef(gen_f0_dtw_voiced, gt_f0_dtw_voiced)[0][1]\n",
    "    # print(f\"{uid} {log_f0_corr:.4f}\")\n",
    "\n",
    "    logf0_rmses_gt.append(log_f0_rmse)\n",
    "    logf0_corrs_gt.append(log_f0_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "febc1723-b5cc-4c52-978e-2e9a0fe34716",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_logf0_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'logf0_rmse_recon': logf0_rmses_recon,\n",
    "    'logf0_corr_recon': logf0_corrs_recon,\n",
    "    'logf0_rmse_gt': logf0_rmses_gt,\n",
    "    'logf0_corr_gt': logf0_corrs_gt,\n",
    "})\n",
    "lb_logf0_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\logF0_LB_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c618f1fd-2887-48be-a137-3741507ab3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_logf0_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23544242-7f9d-4d4c-8af8-8e48d13593de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_logf0_df = lb_logf0_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f2064f0-e8c2-4940-9c10-983a3ba01dc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logf0_rmse_recon mean on LibriTTS: 0.2670626798174982\n",
      "logf0_rmse_recon std on LibriTTS: 0.11483676962626277\n",
      "logf0_corr_recon mean on LibriTTS: 0.5683756745963985\n",
      "logf0_corr_recon std on LibriTTS: 0.24157468871441268\n",
      "logf0_rmse_gt mean on LibriTTS: 0.27041593332575575\n",
      "logf0_rmse_gt std on LibriTTS: 0.12007872804610982\n",
      "logf0_corr_gt mean on LibriTTS: 0.5523046469028201\n",
      "logf0_corr_gt std on LibriTTS: 0.23692520122464472\n"
     ]
    }
   ],
   "source": [
    "print(f\"logf0_rmse_recon mean on LibriTTS: {lb_logf0_df['logf0_rmse_recon'].values.mean()}\")\n",
    "print(f\"logf0_rmse_recon std on LibriTTS: {lb_logf0_df['logf0_rmse_recon'].values.std()}\")\n",
    "print(f\"logf0_corr_recon mean on LibriTTS: {lb_logf0_df['logf0_corr_recon'].values.mean()}\")\n",
    "print(f\"logf0_corr_recon std on LibriTTS: {lb_logf0_df['logf0_corr_recon'].values.std()}\")\n",
    "\n",
    "print(f\"logf0_rmse_gt mean on LibriTTS: {lb_logf0_df['logf0_rmse_gt'].values.mean()}\")\n",
    "print(f\"logf0_rmse_gt std on LibriTTS: {lb_logf0_df['logf0_rmse_gt'].values.std()}\")\n",
    "print(f\"logf0_corr_gt mean on LibriTTS: {lb_logf0_df['logf0_corr_gt'].values.mean()}\")\n",
    "print(f\"logf0_corr_gt std on LibriTTS: {lb_logf0_df['logf0_corr_gt'].values.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bda254-f776-4e5c-9974-ec3988acefeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
