{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d8d559-8f84-47f9-93b0-f8ca9b57ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98334bb1-21e4-4ce1-8500-b08d2bd2a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.auto_tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eacc6d9-7261-4567-865a-91ca3fc6802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e54b167-ff64-47eb-a52c-bb40c6ae729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v6\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770e48c9-d056-45eb-b831-9da7b0daf794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2772\n"
     ]
    }
   ],
   "source": [
    "print(len(val_uids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1419dda-c6fa-408d-b1ea-fdcbe49bb92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77269245-a14e-4713-805d-f8bb5f0e4ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model, get_vocoder, get_param_num, vocoder_infer\n",
    "from utils.tools import to_device, log, synth_one_sample, expand, plot_mel\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset\n",
    "from utils.auto_tqdm import tqdm\n",
    "\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb824442-f53a-49d8-acc9-cd4a6291041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cf05ed-17bf-4b39-afbc-78eb0b851045",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/Ego4D_final_v6/0629b_preprocess.yaml -m ./config/Ego4D_final_v6/0629b_model.yaml -t ./config/Ego4D_final_v6/0629b_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec536df-8b84-4685-a322-e26221b73017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(restore_step=0, preprocess_config='./config/Ego4D_final_v6/0629b_preprocess.yaml', model_config='./config/Ego4D_final_v6/0629b_model.yaml', train_config='./config/Ego4D_final_v6/0629b_train.yaml')\n"
     ]
    }
   ],
   "source": [
    "pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbdae12a-f8eb-4ae0-be01-55a55e07f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f13f577-b9c8-4ad2-a634-55a70d040cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0549bc9-52a7-4951-92ed-5059f0ab17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/0629b/ckpt/Ego4D_final_v6/600000.pth.tar'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c62618-ef41-4a15-a751-9c9c318bb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Not using speaker embeddings.\n",
      "False\n",
      "None\n",
      "Number of FastSpeech2 Parameters: 35159745\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed0aabb-f43c-4284-8475-3a38695a55b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec0d160f-c4e4-48cc-b763-7114bc95391a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "step = args.restore_step + 1\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69db3413-764e-49c9-a804-f82f47fd7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    \"val.txt\", 'val', preprocess_config, train_config, sort=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a1f7d5-db84-4d5f-a80f-359d51a36d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2646d25d-8b89-4b0b-8461-e37f957ab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "# Get loss function\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a236aa5-e39d-415c-8845-fcacf7f41fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/0629b/result/Ego4D_final_v6\\\\plots'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(train_config['path']['result_path'], 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35b651f6-4aff-45b7-a580-cdbd695f9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# => batch:\n",
    "# return (\n",
    "#     ids,\n",
    "#     raw_texts,\n",
    "#     speakers,\n",
    "#     texts,\n",
    "#     text_lens,\n",
    "#     max(text_lens),\n",
    "#     mels,\n",
    "#     mel_lens,\n",
    "#     max(mel_lens),\n",
    "#     pitches,\n",
    "#     energies,\n",
    "#     durations,\n",
    "#     speaker_embeddings,\n",
    "# )\n",
    "# [12] 对应speaker embedding\n",
    "\n",
    "\n",
    "# => model output / prediction\n",
    "# return (\n",
    "#     output,\n",
    "#     postnet_output,\n",
    "#     p_predictions,\n",
    "#     e_predictions,\n",
    "#     log_d_predictions,\n",
    "#     d_rounded,\n",
    "#     src_masks,\n",
    "#     mel_masks,\n",
    "#     src_lens,\n",
    "#     mel_lens,\n",
    "# )\n",
    "output_plot_path = os.path.join(train_config['path']['result_path'], 'plot')\n",
    "output_mel_syn_path = os.path.join(train_config['path']['result_path'], 'mel', 'syn')\n",
    "output_mel_gt_path = os.path.join(train_config['path']['result_path'], 'mel', 'gt')\n",
    "output_wav_syn_path = os.path.join(train_config['path']['result_path'], 'wav', 'synthesized')\n",
    "output_wav_rec_path = os.path.join(train_config['path']['result_path'], 'wav', 'reconstructed')\n",
    "os.makedirs(output_plot_path, exist_ok=True)\n",
    "os.makedirs(output_mel_syn_path, exist_ok=True)\n",
    "os.makedirs(output_mel_gt_path, exist_ok=True)\n",
    "os.makedirs(output_wav_syn_path, exist_ok=True)\n",
    "os.makedirs(output_wav_rec_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0db3412-aebb-44f6-8b62-a6172a9255f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6c70d9144f42d599edff17dbebb5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batchs in tqdm(loader):\n",
    "    for targets in batchs:\n",
    "        targets = to_device(targets, device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(*(targets[2:]))\n",
    "        basenames = targets[0]\n",
    "        for i in range(len(predictions[0])):\n",
    "            basename = basenames[i]\n",
    "            src_len = predictions[8][i].item()\n",
    "            mel_len = predictions[9][i].item()\n",
    "            mel_prediction = predictions[1][i, :mel_len].detach().transpose(0, 1)\n",
    "            mel_target = targets[6][i, :mel_len].detach().transpose(0, 1)\n",
    "\n",
    "            torch.save(mel_prediction.cpu(), os.path.join(output_mel_syn_path, f\"{basename}.pt\"))\n",
    "            torch.save(mel_target.cpu(), os.path.join(output_mel_gt_path, f\"{basename}.pt\"))\n",
    "            \n",
    "            \n",
    "            duration = predictions[5][i, :src_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n",
    "                pitch = predictions[2][i, :src_len].detach().cpu().numpy()\n",
    "                pitch = expand(pitch, duration)\n",
    "            else:\n",
    "                pitch = predictions[2][i, :mel_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"] == \"phoneme_level\":\n",
    "                energy = predictions[3][i, :src_len].detach().cpu().numpy()\n",
    "                energy = expand(energy, duration)\n",
    "            else:\n",
    "                energy = predictions[3][i, :mel_len].detach().cpu().numpy()\n",
    "\n",
    "            with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \n",
    "                                   \"stats.json\")) as f:\n",
    "                stats = json.load(f)\n",
    "                stats = stats[\"pitch\"] + stats[\"energy\"][:2]\n",
    "                                       \n",
    "            fig = plot_mel(\n",
    "                [\n",
    "                    (mel_prediction.cpu().numpy(), pitch, energy),\n",
    "                    (mel_target.cpu().numpy(), pitch, energy),\n",
    "                ],\n",
    "                stats,\n",
    "                [\"Synthetized Spectrogram\", \"Ground-Truth Spectrogram\"],\n",
    "            )\n",
    "            ### TODO: change to svg\n",
    "            plt.savefig(os.path.join(output_plot_path, f\"{basename}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # from .model import vocoder_infer\n",
    "\n",
    "        mel_predictions = predictions[1].transpose(1, 2)\n",
    "        mel_targets = targets[6].transpose(1, 2)\n",
    "        \n",
    "        lengths = predictions[9] * preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "        wav_predictions = vocoder_infer(\n",
    "            mel_predictions, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "        )\n",
    "        wav_targets = vocoder_infer(\n",
    "        mel_targets, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "    )\n",
    "    \n",
    "        sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "        for wav, basename in zip(wav_predictions, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_syn_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "        for wav, basename in zip(wav_targets, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_rec_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "\n",
    "        \n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca11ed-df8b-454c-8ec2-e00e4ae6e9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c40628a-cc3b-4b7c-b948-caf07899472e",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b173ed4-4580-41e2-8ae2-82e23b91c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ad9daa-a251-47a4-828a-b3b7ac1b43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cfbbf6-9f08-43cb-99c0-e5d8d7c2af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6dc1973-5995-4590-ac48-c2527d5a2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfaa7a-bbf7-4678-9114-232cdd9474c6",
   "metadata": {},
   "source": [
    "## MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80c3dcaf-7a5a-44f0-a9cd-b2e20abc6271",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def antidiag_indices(offset, min_i=0, max_i=None, min_j=0, max_j=None):\n",
    "    \"\"\"\n",
    "    for a (3, 4) matrix with min_i=1, max_i=3, min_j=1, max_j=4, outputs\n",
    "    offset=2 (1, 1),\n",
    "    offset=3 (2, 1), (1, 2)\n",
    "    offset=4 (2, 2), (1, 3)\n",
    "    offset=5 (2, 3)\n",
    "    constraints:\n",
    "        i + j = offset\n",
    "        min_j <= j < max_j\n",
    "        min_i <= offset - j < max_i\n",
    "    \"\"\"\n",
    "    if max_i is None:\n",
    "        max_i = offset + 1\n",
    "    if max_j is None:\n",
    "        max_j = offset + 1\n",
    "    min_j = max(min_j, offset - max_i + 1, 0)\n",
    "    max_j = min(max_j, offset - min_i + 1, offset + 1)\n",
    "    j = torch.arange(min_j, max_j)\n",
    "    i = offset - j\n",
    "    return torch.stack([i, j])\n",
    "\n",
    "def batch_dynamic_time_warping(distance, shapes=None):\n",
    "    \"\"\"full batched DTW without any constraints\n",
    "    distance:  (batchsize, max_M, max_N) matrix\n",
    "    shapes: (batchsize,) vector specifying (M, N) for each entry\n",
    "    \"\"\"\n",
    "    # ptr: 0=left, 1=up-left, 2=up\n",
    "    ptr2dij = {0: (0, -1), 1: (-1, -1), 2: (-1, 0)}\n",
    "\n",
    "    bsz, m, n = distance.size()\n",
    "    cumdist = torch.zeros_like(distance)\n",
    "    backptr = torch.zeros_like(distance).type(torch.int32) - 1\n",
    "\n",
    "    # initialize\n",
    "    cumdist[:, 0, :] = distance[:, 0, :].cumsum(dim=-1)\n",
    "    cumdist[:, :, 0] = distance[:, :, 0].cumsum(dim=-1)\n",
    "    backptr[:, 0, :] = 0\n",
    "    backptr[:, :, 0] = 2\n",
    "\n",
    "    # DP with optimized anti-diagonal parallelization, O(M+N) steps\n",
    "    for offset in range(2, m + n - 1):\n",
    "        ind = antidiag_indices(offset, 1, m, 1, n)\n",
    "        c = torch.stack(\n",
    "            [cumdist[:, ind[0], ind[1] - 1], cumdist[:, ind[0] - 1, ind[1] - 1],\n",
    "             cumdist[:, ind[0] - 1, ind[1]], ],\n",
    "            dim=2\n",
    "        )\n",
    "        v, b = c.min(axis=-1)\n",
    "        backptr[:, ind[0], ind[1]] = b.int()\n",
    "        cumdist[:, ind[0], ind[1]] = v + distance[:, ind[0], ind[1]]\n",
    "\n",
    "    # backtrace\n",
    "    pathmap = torch.zeros_like(backptr)\n",
    "    for b in range(bsz):\n",
    "        i = m - 1 if shapes is None else (shapes[b][0] - 1).item()\n",
    "        j = n - 1 if shapes is None else (shapes[b][1] - 1).item()\n",
    "        dtwpath = [(i, j)]\n",
    "        while (i != 0 or j != 0) and len(dtwpath) < 10000:\n",
    "            assert (i >= 0 and j >= 0)\n",
    "            di, dj = ptr2dij[backptr[b, i, j].item()]\n",
    "            i, j = i + di, j + dj\n",
    "            dtwpath.append((i, j))\n",
    "        dtwpath = dtwpath[::-1]\n",
    "        indices = torch.from_numpy(np.array(dtwpath))\n",
    "        pathmap[b, indices[:, 0], indices[:, 1]] = 1\n",
    "\n",
    "    return cumdist, backptr, pathmap\n",
    "\n",
    "\n",
    "def compute_l2_dist(x1, x2):\n",
    "    \"\"\"compute an (m, n) L2 distance matrix from (m, d) and (n, d) matrices\"\"\"\n",
    "    return torch.cdist(x1.unsqueeze(0), x2.unsqueeze(0), p=2).squeeze(0).pow(2)\n",
    "\n",
    "\n",
    "def compute_rms_dist(x1, x2):\n",
    "    l2_dist = compute_l2_dist(x1, x2)\n",
    "    return (l2_dist / x1.size(1)).pow(0.5)\n",
    "\n",
    "\n",
    "def get_divisor(pathmap, normalize_type):\n",
    "    if normalize_type is None:\n",
    "        return 1\n",
    "    elif normalize_type == \"len1\":\n",
    "        return pathmap.size(0)\n",
    "    elif normalize_type == \"len2\":\n",
    "        return pathmap.size(1)\n",
    "    elif normalize_type == \"path\":\n",
    "        return pathmap.sum().item()\n",
    "    else:\n",
    "        raise ValueError(f\"normalize_type {normalize_type} not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcf0e536-d42f-494b-8acb-b3d1cb94d0e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def batch_mel_cepstral_distortion(\n",
    "        y1, y2, sr, normalize_type=\"path\", mfcc_fn=None\n",
    "):\n",
    "    # https://huggingface.co/spaces/OFA-Sys/OFA-Generic_Interface/blob/main/fairseq/fairseq/tasks/text_to_speech.py\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/2011.03568.pdf\n",
    "    The root mean squared error computed on 13-dimensional MFCC using DTW for\n",
    "    alignment. MFCC features are computed from an 80-channel log-mel\n",
    "    spectrogram using a 50ms Hann window and hop of 12.5ms.\n",
    "    y1: list of waveforms\n",
    "    y2: list of waveforms\n",
    "    sr: sampling rate\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        import torchaudio\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install torchaudio: pip install torchaudio\")\n",
    "\n",
    "    if mfcc_fn is None or mfcc_fn.sample_rate != sr:\n",
    "        melkwargs = {\n",
    "            \"n_fft\": int(0.05 * sr), \"win_length\": int(0.05 * sr),\n",
    "            \"hop_length\": int(0.0125 * sr), \"f_min\": 20,\n",
    "            \"n_mels\": 80, \"window_fn\": torch.hann_window\n",
    "        }\n",
    "        mfcc_fn = torchaudio.transforms.MFCC(\n",
    "            sr, n_mfcc=13, log_mels=True, melkwargs=melkwargs\n",
    "        ).to(y1[0].device)\n",
    "    return batch_compute_distortion(\n",
    "        y1, y2, sr, lambda y: mfcc_fn(y).transpose(-1, -2), compute_rms_dist,\n",
    "        normalize_type\n",
    "    )\n",
    "\n",
    "    \n",
    "def batch_compute_distortion(y1, y2, sr, feat_fn, dist_fn, normalize_type):\n",
    "    d, s, x1, x2 = [], [], [], []\n",
    "    for cur_y1, cur_y2 in zip(y1, y2):\n",
    "        assert (cur_y1.ndim == 1 and cur_y2.ndim == 1)\n",
    "        cur_x1 = feat_fn(cur_y1)\n",
    "        cur_x2 = feat_fn(cur_y2)\n",
    "        x1.append(cur_x1)\n",
    "        x2.append(cur_x2)\n",
    "\n",
    "        cur_d = dist_fn(cur_x1, cur_x2)\n",
    "        d.append(cur_d)\n",
    "        s.append(d[-1].size())\n",
    "    max_m = max(ss[0] for ss in s)\n",
    "    max_n = max(ss[1] for ss in s)\n",
    "    d = torch.stack(\n",
    "        [F.pad(dd, (0, max_n - dd.size(1), 0, max_m - dd.size(0))) for dd in d]\n",
    "    )\n",
    "    s = torch.LongTensor(s).to(d.device)\n",
    "    cumdists, backptrs, pathmaps = batch_dynamic_time_warping(d, s)\n",
    "\n",
    "    rets = []\n",
    "    itr = zip(s, x1, x2, d, cumdists, backptrs, pathmaps)\n",
    "    for (m, n), cur_x1, cur_x2, dist, cumdist, backptr, pathmap in itr:\n",
    "        cumdist = cumdist[:m, :n]\n",
    "        backptr = backptr[:m, :n]\n",
    "        pathmap = pathmap[:m, :n]\n",
    "        divisor = get_divisor(pathmap, normalize_type)\n",
    "\n",
    "        distortion = cumdist[-1, -1] / divisor\n",
    "        ret = distortion, (cur_x1, cur_x2, dist, cumdist, backptr, pathmap)\n",
    "        rets.append(ret)\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dc9a40b-4e8c-49e7-ba9b-6cc9bf02dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_folder = r'.\\output\\0629b\\result\\Ego4D_final_v6\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0629b\\result\\Ego4D_final_v6\\wav\\synthesized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d6d63a2-2675-49af-95a5-79a0ac74ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v6\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c60fe162-2269-4a0b-a912-77b1d221ed95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2772"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ec6b34f-dbc3-4cfd-a8df-c2ba6c574591",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "MCDs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6906800-1d80-4f7a-89c7-15900810b52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b5d67e703b424fab7e8ea257a02356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(ref_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    results = batch_mel_cepstral_distortion(\n",
    "            [wav_1],\n",
    "            [wav_2],\n",
    "            sr=sr,\n",
    "    )\n",
    "    MCDs.append(results[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "627be8a6-3841-40c7-8b3e-bde8a070452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lj_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD': MCDs,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb87d03f-b5a8-4ff7-8515-c329da4cc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_0629b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8865826e-ab24-4591-b16a-8817268122a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD mean on 0629b: 4.545107364654541\n",
      "MCD std on 0629b: 1.0436636209487915\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCD mean on 0629b: {torch.tensor(MCDs).mean()}\")\n",
    "print(f\"MCD std on 0629b: {torch.tensor(MCDs).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2236e-23ef-4466-965d-27a0e6c53e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b62b969-221d-4737-b3c5-aa3db2197b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyworld as pw\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "addf9de5-527e-4af0-8665-b9eecf03e0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397470d8-61b9-4b97-a165-135c44b4ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e8f607e-0e80-41bd-bd2a-a6e663f31622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import fnmatch\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pysptk\n",
    "import pyworld as pw\n",
    "import soundfile as sf\n",
    "from fastdtw import fastdtw\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83430229-1035-4d96-b30f-b5495123e48c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def world_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    f0min: int = 40,\n",
    "    f0max: int = 800,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 13,\n",
    "    mcep_alpha: float = 0.41,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract World-based acoustic features.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Minimum f0 value (default=40).\n",
    "        f0 (int): Maximum f0 value (default=800).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "        ndarray: F0 sequence (N,).\n",
    "\n",
    "    \"\"\"\n",
    "    # extract features\n",
    "    x = x.astype(np.float64)\n",
    "    # f0, time_axis = pw.harvest(\n",
    "    #     x,\n",
    "    #     fs,\n",
    "    #     f0_floor=f0min,\n",
    "    #     f0_ceil=f0max,\n",
    "    #     frame_period=n_shift / fs * 1000,\n",
    "    # )\n",
    "    pitch, t = pw.dio(\n",
    "        x,\n",
    "        fs,\n",
    "         frame_period=n_shift / fs * 1000,\n",
    "        )\n",
    "    sp = pw.cheaptrick(x, f0, time_axis, fs, fft_size=n_fft)\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "    mcep = pysptk.sp2mc(sp, 13, mcep_alpha)\n",
    "\n",
    "    return mcep, f0\n",
    "\n",
    "\n",
    "def _get_basename(path: str) -> str:\n",
    "    return os.path.splitext(os.path.split(path)[-1])[0]\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "def0b4fd-9f42-40bb-94b7-a2c118e71f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_folder = r'.\\output\\0629b\\result\\Ego4D_final_v6\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0629b\\result\\Ego4D_final_v6\\wav\\synthesized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a301363-1980-4561-8745-c0a3b483fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v6\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "232c70c0-d6a9-4566-98a2-ceaeb0b13900",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "melkwargs = {\n",
    "    \"n_fft\": int(0.05 * sr), \"win_length\": int(0.05 * sr),\n",
    "    \"hop_length\": int(0.0125 * sr), \"f_min\": 20,\n",
    "    \"n_mels\": 80, \"window_fn\": torch.hann_window\n",
    "}\n",
    "mfcc_fn = torchaudio.transforms.MFCC(\n",
    "    sr, n_mfcc=13, log_mels=True, melkwargs=melkwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c915f80-acb1-44f7-b91f-0b7cba4c6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "MCDs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bd1c31d-0110-4981-b7b6-b177f6651128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6062c6ba29c41499fc4f73aa641dc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(ref_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41b3324b-3304-4c89-9738-018ebcd30635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lj_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD': MCDs,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fcb1af1f-9ebe-479a-acbd-5aa2c4d29a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_0629b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3699018-9209-49db-a773-d0b51854254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCD mean on 0629b: 13.024544715881348\n",
      "MCD std on 0629b: 2.407449960708618\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCD mean on 0629b: {torch.tensor(MCDs).mean()}\")\n",
    "print(f\"MCD std on 0629b: {torch.tensor(MCDs).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48930b4f-37ba-4fe8-aa54-6f847811f153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337d1ab5-23a8-4d06-913e-7cd4e897b986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ff7af5-eaec-44e4-8565-d135bb3e2bde",
   "metadata": {},
   "source": [
    "## log f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4117e2f-d289-48f4-82d9-9e9665bb6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/espnet/espnet/blob/3e0dad524d62ccd45e067e9b36049f2214ea972a/egs2/TEMPLATE/asr1/pyscripts/utils/evaluate_f0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64564c63-dd77-420b-bd99-d191dab39218",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def world_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    f0min: int = 40,\n",
    "    f0max: int = 800,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract World-based acoustic features.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Minimum f0 value (default=40).\n",
    "        f0 (int): Maximum f0 value (default=800).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "        ndarray: F0 sequence (N,).\n",
    "\n",
    "    \"\"\"\n",
    "    # extract features\n",
    "    x = x.astype(np.float64)\n",
    "    f0, time_axis = pw.harvest(\n",
    "        x,\n",
    "        fs,\n",
    "        f0_floor=f0min,\n",
    "        f0_ceil=f0max,\n",
    "        frame_period=n_shift / fs * 1000,\n",
    "    )\n",
    "    sp = pw.cheaptrick(x, f0, time_axis, fs, fft_size=n_fft)\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "    mcep = pysptk.sp2mc(sp, mcep_dim, mcep_alpha)\n",
    "\n",
    "    return mcep, f0\n",
    "\n",
    "\n",
    "def _get_basename(path: str) -> str:\n",
    "    return os.path.splitext(os.path.split(path)[-1])[0]\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb11c9d1-677b-4d88-8aab-b1fa7e5bf3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = []\n",
    "logf0_rmses = []\n",
    "logf0_corrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad46adf4-e854-4ad4-a120-797bd55ffcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02b155fa31446ce92aab2098452ea50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip uid 342edcf2-1150-42a0-af79-030571c0996d. len == 0.\n",
      "Skip uid fb487f40-31b3-4397-a656-5099380efa0b. len == 0.\n",
      "Skip uid f03be5e0-c2b2-4479-9b4f-30730e9e30a6. len == 0.\n",
      "Skip uid 2ff974ac-b0f1-4611-8c9f-61c216a8ba96. len == 0.\n",
      "Skip uid 67ae347b-a4a6-4966-a736-c03a831d1a2a. len == 0.\n",
      "Skip uid 959dcb2b-4291-45b4-aa24-dc144de6fc1a. len == 0.\n",
      "Skip uid 94ab3c42-86a9-427c-9dc7-9d3583836f89. len == 0.\n",
      "Skip uid 3c82b212-ca78-4648-8342-56c8baa097b3. len == 0.\n",
      "Skip uid 1d22ffa5-f5bb-4f58-a466-b5ea864af573. len == 0.\n",
      "Skip uid f47ab348-daa9-4497-8abf-75ff2ca41285. len == 0.\n",
      "Skip uid 2eae0b32-1925-4045-9c7e-b35cbf0273ad. len == 0.\n",
      "Skip uid 8b91e519-e0b5-43b6-a809-652fff3351e9. len == 0.\n",
      "Skip uid 9f56d2e9-8536-4ce4-878e-e31f5c64345a. len == 0.\n",
      "Skip uid 06c43857-25aa-4c20-88ea-bb75150a2b55. len == 0.\n",
      "Skip uid 346a8e85-6aff-43d9-87c1-5884a4466686. len == 0.\n",
      "Skip uid 0d885f9d-8786-40d2-ab4b-d6874b5c6507. len == 0.\n",
      "Skip uid ffc09302-1833-46b3-a42b-0e4a8c52906f. len == 0.\n",
      "Skip uid 62a8c03b-e5ce-494a-bd8e-110d997b5f3d. len == 0.\n",
      "Skip uid cb921c91-f666-492d-82c2-5d8f2dbc98a2. len == 0.\n",
      "Skip uid 33c57960-0f43-49d1-97d3-0087df5c5569. len == 0.\n",
      "Skip uid c2017714-2177-48ca-a2d9-369eb8ae1780. len == 0.\n"
     ]
    }
   ],
   "source": [
    "for uid in tqdm(val_uids):\n",
    "    \n",
    "    # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(ref_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    fs = gen_fs\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep, gen_f0 = world_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep, gt_f0 = world_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_f0_dtw = gen_f0[twf[0]]\n",
    "    gt_f0_dtw = gt_f0[twf[1]]\n",
    "    \n",
    "    # Get voiced part\n",
    "    nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "    eps = 1e-7\n",
    "    gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs] + eps)\n",
    "    gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs] + eps)\n",
    "\n",
    "    if len(gen_f0_dtw_voiced) == 0 or len(gt_f0_dtw_voiced) == 0:\n",
    "        print(f\"Skip uid {uid}. len == 0.\")\n",
    "        continue\n",
    "\n",
    "    # log F0 RMSE\n",
    "    log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "    # print(f\"{uid} {log_f0_rmse:.4f}\")\n",
    "\n",
    "    # log F0 corr\n",
    "    log_f0_corr = np.corrcoef(gen_f0_dtw_voiced, gt_f0_dtw_voiced)[0][1]\n",
    "    # print(f\"{uid} {log_f0_corr:.4f}\")\n",
    "    uids.append(uid)\n",
    "    logf0_rmses.append(log_f0_rmse)\n",
    "    logf0_corrs.append(log_f0_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9655cbc-837e-4125-a7c8-2a648dceab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_logf0_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'logf0_rmse': logf0_rmses,\n",
    "    'logf0_corr': logf0_corrs,\n",
    "})\n",
    "lj_logf0_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\logF0_0629b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88959247-bcb3-4305-a2a9-3c0ac06aa766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2751, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lj_logf0_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be38f250-0a70-4460-8b3a-512bbef1089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2746, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lj_logf0_df.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a09c4fb1-6b6e-4382-bee8-2a6ceef54e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_logf0_df = lj_logf0_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7724bfa6-0856-4f4c-9983-91505dc1d3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>logf0_rmse</th>\n",
       "      <th>logf0_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9d58583c-20de-439d-b1cd-9c2265bdedd8</td>\n",
       "      <td>0.181933</td>\n",
       "      <td>-0.200166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4a506707-52ea-493e-98c4-f667e3222d44</td>\n",
       "      <td>0.368843</td>\n",
       "      <td>-0.594507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0b67f942-6ebf-403d-aff0-f4e5d62d3140</td>\n",
       "      <td>0.128017</td>\n",
       "      <td>0.742773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c237c6d5-f413-4e25-82e2-af7b408d390a</td>\n",
       "      <td>0.301658</td>\n",
       "      <td>-0.167710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b8ed5cf9-b8ff-4da3-a764-a59020b85277</td>\n",
       "      <td>0.497127</td>\n",
       "      <td>0.149655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>c1f709a3-3952-4f3f-9381-3a97d2b303bb</td>\n",
       "      <td>0.694469</td>\n",
       "      <td>0.209543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>b84c4dbf-c88a-43da-a5cb-aa2ced16e17d</td>\n",
       "      <td>0.827369</td>\n",
       "      <td>0.092270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>c7a54b50-6ca6-49cd-b048-abe1770b19c9</td>\n",
       "      <td>0.675416</td>\n",
       "      <td>0.182086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>a0c002a1-ce41-49bb-af4a-6df4dae2e2cd</td>\n",
       "      <td>0.330229</td>\n",
       "      <td>0.315930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>05ed8c62-f57f-4d16-adc0-9ea19cb950c8</td>\n",
       "      <td>0.235645</td>\n",
       "      <td>0.499652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2746 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uid  logf0_rmse  logf0_corr\n",
       "0     9d58583c-20de-439d-b1cd-9c2265bdedd8    0.181933   -0.200166\n",
       "1     4a506707-52ea-493e-98c4-f667e3222d44    0.368843   -0.594507\n",
       "2     0b67f942-6ebf-403d-aff0-f4e5d62d3140    0.128017    0.742773\n",
       "3     c237c6d5-f413-4e25-82e2-af7b408d390a    0.301658   -0.167710\n",
       "4     b8ed5cf9-b8ff-4da3-a764-a59020b85277    0.497127    0.149655\n",
       "...                                    ...         ...         ...\n",
       "2746  c1f709a3-3952-4f3f-9381-3a97d2b303bb    0.694469    0.209543\n",
       "2747  b84c4dbf-c88a-43da-a5cb-aa2ced16e17d    0.827369    0.092270\n",
       "2748  c7a54b50-6ca6-49cd-b048-abe1770b19c9    0.675416    0.182086\n",
       "2749  a0c002a1-ce41-49bb-af4a-6df4dae2e2cd    0.330229    0.315930\n",
       "2750  05ed8c62-f57f-4d16-adc0-9ea19cb950c8    0.235645    0.499652\n",
       "\n",
       "[2746 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lj_logf0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0fdfbb4-a164-42e5-a509-32c1838f6450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20016617, -0.59450695,  0.74277257, ...,  0.18208585,\n",
       "        0.31592969,  0.49965225])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lj_logf0_df['logf0_rmse'].values\n",
    "lj_logf0_df['logf0_corr'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d82afa41-4cde-4f65-923f-f3a107c6dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logf0_rmse mean on 0629b: 0.347476650443757\n",
      "logf0_rmse std on 0629b: 0.27812424555593845\n",
      "logf0_corr mean on 0629b: 0.30949771492752826\n",
      "logf0_corr std on 0629b: 0.4162518243168433\n"
     ]
    }
   ],
   "source": [
    "print(f\"logf0_rmse mean on 0629b: {torch.tensor(lj_logf0_df['logf0_rmse'].values).mean()}\")\n",
    "print(f\"logf0_rmse std on 0629b: {torch.tensor(lj_logf0_df['logf0_rmse'].values).std()}\")\n",
    "\n",
    "print(f\"logf0_corr mean on 0629b: {torch.tensor(lj_logf0_df['logf0_corr'].values).mean()}\")\n",
    "print(f\"logf0_corr std on 0629b: {torch.tensor(lj_logf0_df['logf0_corr'].values).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da9865-8d6e-4781-b919-82fe88ed0b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b2626-fc15-43de-9a74-493eb444bdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ed550-936d-454a-9248-07759371561e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d6e5b-3be8-4e38-9fc7-39959c7c6e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
