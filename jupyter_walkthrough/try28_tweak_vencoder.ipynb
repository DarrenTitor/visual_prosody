{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3347c6be-89f5-4649-be80-b11e3d8c563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477c7716-23bb-4c9f-868e-eb1d77fbefec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bcc18be-13dc-482e-90ac-27e219c64792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.SubLayers import MultiHeadAttention, MultiHeadAttention_VariableQuery, PositionwiseFeedForward\n",
    "\n",
    "from transformer.Layers import FFTBlock, VisualFFTBlock\n",
    "\n",
    "from transformer.Models import VisualEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4500f0b-2893-4380-bde7-8de65629b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded video_embeddings.shape: (batch_size, batch_max_seq_len, 1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d757a87c-ed47-493b-a203-46824afd2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import get_mask_from_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d85a458-be62-48b4-a452-ee462047fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/Ego4D_final_v6/0719a_preprocess.yaml -m ./config/Ego4D_final_v6/0719a_model.yaml -t ./config/Ego4D_final_v6/0719a_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2461145f-27bc-4180-907c-0700704d14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model, get_vocoder, get_param_num\n",
    "from utils.tools import to_device, log, synth_one_sample\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset, VideoDataset\n",
    "from utils.auto_tqdm import tqdm\n",
    "from evaluate import evaluate\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086aba5f-d6d9-4d43-8c5c-4d101a11eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a477585-bca9-40cc-a76d-a88e00115b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs\n",
    "# Get dataset\n",
    "dataset = VideoDataset(\n",
    "    \"train.txt\", 'train', preprocess_config, train_config, sort=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08bfc605-9d65-49bc-a53c-eba713b22c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27292"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cfa3171-76f6-49ae-bedb-211fd75e7884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119077d6-1966-4ac9-a330-99b3f62829e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "group_size = 4  # Set this larger than 1 to enable sorting in Dataset\n",
    "assert batch_size * group_size < len(dataset)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size * group_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321bf511-68a4-4c92-ae6a-887f5865ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return (\n",
    "#     ids,\n",
    "#     raw_texts,\n",
    "#     speakers,\n",
    "#     texts,\n",
    "#     text_lens,\n",
    "#     max(text_lens),\n",
    "#     mels,\n",
    "#     mel_lens,\n",
    "#     max(mel_lens),\n",
    "#     pitches,\n",
    "#     energies,\n",
    "#     durations,\n",
    "#     speaker_embeddings,\n",
    "#     video_embeddings,\n",
    "#     vid_lens,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1cb9580-f958-467d-b0a4-aeb6454c7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a77be13a-41da-4228-9aa5-a624f0e0d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2bc4d34-057d-48a3-8dd5-eabdfbc01f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 27, 1536)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43b6202b-7ac4-4c78-a5f2-803a875d071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using speaker embeddings.\n",
      "=> Using VarianceAdaptorWithSpeaker.\n",
      "=> Using VisualEncoder.\n",
      "Using prosody vector for visual encoder.\n",
      "Successfully loaded from ./output/LibriTTS/LibriTTS_800000.pth.tar\n",
      "Number of FastSpeech2 Parameters: 56828529\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)\n",
    "\n",
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ff62504-fa52-4b20-89b3-179b13bf88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "step = args.restore_step + 1\n",
    "epoch = 1\n",
    "grad_acc_step = train_config[\"optimizer\"][\"grad_acc_step\"]\n",
    "grad_clip_thresh = train_config[\"optimizer\"][\"grad_clip_thresh\"]\n",
    "total_step = train_config[\"step\"][\"total_step\"]\n",
    "log_step = train_config[\"step\"][\"log_step\"]\n",
    "save_step = train_config[\"step\"][\"save_step\"]\n",
    "synth_step = train_config[\"step\"][\"synth_step\"]\n",
    "val_step = train_config[\"step\"][\"val_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7404108-8e34-429a-83bc-c22695433fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batchs in enumerate(loader):\n",
    "    for batch in batchs:\n",
    "        batch = to_device(batch, device)\n",
    "        temp_batch = (batch[2:])\n",
    "        output = model(*(batch[2:]))\n",
    "    if batch_idx >=2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2f34a70-d83e-42ff-aef0-bcb3fdc87035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gpu_usage():\n",
    "    max_memory_allocated = torch.cuda.max_memory_allocated()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device_properties = torch.cuda.get_device_properties(device)\n",
    "    available_memory = device_properties.total_memory - torch.cuda.max_memory_allocated()\n",
    "    message = \"\"\n",
    "    message += f\"Maximum GPU memory allocated by PyTorch: {max_memory_allocated / 1024**3:.2f} GB\\n\"\n",
    "    message += f\"Available GPU memory: {available_memory / 1024**3:.2f} GB\\n\"\n",
    "    print(message)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d587901-23a1-44b9-a0d7-d9da35d47df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum GPU memory allocated by PyTorch: 4.98 GB\n",
      "Available GPU memory: 1.02 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = log_gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a394a8-58ed-48bf-a96f-2fdf9cdaa457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a7fc1-f71b-42b3-b715-657070fc5e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a3f87-d026-46f9-a183-64ae9365f57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c9e75-07e5-4f55-bda3-888cb461f5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202140e4-ec3c-403a-a506-c8e66bf01e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b17b8a-83f7-427d-be1a-380f8893e91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4e7a5-5790-42ff-b56a-3ef4810bf39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f714e1-8ab6-4d4a-95d9-319314c07b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8df21-4e09-4e49-93b4-1a3aeb558bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1049da6-cba3-49a1-b8eb-fddd4d179681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f06842-45cf-40e4-8a71-fd7bc869a726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2b82b-e8f8-435d-a0e6-89b87aa83682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55196f95-bb15-443f-ae4e-f914959176ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f348997-1333-4dc3-9df3-736d7027f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_out = torch.randn([4, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "162a1cea-33f1-4729-9567-ea3ce17cb0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(visual_out, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1b315-b3e8-4dd9-8d59-4ca17e75f01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eebbb10-762a-463a-a792-fba75f845d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_out = torch.randn([4, 36, 1536])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1886b580-6cf4-4a7c-92c9-ee89cc020f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_lens = torch.tensor([4, 36, 10, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ce53c6c-969c-4084-b587-bd059f1c1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pool_x = visual_out.sum(dim=1) / vid_lens.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c099f8c1-83a8-4547-a8ac-0e379cf42148",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool_x, _ = visual_out.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f72574f-9302-4f10-bdcb-a825a09470d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1536])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pool_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dc8af16-9ed4-4734-98d5-028d58fb1da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1536])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pool_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f36cede5-7190-483b-9b7d-67a7f4af15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgmax_x = torch.cat([avg_pool_x, max_pool_x], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ede1a2ac-9f64-453a-aff2-8c7b2f3694ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3072])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgmax_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44eb1981-0e6e-4f9f-b5da-b39fe7762789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0737, 1.0542, 1.0609, 1.0389])\n",
      "tensor([1.5047, 1.1141, 1.2054, 2.3611])\n",
      "tensor([5.1919, 3.8772, 4.3998, 8.7978])\n",
      "tensor([ -4.6305,  -0.5015,  -2.3392, -11.1474])\n"
     ]
    }
   ],
   "source": [
    "print(avgmax_x.mean(dim=1))\n",
    "print(avgmax_x.std(dim=1))\n",
    "print(avgmax_x.max(dim=1)[0])\n",
    "print(avgmax_x.min(dim=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1166e579-a57e-4239-a892-fb8c06c5c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = nn.LayerNorm(2*model_config[\"transformer\"][\"visual_encoder_hidden\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6566618e-b77c-4eae-a3a1-e4a85ccf27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgmax_x = ln(avgmax_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bcc7c15-a2ae-414e-a538-0f259b88bc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3072])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgmax_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51b619a6-5f8a-490a-8193-5493da63c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.5324e-08, -5.4017e-08,  2.0800e-08,  2.2973e-08],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([1.0002, 1.0002, 1.0002, 1.0002], grad_fn=<StdBackward0>)\n",
      "tensor([2.7373, 2.5342, 2.7703, 3.2867], grad_fn=<MaxBackward0>)\n",
      "tensor([-3.7915, -1.3967, -2.8211, -5.1621], grad_fn=<MinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(avgmax_x.mean(dim=1))\n",
    "print(avgmax_x.std(dim=1))\n",
    "print(avgmax_x.max(dim=1)[0])\n",
    "print(avgmax_x.min(dim=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5748be20-7d7f-4bb8-ae50-fdd925fd90c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3072])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgmax_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97aa5de-f897-4811-886e-29280b829437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf340724-3abe-4606-9b56-27e6e4e0879c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db566f8a-8e18-4914-885b-2170bf0818e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d257f6-2eeb-4f73-ae86-b0f6c7bd7e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02f9ec-6ae9-46f1-b616-3cd81899e67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d9363-efbd-4981-8b6a-613b7dd6c46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097a9de-a36c-49b0-8275-6c1e024b1bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7b669-e760-4aca-b0a3-8b0f796c70de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e88bc6-debc-4e60-bfea-7007ddf9e147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a901b00-2854-4140-9eeb-4abf81ce56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_lens = torch.tensor([4, 8, 10, 2])\n",
    "vid_max_len = vid_lens.max().item()\n",
    "vid_mask = get_mask_from_lengths(vid_lens.to(\"cuda\"), max_len=vid_max_len)\n",
    "vid_mask = vid_mask.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa00de76-6fc9-49e7-a16e-971979c802b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "BATCH_SEQ_LEN = vid_max_len\n",
    "EMBED_DIM = 1536\n",
    "QUERY_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "181f7afd-e5e2-402d-80e7-511220954b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_embeddings = torch.randn(BATCH_SIZE, BATCH_SEQ_LEN, EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5695f9bc-5aa3-462b-8ceb-7192963479a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 1536])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "068e37dc-769a-414e-9137-231a313b5647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c9b8286-968b-4b25-92ad-142b3949e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 2\n",
    "d_model = EMBED_DIM\n",
    "d_k = d_v = EMBED_DIM // n_head\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66a74228-d171-4b71-8b98-aed863b0b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "slf_attn_mask = vid_mask.unsqueeze(1).expand(-1, vid_max_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d3071fa-247b-4fe0-95a9-48e1c9596ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output, enc_slf_attn = slf_attn(\n",
    "    q=vid_embeddings, \n",
    "    k=vid_embeddings, \n",
    "    v=vid_embeddings, \n",
    "    mask=slf_attn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6aa55-a51d-48b2-8549-b15ade52cd64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6be9c3-51d4-47c4-a002-11185beeb821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbe8f981-00ed-4055-bc3f-8a8e143c3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_attn = MultiHeadAttention_VariableQuery(\n",
    "    n_head=n_head, \n",
    "    d_model=d_model, \n",
    "    d_k=d_k, \n",
    "    d_v=d_v, \n",
    "    q_input_dim=QUERY_DIM,\n",
    "    dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e67b853-8537-4e4d-bb82-643c925fafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_query = torch.randn(BATCH_SIZE, 1, QUERY_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96fba681-c95c-4c58-bf5c-c9af8cf20a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "671d2b6f-c5a8-4e7f-83a8-34579ba06f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_query.repeat(1, 10, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5adc7a59-d170-4722-a140-a625e5a1c559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 128])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_query.expand(-1, 10, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff932f04-b83f-46c2-a93f-ec89d273e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_query_repeated = vid_query.repeat(1, vid_max_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fa4c449-23e0-4322-95de-649833270269",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_query_expanded = vid_query.expand(-1, 10, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24d16227-0484-4559-88a4-e663543b601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slf_attn_mask = vid_mask.unsqueeze(1).expand(-1, vid_max_len, -1)\n",
    "\n",
    "enc_output, enc_slf_attn = vid_attn(\n",
    "    q=vid_query_expanded, \n",
    "    k=vid_embeddings, \n",
    "    v=vid_embeddings, \n",
    "    mask=slf_attn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62b0b553-96a5-4828-857b-24d4dcc91837",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_inner = 512\n",
    "kernel_size = [5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b51619d-d81e-4b87-be3d-62312045d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_block = FFTBlock(\n",
    "    n_head=n_head, \n",
    "    d_model=d_model, \n",
    "    d_k=d_k, \n",
    "    d_v=d_v, \n",
    "    # q_input_dim=QUERY_DIM,\n",
    "    d_inner=d_inner, \n",
    "    kernel_size=kernel_size, \n",
    "    dropout=dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ec1012d-f2b8-49bd-b9eb-52f7d9da64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = fft_block(vid_embeddings, mask=vid_mask, slf_attn_mask=slf_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27e09c64-cd1f-4d27-b031-290bae2706c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 1536])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19c56ebc-9a69-4261-980c-6ddbd056ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_block = VisualFFTBlock(\n",
    "    n_head=n_head, \n",
    "    d_model=d_model, \n",
    "    d_k=d_k, \n",
    "    d_v=d_v, \n",
    "    d_inner=d_inner, \n",
    "    kernel_size=kernel_size, \n",
    "    q_input_dim=QUERY_DIM,\n",
    "    dropout=dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8a0e2e0-2d17-4076-a504-a31acb74df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = vid_block(vid_embeddings, \n",
    "                   q_vec_expanded=vid_query_expanded,\n",
    "                   mask=vid_mask, \n",
    "                   slf_attn_mask=slf_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277cf1f-d0ea-4412-a92c-5e7af7858a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c31bce82-cc6c-4a68-824a-61bce3a0209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using self attention for visual encoder.\n"
     ]
    }
   ],
   "source": [
    "visual_encoder = VisualEncoder(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "181d5b10-73a2-4852-ae9e-7ce696b12215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5436e7e6-c014-4145-b66a-d742aa7037fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vecs = torch.randn(BATCH_SIZE, QUERY_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2daf7f95-3db3-4c6b-a324-3a029d56217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = visual_encoder(vid_embeddings, vid_mask, q_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e126f7-cf47-4011-8645-26413f492902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8955cf-2baf-4fa9-bf28-aa5025d6194e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
