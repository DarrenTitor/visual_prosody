{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a16189-62d5-402d-97f5-7af5b40c8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb57d2a-b3ec-440a-ae3b-fec0e50baef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aab3399-f244-4e61-aa13-e7618eda6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v5\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03fbd0e-823a-444b-8096-a0789f00a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2715\n"
     ]
    }
   ],
   "source": [
    "print(len(val_uids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d248553a-0cbb-4e65-90cf-0eb1ff2c42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model, get_vocoder, get_param_num, vocoder_infer\n",
    "from utils.tools import to_device, log, synth_one_sample, expand, plot_mel\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset\n",
    "# from utils.auto_tqdm import tqdm\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cff80e7-1769-4d7f-be5c-3c40cb2c31b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf10577-7186-4a81-8172-8b09bfa6f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/Ego4D_final_v5/0707a_preprocess.yaml -m ./config/Ego4D_final_v5/0707a_model.yaml -t ./config/Ego4D_final_v5/0707a_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63286c47-82e9-4109-8406-c2b870131b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(restore_step=0, preprocess_config='./config/Ego4D_final_v5/0707a_preprocess.yaml', model_config='./config/Ego4D_final_v5/0707a_model.yaml', train_config='./config/Ego4D_final_v5/0707a_train.yaml')\n",
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "pprint(args)\n",
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)\n",
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01719f11-0bef-4f18-a453-a64083eb4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/0707a/ckpt/Ego4D_final_v5/1200000.pth.tar'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5323f38-c578-4669-beca-5cf923197aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using speaker embeddings.\n",
      "True\n",
      "2\n",
      "=> Using VarianceAdaptorWithSpeaker.\n",
      "Number of FastSpeech2 Parameters: 35165553\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1778138b-f309-45fa-98b0-d06ea4d20746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)\n",
    "step = args.restore_step + 1\n",
    "model.eval()\n",
    "print()\n",
    "dataset = Dataset(\n",
    "    \"val.txt\", 'val', preprocess_config, train_config, sort=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a330702e-787f-4875-85a1-f96d1e0936e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2715"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06704bd2-0953-4848-8cb6-5298b6f66eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ab5d2a-24ff-4e35-8b8c-be12916ca53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "# Get loss function\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad546bb-5a6c-41ff-804c-5b03f762daec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/0707a/result/Ego4D_final_v5\\\\plots'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(train_config['path']['result_path'], 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13bf7822-dc02-40cc-a019-6102d702cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# => batch:\n",
    "# return (\n",
    "#     ids,\n",
    "#     raw_texts,\n",
    "#     speakers,\n",
    "#     texts,\n",
    "#     text_lens,\n",
    "#     max(text_lens),\n",
    "#     mels,\n",
    "#     mel_lens,\n",
    "#     max(mel_lens),\n",
    "#     pitches,\n",
    "#     energies,\n",
    "#     durations,\n",
    "#     speaker_embeddings,\n",
    "# )\n",
    "# [12] 对应speaker embedding\n",
    "\n",
    "\n",
    "# => model output / prediction\n",
    "# return (\n",
    "#     output,\n",
    "#     postnet_output,\n",
    "#     p_predictions,\n",
    "#     e_predictions,\n",
    "#     log_d_predictions,\n",
    "#     d_rounded,\n",
    "#     src_masks,\n",
    "#     mel_masks,\n",
    "#     src_lens,\n",
    "#     mel_lens,\n",
    "# )\n",
    "output_plot_path = os.path.join(train_config['path']['result_path'], 'plot')\n",
    "output_mel_syn_path = os.path.join(train_config['path']['result_path'], 'mel', 'syn')\n",
    "output_mel_gt_path = os.path.join(train_config['path']['result_path'], 'mel', 'gt')\n",
    "output_wav_syn_path = os.path.join(train_config['path']['result_path'], 'wav', 'synthesized')\n",
    "output_wav_rec_path = os.path.join(train_config['path']['result_path'], 'wav', 'reconstructed')\n",
    "os.makedirs(output_plot_path, exist_ok=True)\n",
    "os.makedirs(output_mel_syn_path, exist_ok=True)\n",
    "os.makedirs(output_mel_gt_path, exist_ok=True)\n",
    "os.makedirs(output_wav_syn_path, exist_ok=True)\n",
    "os.makedirs(output_wav_rec_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69aeaffd-ce50-4eba-81cb-08222aa67ff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd96b7fc8584b4183a2f02a6a9f6537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2715 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batchs in tqdm(loader):\n",
    "    for targets in batchs:\n",
    "        targets = to_device(targets, device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(*(targets[2:]))\n",
    "        basenames = targets[0]\n",
    "        for i in range(len(predictions[0])):\n",
    "            basename = basenames[i]\n",
    "            src_len = predictions[8][i].item()\n",
    "            mel_len = predictions[9][i].item()\n",
    "            mel_prediction = predictions[1][i, :mel_len].detach().transpose(0, 1)\n",
    "            mel_target = targets[6][i, :mel_len].detach().transpose(0, 1)\n",
    "\n",
    "            torch.save(mel_prediction.cpu(), os.path.join(output_mel_syn_path, f\"{basename}.pt\"))\n",
    "            torch.save(mel_target.cpu(), os.path.join(output_mel_gt_path, f\"{basename}.pt\"))\n",
    "            \n",
    "            \n",
    "            duration = predictions[5][i, :src_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n",
    "                pitch = predictions[2][i, :src_len].detach().cpu().numpy()\n",
    "                pitch = expand(pitch, duration)\n",
    "            else:\n",
    "                pitch = predictions[2][i, :mel_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"] == \"phoneme_level\":\n",
    "                energy = predictions[3][i, :src_len].detach().cpu().numpy()\n",
    "                energy = expand(energy, duration)\n",
    "            else:\n",
    "                energy = predictions[3][i, :mel_len].detach().cpu().numpy()\n",
    "\n",
    "            with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \n",
    "                                   \"stats.json\")) as f:\n",
    "                stats = json.load(f)\n",
    "                stats = stats[\"pitch\"] + stats[\"energy\"][:2]\n",
    "                                       \n",
    "            fig = plot_mel(\n",
    "                [\n",
    "                    (mel_prediction.cpu().numpy(), pitch, energy),\n",
    "                    (mel_target.cpu().numpy(), pitch, energy),\n",
    "                ],\n",
    "                stats,\n",
    "                [\"Synthetized Spectrogram\", \"Ground-Truth Spectrogram\"],\n",
    "            )\n",
    "            ### TODO: change to svg\n",
    "            plt.savefig(os.path.join(output_plot_path, f\"{basename}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # from .model import vocoder_infer\n",
    "\n",
    "        mel_predictions = predictions[1].transpose(1, 2)\n",
    "        mel_targets = targets[6].transpose(1, 2)\n",
    "        \n",
    "        lengths = predictions[9] * preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "        wav_predictions = vocoder_infer(\n",
    "            mel_predictions, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "        )\n",
    "        wav_targets = vocoder_infer(\n",
    "        mel_targets, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "    )\n",
    "    \n",
    "        sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "        for wav, basename in zip(wav_predictions, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_syn_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "        for wav, basename in zip(wav_targets, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_rec_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "\n",
    "        \n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637562f6-a7a5-40ba-be53-28db41f56e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763cba9-43a3-4e94-a956-0774ae75780d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba41df21-7834-4eda-b22c-b564d7bf0b3b",
   "metadata": {},
   "source": [
    "# MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8495893-d392-4f90-ae74-d8ff2f21eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import os.path as op\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import argparse\n",
    "import fnmatch\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import librosa\n",
    "import pysptk\n",
    "import pyworld as pw\n",
    "import soundfile as sf\n",
    "from fastdtw import fastdtw\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f83927f-ef88-48c5-96f4-3d54701a3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v5\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17074280-94fc-47ee-9bad-8f7320775659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2715"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22a72f03-a455-4b9e-afe0-dcc0d43a523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_folder = r'.\\output\\0707a\\result\\Ego4D_final_v5\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0707a\\result\\Ego4D_final_v5\\wav\\synthesized'\n",
    "gt_folder = r'.\\raw_data\\Ego4D_final_v5\\val\\Ego4D_final_v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba34847b-3048-4565-bc8f-a614ab49523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "melkwargs = {\n",
    "    \"n_fft\": int(0.05 * sr), \"win_length\": int(0.05 * sr),\n",
    "    \"hop_length\": int(0.0125 * sr), \"f_min\": 20,\n",
    "    \"n_mels\": 80, \"window_fn\": torch.hann_window\n",
    "}\n",
    "mfcc_fn = torchaudio.transforms.MFCC(\n",
    "    sr, n_mfcc=13, log_mels=True, melkwargs=melkwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7735db5-9416-4d8f-8429-03ecc6ae037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ea0e4bfa74439d8f08da62c7117c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2715 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 27s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uids = []\n",
    "MCDs_recon = []\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(recon_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs_recon.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dff90d39-1b53-4124-906f-022c866ddf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f727694d514f443e9dce0d6559937c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2715 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11min 19s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uids = []\n",
    "MCDs_gt = []\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(gt_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs_gt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c848bce7-a5dc-43ba-bb55-d7896cbdaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD_recon': MCDs_recon,\n",
    "    'MCD_gt': MCDs_gt,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "794c3505-1e1d-4026-8349-4409593faec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_0707a_1M2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2228018e-5c2b-4da6-8a68-9076f2509c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCDs_recon mean on 0707a_1M2: 15.63803768157959\n",
      "MCDs_recon std on 0707a_1M2: 2.9097180366516113\n",
      "MCDs_gt mean on 0707a_1M2: 16.158737182617188\n",
      "MCDs_gt std on 0707a_1M2: 2.9617269039154053\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCDs_recon mean on 0707a_1M2: {torch.tensor(MCDs_recon).mean()}\")\n",
    "print(f\"MCDs_recon std on 0707a_1M2: {torch.tensor(MCDs_recon).std()}\")\n",
    "\n",
    "print(f\"MCDs_gt mean on 0707a_1M2: {torch.tensor(MCDs_gt).mean()}\")\n",
    "print(f\"MCDs_gt std on 0707a_1M2: {torch.tensor(MCDs_gt).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f940da-2430-48c8-89f0-272725f514a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>MCD_recon</th>\n",
       "      <th>MCD_gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>0e4ae9e1-640b-45d1-90b2-17b4e941322d</td>\n",
       "      <td>7.243025</td>\n",
       "      <td>10.131522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>a6d4dbc4-969d-4819-af85-f3b3cb56195e</td>\n",
       "      <td>7.783217</td>\n",
       "      <td>13.009296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>af8de660-f916-4a2c-92ac-4e767436ed2c</td>\n",
       "      <td>8.237475</td>\n",
       "      <td>8.663778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1b66fe95-6176-4ee1-98ba-984c9bfe1f5f</td>\n",
       "      <td>8.497393</td>\n",
       "      <td>8.395654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>2667f8d5-30e0-44da-80ce-27df506a8a82</td>\n",
       "      <td>8.592734</td>\n",
       "      <td>10.184369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uid  MCD_recon     MCD_gt\n",
       "2370  0e4ae9e1-640b-45d1-90b2-17b4e941322d   7.243025  10.131522\n",
       "107   a6d4dbc4-969d-4819-af85-f3b3cb56195e   7.783217  13.009296\n",
       "444   af8de660-f916-4a2c-92ac-4e767436ed2c   8.237475   8.663778\n",
       "373   1b66fe95-6176-4ee1-98ba-984c9bfe1f5f   8.497393   8.395654\n",
       "1749  2667f8d5-30e0-44da-80ce-27df506a8a82   8.592734  10.184369"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v5_mcd_df.nsmallest(5, 'MCD_recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd4cd3da-c0dc-4fc9-a9e9-f6244b5393a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>MCD_recon</th>\n",
       "      <th>MCD_gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>c2b61a6a-1e3f-4ab7-a207-c8e748b4d330</td>\n",
       "      <td>30.934711</td>\n",
       "      <td>26.058529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>898b49a9-5b65-49f8-bbca-c0f784f80b98</td>\n",
       "      <td>28.250639</td>\n",
       "      <td>15.672288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>dc0bb287-5310-44a9-b491-b195d98fd53e</td>\n",
       "      <td>27.944120</td>\n",
       "      <td>20.941483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>3ac7d90c-3ef3-4dbe-b818-dd2583a18070</td>\n",
       "      <td>27.534554</td>\n",
       "      <td>26.702335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>ec1ee52c-587c-4f8d-8760-52e6ba6019d3</td>\n",
       "      <td>26.466930</td>\n",
       "      <td>25.490271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uid  MCD_recon     MCD_gt\n",
       "1625  c2b61a6a-1e3f-4ab7-a207-c8e748b4d330  30.934711  26.058529\n",
       "68    898b49a9-5b65-49f8-bbca-c0f784f80b98  28.250639  15.672288\n",
       "2632  dc0bb287-5310-44a9-b491-b195d98fd53e  27.944120  20.941483\n",
       "1866  3ac7d90c-3ef3-4dbe-b818-dd2583a18070  27.534554  26.702335\n",
       "555   ec1ee52c-587c-4f8d-8760-52e6ba6019d3  26.466930  25.490271"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v5_mcd_df.nlargest(5, 'MCD_recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b77752-af70-42f5-b81d-dfcf4d1ec9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d9096-dc49-48f9-9ef1-71f92e810a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6dae3f-dc62-4408-a412-94882cc5380d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2f533-138a-44de-83f3-533f93c0d5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac81cc-b6e4-4558-8e8b-85d8734030be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90207c4f-65b6-4342-8d68-384b64e86408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd59d35-8b94-4e61-881c-ad6a3a225abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02333a5-9901-4c0a-b7ef-34d63cc0c3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b3532-395e-4cc8-97fa-bda7265a33a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc5632-b228-4ace-b7ed-8944cf45392d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3680f-de66-41bb-b2f0-7106b37b351e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e2424-5ca6-4ede-b0c8-34a544edf04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44bbcd-9010-42c3-bb31-530fe0b2e705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62791286-cd0d-460b-8fb2-1eb8a3f81087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71f88e9f-a767-4a8c-a3f8-204f702fb9fe",
   "metadata": {},
   "source": [
    "# inference on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4258248a-894d-4490-9ae8-8aeeb4439cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model, get_vocoder, get_param_num, vocoder_infer\n",
    "from utils.tools import to_device, log, synth_one_sample, expand, plot_mel\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset\n",
    "# from utils.auto_tqdm import tqdm\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bfd3265-a10d-48d3-91a8-707e17e92beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958bb879-26ed-4738-96d3-539abc0bc78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/Ego4D_final_v5/0707a_preprocess.yaml -m ./config/Ego4D_final_v5/0707a_model.yaml -t ./config/Ego4D_final_v5/0707a_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9f9a13-d6df-4fca-a2fd-b7bd0be2da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(restore_step=0, preprocess_config='./config/Ego4D_final_v5/0707a_preprocess.yaml', model_config='./config/Ego4D_final_v5/0707a_model.yaml', train_config='./config/Ego4D_final_v5/0707a_train.yaml')\n",
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "pprint(args)\n",
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)\n",
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa67bc3c-2e71-4762-809e-4046bb396565",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/0707a/ckpt/Ego4D_final_v5/1200000.pth.tar'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58201ee-6295-4a44-9c55-d0b4d660e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using speaker embeddings.\n",
      "True\n",
      "2\n",
      "=> Using VarianceAdaptorWithSpeaker.\n",
      "Number of FastSpeech2 Parameters: 35165553\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e68cf8b2-9d42-408d-8b97-ee1964e40a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)\n",
    "step = args.restore_step + 1\n",
    "model.eval()\n",
    "print()\n",
    "dataset = Dataset(\n",
    "    \"train.txt\", 'train', preprocess_config, train_config, sort=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ab27037-9451-41a8-9a7c-790fba92f1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28010"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e64bbe3-3279-45ed-8b9f-2d1f2065b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88fae644-04b4-4098-a8bb-e4b23adb2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "# Get loss function\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f505f6-156a-458a-8db8-b20b0fd80da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# => batch:\n",
    "# return (\n",
    "#     ids,\n",
    "#     raw_texts,\n",
    "#     speakers,\n",
    "#     texts,\n",
    "#     text_lens,\n",
    "#     max(text_lens),\n",
    "#     mels,\n",
    "#     mel_lens,\n",
    "#     max(mel_lens),\n",
    "#     pitches,\n",
    "#     energies,\n",
    "#     durations,\n",
    "#     speaker_embeddings,\n",
    "# )\n",
    "# [12] 对应speaker embedding\n",
    "\n",
    "\n",
    "# => model output / prediction\n",
    "# return (\n",
    "#     output,\n",
    "#     postnet_output,\n",
    "#     p_predictions,\n",
    "#     e_predictions,\n",
    "#     log_d_predictions,\n",
    "#     d_rounded,\n",
    "#     src_masks,\n",
    "#     mel_masks,\n",
    "#     src_lens,\n",
    "#     mel_lens,\n",
    "# )\n",
    "output_plot_path = os.path.join(train_config['path']['result_path'], 'train', 'plot')\n",
    "output_mel_syn_path = os.path.join(train_config['path']['result_path'], 'train', 'mel', 'syn')\n",
    "output_mel_gt_path = os.path.join(train_config['path']['result_path'], 'train', 'mel', 'gt')\n",
    "output_wav_syn_path = os.path.join(train_config['path']['result_path'], 'train', 'wav', 'synthesized')\n",
    "output_wav_rec_path = os.path.join(train_config['path']['result_path'], 'train', 'wav', 'reconstructed')\n",
    "os.makedirs(output_plot_path, exist_ok=True)\n",
    "os.makedirs(output_mel_syn_path, exist_ok=True)\n",
    "os.makedirs(output_mel_gt_path, exist_ok=True)\n",
    "os.makedirs(output_wav_syn_path, exist_ok=True)\n",
    "os.makedirs(output_wav_rec_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3d6038c-d939-4ea9-8d9a-ada145cd492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batchs in tqdm(loader):\n",
    "#     for targets in batchs:\n",
    "#         targets = to_device(targets, device)\n",
    "#         with torch.no_grad():\n",
    "#             predictions = model(*(targets[2:]))\n",
    "#         basenames = targets[0]\n",
    "#         for i in range(len(predictions[0])):\n",
    "#             basename = basenames[i]\n",
    "#             src_len = predictions[8][i].item()\n",
    "#             mel_len = predictions[9][i].item()\n",
    "#             mel_prediction = predictions[1][i, :mel_len].detach().transpose(0, 1)\n",
    "#             mel_target = targets[6][i, :mel_len].detach().transpose(0, 1)\n",
    "\n",
    "#             torch.save(mel_prediction.cpu(), os.path.join(output_mel_syn_path, f\"{basename}.pt\"))\n",
    "#             torch.save(mel_target.cpu(), os.path.join(output_mel_gt_path, f\"{basename}.pt\"))\n",
    "            \n",
    "            \n",
    "#             duration = predictions[5][i, :src_len].detach().cpu().numpy()\n",
    "#             if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n",
    "#                 pitch = predictions[2][i, :src_len].detach().cpu().numpy()\n",
    "#                 pitch = expand(pitch, duration)\n",
    "#             else:\n",
    "#                 pitch = predictions[2][i, :mel_len].detach().cpu().numpy()\n",
    "#             if preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"] == \"phoneme_level\":\n",
    "#                 energy = predictions[3][i, :src_len].detach().cpu().numpy()\n",
    "#                 energy = expand(energy, duration)\n",
    "#             else:\n",
    "#                 energy = predictions[3][i, :mel_len].detach().cpu().numpy()\n",
    "\n",
    "#             with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \n",
    "#                                    \"stats.json\")) as f:\n",
    "#                 stats = json.load(f)\n",
    "#                 stats = stats[\"pitch\"] + stats[\"energy\"][:2]\n",
    "                                       \n",
    "#             fig = plot_mel(\n",
    "#                 [\n",
    "#                     (mel_prediction.cpu().numpy(), pitch, energy),\n",
    "#                     (mel_target.cpu().numpy(), pitch, energy),\n",
    "#                 ],\n",
    "#                 stats,\n",
    "#                 [\"Synthetized Spectrogram\", \"Ground-Truth Spectrogram\"],\n",
    "#             )\n",
    "#             ### TODO: change to svg\n",
    "#             plt.savefig(os.path.join(output_plot_path, f\"{basename}.png\"))\n",
    "#             plt.close()\n",
    "\n",
    "#         # from .model import vocoder_infer\n",
    "\n",
    "#         mel_predictions = predictions[1].transpose(1, 2)\n",
    "#         mel_targets = targets[6].transpose(1, 2)\n",
    "        \n",
    "#         lengths = predictions[9] * preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "#         wav_predictions = vocoder_infer(\n",
    "#             mel_predictions, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "#         )\n",
    "#         wav_targets = vocoder_infer(\n",
    "#         mel_targets, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "#     )\n",
    "    \n",
    "#         sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "#         for wav, basename in zip(wav_predictions, basenames):\n",
    "#             wavfile.write(os.path.join(output_wav_syn_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "#         for wav, basename in zip(wav_targets, basenames):\n",
    "#             wavfile.write(os.path.join(output_wav_rec_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "\n",
    "        \n",
    "#     #     break\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "956e6189-0dc8-4fc5-833f-e3374cb66adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fdc251351a49a1974ee0862b6c3cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, batchs in tqdm(enumerate(loader)):\n",
    "    if idx < 27760:\n",
    "        continue\n",
    "    for targets in batchs:\n",
    "        targets = to_device(targets, device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(*(targets[2:]))\n",
    "        basenames = targets[0]\n",
    "        for i in range(len(predictions[0])):\n",
    "            basename = basenames[i]\n",
    "            src_len = predictions[8][i].item()\n",
    "            mel_len = predictions[9][i].item()\n",
    "            mel_prediction = predictions[1][i, :mel_len].detach().transpose(0, 1)\n",
    "            mel_target = targets[6][i, :mel_len].detach().transpose(0, 1)\n",
    "\n",
    "            torch.save(mel_prediction.cpu(), os.path.join(output_mel_syn_path, f\"{basename}.pt\"))\n",
    "            torch.save(mel_target.cpu(), os.path.join(output_mel_gt_path, f\"{basename}.pt\"))\n",
    "            \n",
    "            \n",
    "            duration = predictions[5][i, :src_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n",
    "                pitch = predictions[2][i, :src_len].detach().cpu().numpy()\n",
    "                pitch = expand(pitch, duration)\n",
    "            else:\n",
    "                pitch = predictions[2][i, :mel_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"] == \"phoneme_level\":\n",
    "                energy = predictions[3][i, :src_len].detach().cpu().numpy()\n",
    "                energy = expand(energy, duration)\n",
    "            else:\n",
    "                energy = predictions[3][i, :mel_len].detach().cpu().numpy()\n",
    "\n",
    "            with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \n",
    "                                   \"stats.json\")) as f:\n",
    "                stats = json.load(f)\n",
    "                stats = stats[\"pitch\"] + stats[\"energy\"][:2]\n",
    "                                       \n",
    "            fig = plot_mel(\n",
    "                [\n",
    "                    (mel_prediction.cpu().numpy(), pitch, energy),\n",
    "                    (mel_target.cpu().numpy(), pitch, energy),\n",
    "                ],\n",
    "                stats,\n",
    "                [\"Synthetized Spectrogram\", \"Ground-Truth Spectrogram\"],\n",
    "            )\n",
    "            ### TODO: change to svg\n",
    "            plt.savefig(os.path.join(output_plot_path, f\"{basename}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # from .model import vocoder_infer\n",
    "\n",
    "        mel_predictions = predictions[1].transpose(1, 2)\n",
    "        mel_targets = targets[6].transpose(1, 2)\n",
    "        \n",
    "        lengths = predictions[9] * preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "        wav_predictions = vocoder_infer(\n",
    "            mel_predictions, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "        )\n",
    "        wav_targets = vocoder_infer(\n",
    "        mel_targets, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "    )\n",
    "    \n",
    "        sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "        for wav, basename in zip(wav_predictions, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_syn_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "        for wav, basename in zip(wav_targets, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_rec_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "\n",
    "        \n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "749fd0f8-c4ad-47c4-9452-3f480ffdc85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111\n"
     ]
    }
   ],
   "source": [
    "print(11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb3e38-22ad-42fc-a522-e320866f2b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ab2b5-0ae9-4e8f-9f8b-5a2e0ec78e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32381965-ec69-4887-9d78-afd44eaeb36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d792894b-deb8-4b94-a215-2c9aa5a93d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb9673-300a-4405-9bcb-b70dacba04b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbd10c-29a2-4b89-a8ff-5ff3a8d2ab59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6c2d8-24d4-4d3f-a0df-aa19a7fe9be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e4df7d0-84a6-48e9-91ae-a62031c7f4a0",
   "metadata": {},
   "source": [
    "# MCD on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0278646-a7e0-40a9-945b-9efaf1d169fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_train_path = r'.\\preprocessed_data\\Ego4D_final_v5\\train.txt'\n",
    "train_uids = []\n",
    "with open(split_txt_train_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        train_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f242f940-f936-4435-a21c-6a7754ed5581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28010"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b664fcd0-0739-4572-85f4-dcd82ac44b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_folder = r'.\\output\\0707a\\result\\Ego4D_final_v5\\train\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0707a\\result\\Ego4D_final_v5\\train\\wav\\synthesized'\n",
    "gt_folder = r'.\\raw_data\\Ego4D_final_v5\\train\\Ego4D_final_v5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b448eecd-4397-4aae-8dc7-da0f5261cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "melkwargs = {\n",
    "    \"n_fft\": int(0.05 * sr), \"win_length\": int(0.05 * sr),\n",
    "    \"hop_length\": int(0.0125 * sr), \"f_min\": 20,\n",
    "    \"n_mels\": 80, \"window_fn\": torch.hann_window\n",
    "}\n",
    "mfcc_fn = torchaudio.transforms.MFCC(\n",
    "    sr, n_mfcc=13, log_mels=True, melkwargs=melkwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbd1a70c-8cf4-4ffb-8e2f-586741cd3112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64321c6857eb4b0c87dee9483a7559c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 9min 59s\n",
      "Wall time: 21min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uids = []\n",
    "MCDs_recon = []\n",
    "for uid in tqdm(train_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(recon_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs_recon.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49081803-1827-4c14-85d5-01bda38a7016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc8e927c4c04466ac97eb77b0fa184d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2h 16min 37s\n",
      "Wall time: 23min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uids = []\n",
    "MCDs_gt = []\n",
    "for uid in tqdm(train_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(gt_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs_gt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3482bf4b-960e-4437-a194-8bf41cabff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD_recon': MCDs_recon,\n",
    "    'MCD_gt': MCDs_gt,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fc236e5-4e16-4665-aa08-8770eea7f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_0707a_1M2_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c143b162-1b4a-40bb-b06b-50aa0927ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCDs_recon mean on 0707a_1M2_train: 9.539996147155762\n",
      "MCDs_recon std on 0707a_1M2_train: 1.7698731422424316\n",
      "MCDs_gt mean on 0707a_1M2_train: 11.003610610961914\n",
      "MCDs_gt std on 0707a_1M2_train: 2.176541566848755\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCDs_recon mean on 0707a_1M2_train: {torch.tensor(MCDs_recon).mean()}\")\n",
    "print(f\"MCDs_recon std on 0707a_1M2_train: {torch.tensor(MCDs_recon).std()}\")\n",
    "\n",
    "print(f\"MCDs_gt mean on 0707a_1M2_train: {torch.tensor(MCDs_gt).mean()}\")\n",
    "print(f\"MCDs_gt std on 0707a_1M2_train: {torch.tensor(MCDs_gt).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fc644fc-f098-4ef6-acf5-5259d7fd84bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>MCD_recon</th>\n",
       "      <th>MCD_gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17652</th>\n",
       "      <td>36942da8-0899-4eb9-97a0-00832dc652cc</td>\n",
       "      <td>3.679867</td>\n",
       "      <td>13.465104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13537</th>\n",
       "      <td>6e3331e3-01dd-4b4e-9718-6877ca793ca3</td>\n",
       "      <td>3.785106</td>\n",
       "      <td>16.777239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>009e6bb2-3d56-4644-8c4a-09503518a22c</td>\n",
       "      <td>3.970977</td>\n",
       "      <td>11.511535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>78bed0db-eaa5-4bdf-b583-e3306e7d8f61</td>\n",
       "      <td>3.979556</td>\n",
       "      <td>9.241287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24610</th>\n",
       "      <td>3041a0da-8b32-43a8-b8fa-4c2674df353a</td>\n",
       "      <td>4.017649</td>\n",
       "      <td>12.608796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        uid  MCD_recon     MCD_gt\n",
       "17652  36942da8-0899-4eb9-97a0-00832dc652cc   3.679867  13.465104\n",
       "13537  6e3331e3-01dd-4b4e-9718-6877ca793ca3   3.785106  16.777239\n",
       "9348   009e6bb2-3d56-4644-8c4a-09503518a22c   3.970977  11.511535\n",
       "123    78bed0db-eaa5-4bdf-b583-e3306e7d8f61   3.979556   9.241287\n",
       "24610  3041a0da-8b32-43a8-b8fa-4c2674df353a   4.017649  12.608796"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v5_mcd_df.nsmallest(5, 'MCD_recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c19b4e3-42ea-4df3-a13c-be17a24863e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>MCD_recon</th>\n",
       "      <th>MCD_gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>55b72a13-9e0a-4e0e-9289-fd14ab24e627</td>\n",
       "      <td>21.148561</td>\n",
       "      <td>20.978930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>797cc95a-9062-4bc6-b28d-d3100a6ad817</td>\n",
       "      <td>20.886789</td>\n",
       "      <td>17.400734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>b69e0da9-dd12-4394-8727-c75a3c15d9fb</td>\n",
       "      <td>20.741346</td>\n",
       "      <td>20.559359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>a40fce22-6afc-42b6-9101-306fd14ca3d6</td>\n",
       "      <td>19.676283</td>\n",
       "      <td>21.359287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>5248ebe3-4905-4799-82b2-410205c8509a</td>\n",
       "      <td>19.167067</td>\n",
       "      <td>19.473600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uid  MCD_recon     MCD_gt\n",
       "1841  55b72a13-9e0a-4e0e-9289-fd14ab24e627  21.148561  20.978930\n",
       "1318  797cc95a-9062-4bc6-b28d-d3100a6ad817  20.886789  17.400734\n",
       "1875  b69e0da9-dd12-4394-8727-c75a3c15d9fb  20.741346  20.559359\n",
       "1425  a40fce22-6afc-42b6-9101-306fd14ca3d6  19.676283  21.359287\n",
       "1488  5248ebe3-4905-4799-82b2-410205c8509a  19.167067  19.473600"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v5_mcd_df.nlargest(5, 'MCD_recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ee520-d8c2-4f49-ba78-7ec3f8b75fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b2f8e-dcb1-4540-8f05-5369a7b891e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6426a3-f644-4ec5-be3d-497f1fa1c57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c58460-9df7-49b4-8225-65e3ed31bf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9333348-7e37-4398-ac61-7134be75d5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
