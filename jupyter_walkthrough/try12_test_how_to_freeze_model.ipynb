{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e9b3c2-342d-496c-b157-c6afc463706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d0193f-b40d-42bf-9f99-ce4d5b421cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435fdbef-ca05-4478-86ec-bc82c78fff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v6\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d56af2-9555-4891-aacd-a2b32014273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2772\n"
     ]
    }
   ],
   "source": [
    "print(len(val_uids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d74d31-1f39-4f63-b8e9-582f1d88045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model, get_vocoder, get_param_num, vocoder_infer\n",
    "from utils.tools import to_device, log, synth_one_sample, expand, plot_mel\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset\n",
    "# from utils.auto_tqdm import tqdm\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f01e25f-205d-4104-a4c9-149af704e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81839baa-76bc-4132-b8ca-00d2ea735dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/Ego4D_final_v6/0703a_preprocess.yaml -m ./config/Ego4D_final_v6/0703a_model.yaml -t ./config/Ego4D_final_v6/0703a_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61edd7dc-22dd-47d9-9360-06c0ca9c868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(restore_step=0, preprocess_config='./config/Ego4D_final_v6/0703a_preprocess.yaml', model_config='./config/Ego4D_final_v6/0703a_model.yaml', train_config='./config/Ego4D_final_v6/0703a_train.yaml')\n",
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "pprint(args)\n",
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)\n",
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6056b9-7868-4389-83b5-5094054c94a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/LibriTTS/LibriTTS_800000.pth.tar'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b1a795d-2669-45ba-93c8-2265bfe5ad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoder.position_enc',\n",
      " 'encoder.src_word_emb.weight',\n",
      " 'encoder.layer_stack.0.slf_attn.w_qs.weight',\n",
      " 'encoder.layer_stack.0.slf_attn.w_qs.bias',\n",
      " 'encoder.layer_stack.0.slf_attn.w_ks.weight',\n",
      " 'encoder.layer_stack.0.slf_attn.w_ks.bias',\n",
      " 'encoder.layer_stack.0.slf_attn.w_vs.weight',\n",
      " 'encoder.layer_stack.0.slf_attn.w_vs.bias',\n",
      " 'encoder.layer_stack.0.slf_attn.layer_norm.weight',\n",
      " 'encoder.layer_stack.0.slf_attn.layer_norm.bias',\n",
      " 'encoder.layer_stack.0.slf_attn.fc.weight',\n",
      " 'encoder.layer_stack.0.slf_attn.fc.bias',\n",
      " 'encoder.layer_stack.0.pos_ffn.w_1.weight',\n",
      " 'encoder.layer_stack.0.pos_ffn.w_1.bias',\n",
      " 'encoder.layer_stack.0.pos_ffn.w_2.weight',\n",
      " 'encoder.layer_stack.0.pos_ffn.w_2.bias',\n",
      " 'encoder.layer_stack.0.pos_ffn.layer_norm.weight',\n",
      " 'encoder.layer_stack.0.pos_ffn.layer_norm.bias',\n",
      " 'encoder.layer_stack.1.slf_attn.w_qs.weight',\n",
      " 'encoder.layer_stack.1.slf_attn.w_qs.bias',\n",
      " 'encoder.layer_stack.1.slf_attn.w_ks.weight',\n",
      " 'encoder.layer_stack.1.slf_attn.w_ks.bias',\n",
      " 'encoder.layer_stack.1.slf_attn.w_vs.weight',\n",
      " 'encoder.layer_stack.1.slf_attn.w_vs.bias',\n",
      " 'encoder.layer_stack.1.slf_attn.layer_norm.weight',\n",
      " 'encoder.layer_stack.1.slf_attn.layer_norm.bias',\n",
      " 'encoder.layer_stack.1.slf_attn.fc.weight',\n",
      " 'encoder.layer_stack.1.slf_attn.fc.bias',\n",
      " 'encoder.layer_stack.1.pos_ffn.w_1.weight',\n",
      " 'encoder.layer_stack.1.pos_ffn.w_1.bias',\n",
      " 'encoder.layer_stack.1.pos_ffn.w_2.weight',\n",
      " 'encoder.layer_stack.1.pos_ffn.w_2.bias',\n",
      " 'encoder.layer_stack.1.pos_ffn.layer_norm.weight',\n",
      " 'encoder.layer_stack.1.pos_ffn.layer_norm.bias',\n",
      " 'encoder.layer_stack.2.slf_attn.w_qs.weight',\n",
      " 'encoder.layer_stack.2.slf_attn.w_qs.bias',\n",
      " 'encoder.layer_stack.2.slf_attn.w_ks.weight',\n",
      " 'encoder.layer_stack.2.slf_attn.w_ks.bias',\n",
      " 'encoder.layer_stack.2.slf_attn.w_vs.weight',\n",
      " 'encoder.layer_stack.2.slf_attn.w_vs.bias',\n",
      " 'encoder.layer_stack.2.slf_attn.layer_norm.weight',\n",
      " 'encoder.layer_stack.2.slf_attn.layer_norm.bias',\n",
      " 'encoder.layer_stack.2.slf_attn.fc.weight',\n",
      " 'encoder.layer_stack.2.slf_attn.fc.bias',\n",
      " 'encoder.layer_stack.2.pos_ffn.w_1.weight',\n",
      " 'encoder.layer_stack.2.pos_ffn.w_1.bias',\n",
      " 'encoder.layer_stack.2.pos_ffn.w_2.weight',\n",
      " 'encoder.layer_stack.2.pos_ffn.w_2.bias',\n",
      " 'encoder.layer_stack.2.pos_ffn.layer_norm.weight',\n",
      " 'encoder.layer_stack.2.pos_ffn.layer_norm.bias',\n",
      " 'encoder.layer_stack.3.slf_attn.w_qs.weight',\n",
      " 'encoder.layer_stack.3.slf_attn.w_qs.bias',\n",
      " 'encoder.layer_stack.3.slf_attn.w_ks.weight',\n",
      " 'encoder.layer_stack.3.slf_attn.w_ks.bias',\n",
      " 'encoder.layer_stack.3.slf_attn.w_vs.weight',\n",
      " 'encoder.layer_stack.3.slf_attn.w_vs.bias',\n",
      " 'encoder.layer_stack.3.slf_attn.layer_norm.weight',\n",
      " 'encoder.layer_stack.3.slf_attn.layer_norm.bias',\n",
      " 'encoder.layer_stack.3.slf_attn.fc.weight',\n",
      " 'encoder.layer_stack.3.slf_attn.fc.bias',\n",
      " 'encoder.layer_stack.3.pos_ffn.w_1.weight',\n",
      " 'encoder.layer_stack.3.pos_ffn.w_1.bias',\n",
      " 'encoder.layer_stack.3.pos_ffn.w_2.weight',\n",
      " 'encoder.layer_stack.3.pos_ffn.w_2.bias',\n",
      " 'encoder.layer_stack.3.pos_ffn.layer_norm.weight',\n",
      " 'encoder.layer_stack.3.pos_ffn.layer_norm.bias',\n",
      " 'variance_adaptor.pitch_bins',\n",
      " 'variance_adaptor.energy_bins',\n",
      " 'variance_adaptor.duration_predictor.conv_layer.conv1d_1.conv.weight',\n",
      " 'variance_adaptor.duration_predictor.conv_layer.conv1d_1.conv.bias',\n",
      " 'variance_adaptor.duration_predictor.conv_layer.layer_norm_1.weight',\n",
      " 'variance_adaptor.duration_predictor.conv_layer.layer_norm_1.bias',\n",
      " 'variance_adaptor.duration_predictor.conv_layer.conv1d_2.conv.weight',\n",
      " 'variance_adaptor.duration_predictor.conv_layer.conv1d_2.conv.bias',\n",
      " 'variance_adaptor.duration_predictor.conv_layer.layer_norm_2.weight',\n",
      " 'variance_adaptor.duration_predictor.conv_layer.layer_norm_2.bias',\n",
      " 'variance_adaptor.duration_predictor.linear_layer.weight',\n",
      " 'variance_adaptor.duration_predictor.linear_layer.bias',\n",
      " 'variance_adaptor.pitch_predictor.conv_layer.conv1d_1.conv.weight',\n",
      " 'variance_adaptor.pitch_predictor.conv_layer.conv1d_1.conv.bias',\n",
      " 'variance_adaptor.pitch_predictor.conv_layer.layer_norm_1.weight',\n",
      " 'variance_adaptor.pitch_predictor.conv_layer.layer_norm_1.bias',\n",
      " 'variance_adaptor.pitch_predictor.conv_layer.conv1d_2.conv.weight',\n",
      " 'variance_adaptor.pitch_predictor.conv_layer.conv1d_2.conv.bias',\n",
      " 'variance_adaptor.pitch_predictor.conv_layer.layer_norm_2.weight',\n",
      " 'variance_adaptor.pitch_predictor.conv_layer.layer_norm_2.bias',\n",
      " 'variance_adaptor.pitch_predictor.linear_layer.weight',\n",
      " 'variance_adaptor.pitch_predictor.linear_layer.bias',\n",
      " 'variance_adaptor.energy_predictor.conv_layer.conv1d_1.conv.weight',\n",
      " 'variance_adaptor.energy_predictor.conv_layer.conv1d_1.conv.bias',\n",
      " 'variance_adaptor.energy_predictor.conv_layer.layer_norm_1.weight',\n",
      " 'variance_adaptor.energy_predictor.conv_layer.layer_norm_1.bias',\n",
      " 'variance_adaptor.energy_predictor.conv_layer.conv1d_2.conv.weight',\n",
      " 'variance_adaptor.energy_predictor.conv_layer.conv1d_2.conv.bias',\n",
      " 'variance_adaptor.energy_predictor.conv_layer.layer_norm_2.weight',\n",
      " 'variance_adaptor.energy_predictor.conv_layer.layer_norm_2.bias',\n",
      " 'variance_adaptor.energy_predictor.linear_layer.weight',\n",
      " 'variance_adaptor.energy_predictor.linear_layer.bias',\n",
      " 'variance_adaptor.pitch_embedding.weight',\n",
      " 'variance_adaptor.energy_embedding.weight',\n",
      " 'decoder.position_enc',\n",
      " 'decoder.layer_stack.0.slf_attn.w_qs.weight',\n",
      " 'decoder.layer_stack.0.slf_attn.w_qs.bias',\n",
      " 'decoder.layer_stack.0.slf_attn.w_ks.weight',\n",
      " 'decoder.layer_stack.0.slf_attn.w_ks.bias',\n",
      " 'decoder.layer_stack.0.slf_attn.w_vs.weight',\n",
      " 'decoder.layer_stack.0.slf_attn.w_vs.bias',\n",
      " 'decoder.layer_stack.0.slf_attn.layer_norm.weight',\n",
      " 'decoder.layer_stack.0.slf_attn.layer_norm.bias',\n",
      " 'decoder.layer_stack.0.slf_attn.fc.weight',\n",
      " 'decoder.layer_stack.0.slf_attn.fc.bias',\n",
      " 'decoder.layer_stack.0.pos_ffn.w_1.weight',\n",
      " 'decoder.layer_stack.0.pos_ffn.w_1.bias',\n",
      " 'decoder.layer_stack.0.pos_ffn.w_2.weight',\n",
      " 'decoder.layer_stack.0.pos_ffn.w_2.bias',\n",
      " 'decoder.layer_stack.0.pos_ffn.layer_norm.weight',\n",
      " 'decoder.layer_stack.0.pos_ffn.layer_norm.bias',\n",
      " 'decoder.layer_stack.1.slf_attn.w_qs.weight',\n",
      " 'decoder.layer_stack.1.slf_attn.w_qs.bias',\n",
      " 'decoder.layer_stack.1.slf_attn.w_ks.weight',\n",
      " 'decoder.layer_stack.1.slf_attn.w_ks.bias',\n",
      " 'decoder.layer_stack.1.slf_attn.w_vs.weight',\n",
      " 'decoder.layer_stack.1.slf_attn.w_vs.bias',\n",
      " 'decoder.layer_stack.1.slf_attn.layer_norm.weight',\n",
      " 'decoder.layer_stack.1.slf_attn.layer_norm.bias',\n",
      " 'decoder.layer_stack.1.slf_attn.fc.weight',\n",
      " 'decoder.layer_stack.1.slf_attn.fc.bias',\n",
      " 'decoder.layer_stack.1.pos_ffn.w_1.weight',\n",
      " 'decoder.layer_stack.1.pos_ffn.w_1.bias',\n",
      " 'decoder.layer_stack.1.pos_ffn.w_2.weight',\n",
      " 'decoder.layer_stack.1.pos_ffn.w_2.bias',\n",
      " 'decoder.layer_stack.1.pos_ffn.layer_norm.weight',\n",
      " 'decoder.layer_stack.1.pos_ffn.layer_norm.bias',\n",
      " 'decoder.layer_stack.2.slf_attn.w_qs.weight',\n",
      " 'decoder.layer_stack.2.slf_attn.w_qs.bias',\n",
      " 'decoder.layer_stack.2.slf_attn.w_ks.weight',\n",
      " 'decoder.layer_stack.2.slf_attn.w_ks.bias',\n",
      " 'decoder.layer_stack.2.slf_attn.w_vs.weight',\n",
      " 'decoder.layer_stack.2.slf_attn.w_vs.bias',\n",
      " 'decoder.layer_stack.2.slf_attn.layer_norm.weight',\n",
      " 'decoder.layer_stack.2.slf_attn.layer_norm.bias',\n",
      " 'decoder.layer_stack.2.slf_attn.fc.weight',\n",
      " 'decoder.layer_stack.2.slf_attn.fc.bias',\n",
      " 'decoder.layer_stack.2.pos_ffn.w_1.weight',\n",
      " 'decoder.layer_stack.2.pos_ffn.w_1.bias',\n",
      " 'decoder.layer_stack.2.pos_ffn.w_2.weight',\n",
      " 'decoder.layer_stack.2.pos_ffn.w_2.bias',\n",
      " 'decoder.layer_stack.2.pos_ffn.layer_norm.weight',\n",
      " 'decoder.layer_stack.2.pos_ffn.layer_norm.bias',\n",
      " 'decoder.layer_stack.3.slf_attn.w_qs.weight',\n",
      " 'decoder.layer_stack.3.slf_attn.w_qs.bias',\n",
      " 'decoder.layer_stack.3.slf_attn.w_ks.weight',\n",
      " 'decoder.layer_stack.3.slf_attn.w_ks.bias',\n",
      " 'decoder.layer_stack.3.slf_attn.w_vs.weight',\n",
      " 'decoder.layer_stack.3.slf_attn.w_vs.bias',\n",
      " 'decoder.layer_stack.3.slf_attn.layer_norm.weight',\n",
      " 'decoder.layer_stack.3.slf_attn.layer_norm.bias',\n",
      " 'decoder.layer_stack.3.slf_attn.fc.weight',\n",
      " 'decoder.layer_stack.3.slf_attn.fc.bias',\n",
      " 'decoder.layer_stack.3.pos_ffn.w_1.weight',\n",
      " 'decoder.layer_stack.3.pos_ffn.w_1.bias',\n",
      " 'decoder.layer_stack.3.pos_ffn.w_2.weight',\n",
      " 'decoder.layer_stack.3.pos_ffn.w_2.bias',\n",
      " 'decoder.layer_stack.3.pos_ffn.layer_norm.weight',\n",
      " 'decoder.layer_stack.3.pos_ffn.layer_norm.bias',\n",
      " 'decoder.layer_stack.4.slf_attn.w_qs.weight',\n",
      " 'decoder.layer_stack.4.slf_attn.w_qs.bias',\n",
      " 'decoder.layer_stack.4.slf_attn.w_ks.weight',\n",
      " 'decoder.layer_stack.4.slf_attn.w_ks.bias',\n",
      " 'decoder.layer_stack.4.slf_attn.w_vs.weight',\n",
      " 'decoder.layer_stack.4.slf_attn.w_vs.bias',\n",
      " 'decoder.layer_stack.4.slf_attn.layer_norm.weight',\n",
      " 'decoder.layer_stack.4.slf_attn.layer_norm.bias',\n",
      " 'decoder.layer_stack.4.slf_attn.fc.weight',\n",
      " 'decoder.layer_stack.4.slf_attn.fc.bias',\n",
      " 'decoder.layer_stack.4.pos_ffn.w_1.weight',\n",
      " 'decoder.layer_stack.4.pos_ffn.w_1.bias',\n",
      " 'decoder.layer_stack.4.pos_ffn.w_2.weight',\n",
      " 'decoder.layer_stack.4.pos_ffn.w_2.bias',\n",
      " 'decoder.layer_stack.4.pos_ffn.layer_norm.weight',\n",
      " 'decoder.layer_stack.4.pos_ffn.layer_norm.bias',\n",
      " 'decoder.layer_stack.5.slf_attn.w_qs.weight',\n",
      " 'decoder.layer_stack.5.slf_attn.w_qs.bias',\n",
      " 'decoder.layer_stack.5.slf_attn.w_ks.weight',\n",
      " 'decoder.layer_stack.5.slf_attn.w_ks.bias',\n",
      " 'decoder.layer_stack.5.slf_attn.w_vs.weight',\n",
      " 'decoder.layer_stack.5.slf_attn.w_vs.bias',\n",
      " 'decoder.layer_stack.5.slf_attn.layer_norm.weight',\n",
      " 'decoder.layer_stack.5.slf_attn.layer_norm.bias',\n",
      " 'decoder.layer_stack.5.slf_attn.fc.weight',\n",
      " 'decoder.layer_stack.5.slf_attn.fc.bias',\n",
      " 'decoder.layer_stack.5.pos_ffn.w_1.weight',\n",
      " 'decoder.layer_stack.5.pos_ffn.w_1.bias',\n",
      " 'decoder.layer_stack.5.pos_ffn.w_2.weight',\n",
      " 'decoder.layer_stack.5.pos_ffn.w_2.bias',\n",
      " 'decoder.layer_stack.5.pos_ffn.layer_norm.weight',\n",
      " 'decoder.layer_stack.5.pos_ffn.layer_norm.bias',\n",
      " 'mel_linear.weight',\n",
      " 'mel_linear.bias',\n",
      " 'postnet.convolutions.0.0.conv.weight',\n",
      " 'postnet.convolutions.0.0.conv.bias',\n",
      " 'postnet.convolutions.0.1.weight',\n",
      " 'postnet.convolutions.0.1.bias',\n",
      " 'postnet.convolutions.0.1.running_mean',\n",
      " 'postnet.convolutions.0.1.running_var',\n",
      " 'postnet.convolutions.0.1.num_batches_tracked',\n",
      " 'postnet.convolutions.1.0.conv.weight',\n",
      " 'postnet.convolutions.1.0.conv.bias',\n",
      " 'postnet.convolutions.1.1.weight',\n",
      " 'postnet.convolutions.1.1.bias',\n",
      " 'postnet.convolutions.1.1.running_mean',\n",
      " 'postnet.convolutions.1.1.running_var',\n",
      " 'postnet.convolutions.1.1.num_batches_tracked',\n",
      " 'postnet.convolutions.2.0.conv.weight',\n",
      " 'postnet.convolutions.2.0.conv.bias',\n",
      " 'postnet.convolutions.2.1.weight',\n",
      " 'postnet.convolutions.2.1.bias',\n",
      " 'postnet.convolutions.2.1.running_mean',\n",
      " 'postnet.convolutions.2.1.running_var',\n",
      " 'postnet.convolutions.2.1.num_batches_tracked',\n",
      " 'postnet.convolutions.3.0.conv.weight',\n",
      " 'postnet.convolutions.3.0.conv.bias',\n",
      " 'postnet.convolutions.3.1.weight',\n",
      " 'postnet.convolutions.3.1.bias',\n",
      " 'postnet.convolutions.3.1.running_mean',\n",
      " 'postnet.convolutions.3.1.running_var',\n",
      " 'postnet.convolutions.3.1.num_batches_tracked',\n",
      " 'postnet.convolutions.4.0.conv.weight',\n",
      " 'postnet.convolutions.4.0.conv.bias',\n",
      " 'postnet.convolutions.4.1.weight',\n",
      " 'postnet.convolutions.4.1.bias',\n",
      " 'postnet.convolutions.4.1.running_mean',\n",
      " 'postnet.convolutions.4.1.running_var',\n",
      " 'postnet.convolutions.4.1.num_batches_tracked',\n",
      " 'speaker_emb.weight']\n"
     ]
    }
   ],
   "source": [
    "pprint([n for n in ckpt['model'].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47f07cb3-4707-4d19-8073-c02c793f4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using speaker embeddings.\n",
      "True\n",
      "2\n",
      "=> Using VarianceAdaptorWithSpeaker.\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31ebfb0c-5cf7-4f1b-8c0c-799f06b4b02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoder', 'variance_adaptor', 'decoder', 'mel_linear', 'postnet', 'speaker_layernorm']\n"
     ]
    }
   ],
   "source": [
    "print([n for n, _ in model.named_children()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3d46a4a-aa96-4323-8c68-0796fcdc3cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder',\n",
       " 'variance_adaptor',\n",
       " 'decoder',\n",
       " 'mel_linear',\n",
       " 'postnet',\n",
       " 'speaker_layernorm']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n, _ in model.named_children()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6eaada-e1c1-458f-8372-1ba2a83f6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(ckpt[\"model\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a9e8a3-5ccd-446a-8158-c209a1e09364",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad_(False)\n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad_(False)\n",
    "for param in model.mel_linear.parameters():\n",
    "    param.requires_grad_(False)\n",
    "for param in model.postnet.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d6059-d720-4fd5-96ff-6be6d5ae4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292fcde-b3c5-49c2-a6a9-e0cdb0167ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfc4c8-1cf4-4c60-bb64-0b69fed55125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cdd641-9e83-4440-83cf-ebb4e129566f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
