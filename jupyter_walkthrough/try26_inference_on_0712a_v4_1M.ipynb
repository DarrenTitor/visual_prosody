{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00b4c6b-f301-46e1-9c0d-912d0aadef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20e3c19-2e21-4101-8143-b3b969679d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0519569d-4763-44e7-90c1-05e206f97df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v4\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af2dfda-c3a0-408c-918e-a1587be07fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2641\n"
     ]
    }
   ],
   "source": [
    "print(len(val_uids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5f58f0-ae29-4788-86a1-7beb893ca12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model, get_vocoder, get_param_num, vocoder_infer\n",
    "from utils.tools import to_device, log, synth_one_sample, expand, plot_mel\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset\n",
    "# from utils.auto_tqdm import tqdm\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f48ca035-783e-4db7-8ef3-21967da54340",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ffc6cd6-bfde-4d61-bd24-c8e25207ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/Ego4D_final_v4/0712a_preprocess.yaml -m ./config/Ego4D_final_v4/0712a_model.yaml -t ./config/Ego4D_final_v4/0712a_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ad7d0e1-84ce-44bd-9659-0ff215b264e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(restore_step=0, preprocess_config='./config/Ego4D_final_v4/0712a_preprocess.yaml', model_config='./config/Ego4D_final_v4/0712a_model.yaml', train_config='./config/Ego4D_final_v4/0712a_train.yaml')\n",
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "pprint(args)\n",
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)\n",
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bb228eb-2d99-40f5-9b19-947cc655aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/0712a/ckpt/1000000.pth.tar'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "128702d3-46d3-4675-a7d0-e187f4c92359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Not using speaker embeddings.\n",
      "False\n",
      "None\n",
      "Number of FastSpeech2 Parameters: 35159745\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c18dcce-f404-4f3f-b055-aa56b7f6f65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)\n",
    "step = args.restore_step + 1\n",
    "model.eval()\n",
    "print()\n",
    "dataset = Dataset(\n",
    "    \"val.txt\", 'val', preprocess_config, train_config, sort=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d0021e7-61a9-4efe-9c0f-acd5554d502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8c7ab59-6a4d-4b1e-81f1-1173888bd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "# Get loss function\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c45e9af7-8142-4903-b603-1a0d957992d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/0712a/result/Ego4D_final_v4\\\\plots'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(train_config['path']['result_path'], 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4ad103f-8a18-4943-86c2-8f17dfbf25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_plot_path = os.path.join(train_config['path']['result_path'], 'plot')\n",
    "output_mel_syn_path = os.path.join(train_config['path']['result_path'], 'mel', 'syn')\n",
    "output_mel_gt_path = os.path.join(train_config['path']['result_path'], 'mel', 'gt')\n",
    "output_wav_syn_path = os.path.join(train_config['path']['result_path'], 'wav', 'synthesized')\n",
    "output_wav_rec_path = os.path.join(train_config['path']['result_path'], 'wav', 'reconstructed')\n",
    "os.makedirs(output_plot_path, exist_ok=True)\n",
    "os.makedirs(output_mel_syn_path, exist_ok=True)\n",
    "os.makedirs(output_mel_gt_path, exist_ok=True)\n",
    "os.makedirs(output_wav_syn_path, exist_ok=True)\n",
    "os.makedirs(output_wav_rec_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92ff4da3-6e71-465d-b4c8-aaf1f1e969df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be499d0b912e4b8c8c103a8941d75d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batchs in tqdm(loader):\n",
    "    for targets in batchs:\n",
    "        targets = to_device(targets, device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(*(targets[2:]))\n",
    "        basenames = targets[0]\n",
    "        for i in range(len(predictions[0])):\n",
    "            basename = basenames[i]\n",
    "            src_len = predictions[8][i].item()\n",
    "            mel_len = predictions[9][i].item()\n",
    "            mel_prediction = predictions[1][i, :mel_len].detach().transpose(0, 1)\n",
    "            mel_target = targets[6][i, :mel_len].detach().transpose(0, 1)\n",
    "\n",
    "            torch.save(mel_prediction.cpu(), os.path.join(output_mel_syn_path, f\"{basename}.pt\"))\n",
    "            torch.save(mel_target.cpu(), os.path.join(output_mel_gt_path, f\"{basename}.pt\"))\n",
    "            \n",
    "            \n",
    "            duration = predictions[5][i, :src_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n",
    "                pitch = predictions[2][i, :src_len].detach().cpu().numpy()\n",
    "                pitch = expand(pitch, duration)\n",
    "            else:\n",
    "                pitch = predictions[2][i, :mel_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"] == \"phoneme_level\":\n",
    "                energy = predictions[3][i, :src_len].detach().cpu().numpy()\n",
    "                energy = expand(energy, duration)\n",
    "            else:\n",
    "                energy = predictions[3][i, :mel_len].detach().cpu().numpy()\n",
    "\n",
    "            with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \n",
    "                                   \"stats.json\")) as f:\n",
    "                stats = json.load(f)\n",
    "                stats = stats[\"pitch\"] + stats[\"energy\"][:2]\n",
    "                                       \n",
    "            fig = plot_mel(\n",
    "                [\n",
    "                    (mel_prediction.cpu().numpy(), pitch, energy),\n",
    "                    (mel_target.cpu().numpy(), pitch, energy),\n",
    "                ],\n",
    "                stats,\n",
    "                [\"Synthetized Spectrogram\", \"Ground-Truth Spectrogram\"],\n",
    "            )\n",
    "            ### TODO: change to svg\n",
    "            plt.savefig(os.path.join(output_plot_path, f\"{basename}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # from .model import vocoder_infer\n",
    "\n",
    "        mel_predictions = predictions[1].transpose(1, 2)\n",
    "        mel_targets = targets[6].transpose(1, 2)\n",
    "        \n",
    "        lengths = predictions[9] * preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "        wav_predictions = vocoder_infer(\n",
    "            mel_predictions, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "        )\n",
    "        wav_targets = vocoder_infer(\n",
    "        mel_targets, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "    )\n",
    "    \n",
    "        sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "        for wav, basename in zip(wav_predictions, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_syn_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "        for wav, basename in zip(wav_targets, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_rec_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "\n",
    "        \n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16dec5-e031-4b58-8b24-6a89b80ffb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd8a84-4163-4901-b8bc-4cb4cbe24de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc64c3-8cfd-4ef3-ac36-61e7e4bd2ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c1bc5fa-95f8-43af-b754-0b3f98c100a1",
   "metadata": {},
   "source": [
    "# MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0cb071b-5189-4391-8b70-fbb50da9fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import os.path as op\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import argparse\n",
    "import fnmatch\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import librosa\n",
    "import pysptk\n",
    "import pyworld as pw\n",
    "import soundfile as sf\n",
    "from fastdtw import fastdtw\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22c17a0b-ac41-49d2-8b36-e4f36b02065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v4\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "859c65a0-ea86-49ea-9c1f-5fb7888ac1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f491b02c-0fd8-4841-a882-43ff0189bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_folder = r'.\\output\\0712a\\result\\Ego4D_final_v4\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0712a\\result\\Ego4D_final_v4\\wav\\synthesized'\n",
    "gt_folder = r'.\\raw_data\\Ego4D_final_v4\\val\\Ego4D_final_v4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a61f0f2a-2b18-43f9-8093-5110d23cfcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bd3cb8fd4d408f832230636720b80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11min 45s\n",
      "Wall time: 14min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def sptk_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    "    is_padding: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract SPTK-based mel-cepstrum.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Sampling rate\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "        is_padding (bool): Whether to pad the end of signal (default=False).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "\n",
    "    \"\"\"\n",
    "    # perform padding\n",
    "    if is_padding:\n",
    "        n_pad = n_fft - (len(x) - n_fft) % n_shift\n",
    "        x = np.pad(x, (0, n_pad), \"reflect\")\n",
    "\n",
    "    # get number of frames\n",
    "    n_frame = (len(x) - n_fft) // n_shift + 1\n",
    "\n",
    "    # get window function\n",
    "    win = pysptk.sptk.hamming(n_fft)\n",
    "\n",
    "    # check mcep and alpha\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "\n",
    "    # calculate spectrogram\n",
    "    mcep = [\n",
    "        pysptk.mcep(\n",
    "            x[n_shift * i : n_shift * i + n_fft] * win,\n",
    "            mcep_dim,\n",
    "            mcep_alpha,\n",
    "            eps=1e-6,\n",
    "            etype=1,\n",
    "        )\n",
    "        for i in range(n_frame)\n",
    "    ]\n",
    "\n",
    "    return np.stack(mcep)\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uids = []\n",
    "MCDs_recon = []\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    gen_x, gen_fs = sf.read(op.join(recon_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(gt_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    # print(gen_fs, gt_fs)\n",
    "    fs = gen_fs\n",
    "    if gen_fs != gt_fs:\n",
    "        gt_x = librosa.resample(gt_x.astype(float), orig_sr=gt_fs, target_sr=gen_fs)\n",
    "\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep = sptk_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        n_fft=512,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep = sptk_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        n_fft=512,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_mcep_dtw = gen_mcep[twf[0]]\n",
    "    gt_mcep_dtw = gt_mcep[twf[1]]\n",
    "\n",
    "    # MCD\n",
    "    diff2sum = np.sum((gen_mcep_dtw - gt_mcep_dtw) ** 2, 1)\n",
    "    mcd = np.mean(10.0 / np.log(10.0) * np.sqrt(2 * diff2sum), 0)\n",
    "    result = mcd\n",
    "    MCDs_recon.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcb61670-5353-4dce-81c4-65ca21237d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c428bcd3da946f2bf4a38ea59e83a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11min 55s\n",
      "Wall time: 13min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def sptk_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    "    is_padding: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract SPTK-based mel-cepstrum.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Sampling rate\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "        is_padding (bool): Whether to pad the end of signal (default=False).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "\n",
    "    \"\"\"\n",
    "    # perform padding\n",
    "    if is_padding:\n",
    "        n_pad = n_fft - (len(x) - n_fft) % n_shift\n",
    "        x = np.pad(x, (0, n_pad), \"reflect\")\n",
    "\n",
    "    # get number of frames\n",
    "    n_frame = (len(x) - n_fft) // n_shift + 1\n",
    "\n",
    "    # get window function\n",
    "    win = pysptk.sptk.hamming(n_fft)\n",
    "\n",
    "    # check mcep and alpha\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "\n",
    "    # calculate spectrogram\n",
    "    mcep = [\n",
    "        pysptk.mcep(\n",
    "            x[n_shift * i : n_shift * i + n_fft] * win,\n",
    "            mcep_dim,\n",
    "            mcep_alpha,\n",
    "            eps=1e-6,\n",
    "            etype=1,\n",
    "        )\n",
    "        for i in range(n_frame)\n",
    "    ]\n",
    "\n",
    "    return np.stack(mcep)\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "uids = []\n",
    "MCDs_gt = []\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(gt_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    # print(gen_fs, gt_fs)\n",
    "    fs = gen_fs\n",
    "    if gen_fs != gt_fs:\n",
    "        gt_x = librosa.resample(gt_x.astype(float), orig_sr=gt_fs, target_sr=gen_fs)\n",
    "\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep = sptk_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        n_fft=512,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep = sptk_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        n_fft=512,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_mcep_dtw = gen_mcep[twf[0]]\n",
    "    gt_mcep_dtw = gt_mcep[twf[1]]\n",
    "\n",
    "    # MCD\n",
    "    diff2sum = np.sum((gen_mcep_dtw - gt_mcep_dtw) ** 2, 1)\n",
    "    mcd = np.mean(10.0 / np.log(10.0) * np.sqrt(2 * diff2sum), 0)\n",
    "    result = mcd\n",
    "    MCDs_gt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96054f5e-8b39-4a3e-97d1-1d82d1f336e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD_recon': MCDs_recon,\n",
    "    'MCD_gt': MCDs_gt,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0250080-630f-4baa-95ab-7aedfab9eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_0712a_1M_v4_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "407251ac-1fa6-4def-8b71-f3456c75d3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCDs_recon mean on 0712a_1M_v4: 4.710335337567078\n",
      "MCDs_recon std on 0712a_1M_v4: 1.196364770343932\n",
      "MCDs_gt mean on 0712a_1M_v4: 9.20648139192688\n",
      "MCDs_gt std on 0712a_1M_v4: 1.5928134651664063\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCDs_recon mean on 0712a_1M_v4: {np.nanmean(MCDs_recon)}\")\n",
    "print(f\"MCDs_recon std on 0712a_1M_v4: {np.nanstd(MCDs_recon)}\")\n",
    "\n",
    "print(f\"MCDs_gt mean on 0712a_1M_v4: {np.nanmean(MCDs_gt)}\")\n",
    "print(f\"MCDs_gt std on 0712a_1M_v4: {np.nanstd(MCDs_gt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "039f169b-0e1d-4c4a-a1c5-c670b442186b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>MCD_recon</th>\n",
       "      <th>MCD_gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>d6f99bd9-e9c7-41cd-8dfb-86b66871fc7e</td>\n",
       "      <td>2.997942</td>\n",
       "      <td>9.117190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>b6335079-b4e0-47a0-ae26-aeebe29f0441</td>\n",
       "      <td>3.033956</td>\n",
       "      <td>11.973094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>f08e023f-3d18-4243-8c17-26161d255030</td>\n",
       "      <td>3.056815</td>\n",
       "      <td>7.497515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>273f6847-2c3c-44b4-8fb8-1e636d3cf6eb</td>\n",
       "      <td>3.059826</td>\n",
       "      <td>8.043425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>4c848686-910e-41bd-b183-9a8867d236a9</td>\n",
       "      <td>3.121161</td>\n",
       "      <td>8.466061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uid  MCD_recon     MCD_gt\n",
       "2491  d6f99bd9-e9c7-41cd-8dfb-86b66871fc7e   2.997942   9.117190\n",
       "1352  b6335079-b4e0-47a0-ae26-aeebe29f0441   3.033956  11.973094\n",
       "2240  f08e023f-3d18-4243-8c17-26161d255030   3.056815   7.497515\n",
       "465   273f6847-2c3c-44b4-8fb8-1e636d3cf6eb   3.059826   8.043425\n",
       "715   4c848686-910e-41bd-b183-9a8867d236a9   3.121161   8.466061"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v4_mcd_df.nsmallest(5, 'MCD_recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6668e9b5-f732-4f77-9fe2-a27988aa420e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>MCD_recon</th>\n",
       "      <th>MCD_gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>49a76e36-8573-45b9-9bf5-d27b8ea26dbb</td>\n",
       "      <td>15.321158</td>\n",
       "      <td>13.939626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>80eab8d0-a9ff-44ec-9778-3f35bf8020a1</td>\n",
       "      <td>12.842950</td>\n",
       "      <td>13.069163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>e053504c-4a11-4ba4-b2b5-2be37ed6f5e2</td>\n",
       "      <td>12.733121</td>\n",
       "      <td>12.676626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>3bdc5c8b-f30e-457a-83e4-6b179f0a0cd0</td>\n",
       "      <td>12.079830</td>\n",
       "      <td>11.678589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28f314aa-e11d-48b3-b897-2f0bd92943b7</td>\n",
       "      <td>11.818570</td>\n",
       "      <td>11.825336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       uid  MCD_recon     MCD_gt\n",
       "1635  49a76e36-8573-45b9-9bf5-d27b8ea26dbb  15.321158  13.939626\n",
       "2477  80eab8d0-a9ff-44ec-9778-3f35bf8020a1  12.842950  13.069163\n",
       "432   e053504c-4a11-4ba4-b2b5-2be37ed6f5e2  12.733121  12.676626\n",
       "681   3bdc5c8b-f30e-457a-83e4-6b179f0a0cd0  12.079830  11.678589\n",
       "17    28f314aa-e11d-48b3-b897-2f0bd92943b7  11.818570  11.825336"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v4_mcd_df.nlargest(5, 'MCD_recon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5bea8-517e-4d53-a1b5-5f12b0df5173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3518e-4911-46d3-8c66-2735964dbc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11915312-844f-4f14-808c-5544eecaf9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27602d4-ce05-4a1b-be12-f77db1f7f9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d98eb008-0fee-44b6-9fa9-de9f68868073",
   "metadata": {},
   "source": [
    "# log f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abd7068-7504-4aa1-83e2-4ee7648ad195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/espnet/espnet/blob/3e0dad524d62ccd45e067e9b36049f2214ea972a/egs2/TEMPLATE/asr1/pyscripts/utils/evaluate_f0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a54d899c-c160-4b0a-9289-0f1fb9d0effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    f0min: int = 40,\n",
    "    f0max: int = 800,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract World-based acoustic features.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Minimum f0 value (default=40).\n",
    "        f0 (int): Maximum f0 value (default=800).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "        ndarray: F0 sequence (N,).\n",
    "\n",
    "    \"\"\"\n",
    "    # extract features\n",
    "    x = x.astype(np.float64)\n",
    "    f0, time_axis = pw.harvest(\n",
    "        x,\n",
    "        fs,\n",
    "        f0_floor=f0min,\n",
    "        f0_ceil=f0max,\n",
    "        frame_period=n_shift / fs * 1000,\n",
    "    )\n",
    "    sp = pw.cheaptrick(x, f0, time_axis, fs, fft_size=n_fft)\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "    mcep = pysptk.sp2mc(sp, mcep_dim, mcep_alpha)\n",
    "\n",
    "    return mcep, f0\n",
    "\n",
    "\n",
    "def _get_basename(path: str) -> str:\n",
    "    return os.path.splitext(os.path.split(path)[-1])[0]\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a9cc54e-d4ee-4448-b47a-ca932ef43937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6eaa0240f2243babfbbd65873ce7023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2846: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "uids = []\n",
    "logf0_rmses_recon = []\n",
    "logf0_corrs_recon = []\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(recon_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    fs = gen_fs\n",
    "    if gen_fs != gt_fs:\n",
    "        gt_x = librosa.resample(gt_x.astype(float), orig_sr=gt_fs, target_sr=gen_fs)\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep, gen_f0 = world_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep, gt_f0 = world_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_f0_dtw = gen_f0[twf[0]]\n",
    "    gt_f0_dtw = gt_f0[twf[1]]\n",
    "    \n",
    "    # Get voiced part\n",
    "    nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "    gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs])\n",
    "    gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs])\n",
    "\n",
    "    # log F0 RMSE\n",
    "    log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "    # print(f\"{uid} {log_f0_rmse:.4f}\")\n",
    "\n",
    "    # log F0 corr\n",
    "    log_f0_corr = np.corrcoef(gen_f0_dtw_voiced, gt_f0_dtw_voiced)[0][1]\n",
    "    # print(f\"{uid} {log_f0_corr:.4f}\")\n",
    "\n",
    "    logf0_rmses_recon.append(log_f0_rmse)\n",
    "    logf0_corrs_recon.append(log_f0_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb97ca20-70b6-457a-8e7b-07ac3a0ac0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bf9d2a2a6f4f6cb0a045675ff14812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2641 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "uids = []\n",
    "logf0_rmses_gt = []\n",
    "logf0_corrs_gt = []\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(gt_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    fs = gen_fs\n",
    "    if gen_fs != gt_fs:\n",
    "        gt_x = librosa.resample(gt_x.astype(float), orig_sr=gt_fs, target_sr=gen_fs)\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep, gen_f0 = world_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep, gt_f0 = world_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_f0_dtw = gen_f0[twf[0]]\n",
    "    gt_f0_dtw = gt_f0[twf[1]]\n",
    "    \n",
    "    # Get voiced part\n",
    "    nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "    gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs])\n",
    "    gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs])\n",
    "\n",
    "    # log F0 RMSE\n",
    "    log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "    # print(f\"{uid} {log_f0_rmse:.4f}\")\n",
    "\n",
    "    # log F0 corr\n",
    "    log_f0_corr = np.corrcoef(gen_f0_dtw_voiced, gt_f0_dtw_voiced)[0][1]\n",
    "    # print(f\"{uid} {log_f0_corr:.4f}\")\n",
    "\n",
    "    logf0_rmses_gt.append(log_f0_rmse)\n",
    "    logf0_corrs_gt.append(log_f0_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4680d75c-afbc-410f-87f2-540968bee1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4_logf0_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'logf0_rmse_recon': logf0_rmses_recon,\n",
    "    'logf0_corr_recon': logf0_corrs_recon,\n",
    "    'logf0_rmse_gt': logf0_rmses_gt,\n",
    "    'logf0_corr_gt': logf0_corrs_gt,\n",
    "})\n",
    "v4_logf0_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\logF0_0712a_v4_1M_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0fd9f43-c470-4e5b-8051-5288b03346f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4_logf0_df = v4_logf0_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e026dc6-de3e-40e9-b724-63f9e3cee8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logf0_rmse_recon mean on 0712a_v4_1M: 0.5508572610451278\n",
      "logf0_rmse_recon std on 0712a_v4_1M: 0.3309150585063425\n",
      "logf0_corr_recon mean on 0712a_v4_1M: 0.1335724405704627\n",
      "logf0_corr_recon std on 0712a_v4_1M: 0.38795448299072294\n",
      "logf0_rmse_gt mean on 0712a_v4_1M: 0.5688654798912349\n",
      "logf0_rmse_gt std on 0712a_v4_1M: 0.33464315738411515\n",
      "logf0_corr_gt mean on 0712a_v4_1M: 0.11785446972405585\n",
      "logf0_corr_gt std on 0712a_v4_1M: 0.38337624020984956\n"
     ]
    }
   ],
   "source": [
    "print(f\"logf0_rmse_recon mean on 0712a_v4_1M: {v4_logf0_df['logf0_rmse_recon'].values.mean()}\")\n",
    "print(f\"logf0_rmse_recon std on 0712a_v4_1M: {v4_logf0_df['logf0_rmse_recon'].values.std()}\")\n",
    "print(f\"logf0_corr_recon mean on 0712a_v4_1M: {v4_logf0_df['logf0_corr_recon'].values.mean()}\")\n",
    "print(f\"logf0_corr_recon std on 0712a_v4_1M: {v4_logf0_df['logf0_corr_recon'].values.std()}\")\n",
    "\n",
    "print(f\"logf0_rmse_gt mean on 0712a_v4_1M: {v4_logf0_df['logf0_rmse_gt'].values.mean()}\")\n",
    "print(f\"logf0_rmse_gt std on 0712a_v4_1M: {v4_logf0_df['logf0_rmse_gt'].values.std()}\")\n",
    "print(f\"logf0_corr_gt mean on 0712a_v4_1M: {v4_logf0_df['logf0_corr_gt'].values.mean()}\")\n",
    "print(f\"logf0_corr_gt std on 0712a_v4_1M: {v4_logf0_df['logf0_corr_gt'].values.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e32d97-ad91-43c0-a7fa-07a442c54a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a77e1a-9837-4bb2-8d4e-57f151ff0207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cae89-637f-438e-ae9a-9028f49083a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfb0a7-b8f1-405a-b339-a36041e40916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
