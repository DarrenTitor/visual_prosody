{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a929dc-e8ff-458d-848e-ddb1572edff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b4b5b0-77a4-4401-b5e3-004b6383e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\Schoolwork\\TERM 3\\WORK\\visual_prosody\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1d69d6-d7ef-4b1b-ab31-12858687b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v6\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e52c80-2dab-43c9-94ef-47594d041357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2772\n"
     ]
    }
   ],
   "source": [
    "print(len(val_uids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced3c4d1-e41b-4f9b-ba99-572719ee04ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model, get_vocoder, get_param_num, vocoder_infer\n",
    "from utils.tools import to_device, log, synth_one_sample, expand, plot_mel\n",
    "from model import FastSpeech2Loss\n",
    "from dataset import Dataset\n",
    "# from utils.auto_tqdm import tqdm\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09c4086c-c76d-4df1-b1c9-2392bc43a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e67c3d73-cb7d-47db-9f7f-90698a9050aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--restore_step\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"-p\",\n",
    "    \"--preprocess_config\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"path to preprocess.yaml\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-m\", \"--model_config\", type=str, required=True, help=\"path to model.yaml\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-t\", \"--train_config\", type=str, required=True, help=\"path to train.yaml\"\n",
    ")\n",
    "\n",
    "argString = '-p ./config/Ego4D_final_v6/0703a_preprocess.yaml -m ./config/Ego4D_final_v6/0703a_model.yaml -t ./config/Ego4D_final_v6/0703a_train.yaml'\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(argString.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315cab3a-db22-44bf-84db-792f6a912ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(restore_step=0, preprocess_config='./config/Ego4D_final_v6/0703a_preprocess.yaml', model_config='./config/Ego4D_final_v6/0703a_model.yaml', train_config='./config/Ego4D_final_v6/0703a_train.yaml')\n",
      "Prepare training ...\n"
     ]
    }
   ],
   "source": [
    "pprint(args)\n",
    "# Read Config\n",
    "preprocess_config = yaml.load(\n",
    "    open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader\n",
    ")\n",
    "model_config = yaml.load(open(args.model_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "configs = (preprocess_config, model_config, train_config)\n",
    "print(\"Prepare training ...\")\n",
    "\n",
    "preprocess_config, model_config, train_config = configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d891482-4ff5-4697-8b98-be6718572a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/0703a/ckpt/Ego4D_final_v6/1200000.pth.tar'\n",
    "ckpt = torch.load(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2330055-1c72-4713-892a-8cff5b703abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Using speaker embeddings.\n",
      "True\n",
      "2\n",
      "=> Using VarianceAdaptorWithSpeaker.\n",
      "Number of FastSpeech2 Parameters: 35165553\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "model, optimizer = get_model(args, configs, device, train=True)\n",
    "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "num_param = get_param_num(model)\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a650f5-260e-4579-8d87-892ef6dea26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load vocoder\n",
    "vocoder = get_vocoder(model_config, device)\n",
    "step = args.restore_step + 1\n",
    "model.eval()\n",
    "print()\n",
    "dataset = Dataset(\n",
    "    \"val.txt\", 'val', preprocess_config, train_config, sort=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8584d462-8b03-4ad0-bd8b-4e113dd51b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81575bc8-44e4-46e8-af85-f0190b5b0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "# Get loss function\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a165be9-c99d-45bd-bf1b-f329de4297e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./output/0703a/result/Ego4D_final_v6\\\\plots'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(train_config['path']['result_path'], 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a2b0cc0-aa76-4fc4-bf2e-c781e7d0a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# => batch:\n",
    "# return (\n",
    "#     ids,\n",
    "#     raw_texts,\n",
    "#     speakers,\n",
    "#     texts,\n",
    "#     text_lens,\n",
    "#     max(text_lens),\n",
    "#     mels,\n",
    "#     mel_lens,\n",
    "#     max(mel_lens),\n",
    "#     pitches,\n",
    "#     energies,\n",
    "#     durations,\n",
    "#     speaker_embeddings,\n",
    "# )\n",
    "# [12] 对应speaker embedding\n",
    "\n",
    "\n",
    "# => model output / prediction\n",
    "# return (\n",
    "#     output,\n",
    "#     postnet_output,\n",
    "#     p_predictions,\n",
    "#     e_predictions,\n",
    "#     log_d_predictions,\n",
    "#     d_rounded,\n",
    "#     src_masks,\n",
    "#     mel_masks,\n",
    "#     src_lens,\n",
    "#     mel_lens,\n",
    "# )\n",
    "output_plot_path = os.path.join(train_config['path']['result_path'], 'plot')\n",
    "output_mel_syn_path = os.path.join(train_config['path']['result_path'], 'mel', 'syn')\n",
    "output_mel_gt_path = os.path.join(train_config['path']['result_path'], 'mel', 'gt')\n",
    "output_wav_syn_path = os.path.join(train_config['path']['result_path'], 'wav', 'synthesized')\n",
    "output_wav_rec_path = os.path.join(train_config['path']['result_path'], 'wav', 'reconstructed')\n",
    "os.makedirs(output_plot_path, exist_ok=True)\n",
    "os.makedirs(output_mel_syn_path, exist_ok=True)\n",
    "os.makedirs(output_mel_gt_path, exist_ok=True)\n",
    "os.makedirs(output_wav_syn_path, exist_ok=True)\n",
    "os.makedirs(output_wav_rec_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07cbbe9a-a699-4ac2-b7b2-10000d13890f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9182c0b667da477bafbbe6cc0e6a4b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batchs in tqdm(loader):\n",
    "    for targets in batchs:\n",
    "        targets = to_device(targets, device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(*(targets[2:]))\n",
    "        basenames = targets[0]\n",
    "        for i in range(len(predictions[0])):\n",
    "            basename = basenames[i]\n",
    "            src_len = predictions[8][i].item()\n",
    "            mel_len = predictions[9][i].item()\n",
    "            mel_prediction = predictions[1][i, :mel_len].detach().transpose(0, 1)\n",
    "            mel_target = targets[6][i, :mel_len].detach().transpose(0, 1)\n",
    "\n",
    "            torch.save(mel_prediction.cpu(), os.path.join(output_mel_syn_path, f\"{basename}.pt\"))\n",
    "            torch.save(mel_target.cpu(), os.path.join(output_mel_gt_path, f\"{basename}.pt\"))\n",
    "            \n",
    "            \n",
    "            duration = predictions[5][i, :src_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n",
    "                pitch = predictions[2][i, :src_len].detach().cpu().numpy()\n",
    "                pitch = expand(pitch, duration)\n",
    "            else:\n",
    "                pitch = predictions[2][i, :mel_len].detach().cpu().numpy()\n",
    "            if preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"] == \"phoneme_level\":\n",
    "                energy = predictions[3][i, :src_len].detach().cpu().numpy()\n",
    "                energy = expand(energy, duration)\n",
    "            else:\n",
    "                energy = predictions[3][i, :mel_len].detach().cpu().numpy()\n",
    "\n",
    "            with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \n",
    "                                   \"stats.json\")) as f:\n",
    "                stats = json.load(f)\n",
    "                stats = stats[\"pitch\"] + stats[\"energy\"][:2]\n",
    "                                       \n",
    "            fig = plot_mel(\n",
    "                [\n",
    "                    (mel_prediction.cpu().numpy(), pitch, energy),\n",
    "                    (mel_target.cpu().numpy(), pitch, energy),\n",
    "                ],\n",
    "                stats,\n",
    "                [\"Synthetized Spectrogram\", \"Ground-Truth Spectrogram\"],\n",
    "            )\n",
    "            ### TODO: change to svg\n",
    "            plt.savefig(os.path.join(output_plot_path, f\"{basename}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # from .model import vocoder_infer\n",
    "\n",
    "        mel_predictions = predictions[1].transpose(1, 2)\n",
    "        mel_targets = targets[6].transpose(1, 2)\n",
    "        \n",
    "        lengths = predictions[9] * preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "        wav_predictions = vocoder_infer(\n",
    "            mel_predictions, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "        )\n",
    "        wav_targets = vocoder_infer(\n",
    "        mel_targets, vocoder, model_config, preprocess_config, lengths=lengths\n",
    "    )\n",
    "    \n",
    "        sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "        for wav, basename in zip(wav_predictions, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_syn_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "        for wav, basename in zip(wav_targets, basenames):\n",
    "            wavfile.write(os.path.join(output_wav_rec_path, f\"{basename}.wav\"), sampling_rate, wav)\n",
    "\n",
    "        \n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b7374-bccc-4290-8657-724b7134da65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada8489-96ce-42be-a24f-3a61af6a7122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a68832d-261a-4b18-87a6-158977344d00",
   "metadata": {},
   "source": [
    "# MCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15315af6-a4c3-4011-8560-d0725ca97bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, shutil\n",
    "import os.path as op\n",
    "import argparse\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os.path as op\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import argparse\n",
    "import fnmatch\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import librosa\n",
    "import pysptk\n",
    "import pyworld as pw\n",
    "import soundfile as sf\n",
    "from fastdtw import fastdtw\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf92d6af-a61b-4dc4-8da8-3639dd6e48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_txt_val_path = r'.\\preprocessed_data\\Ego4D_final_v6\\val.txt'\n",
    "val_uids = []\n",
    "with open(split_txt_val_path) as file:\n",
    "    for line in file:\n",
    "        # print(line.split('|')[0])\n",
    "        val_uids.append(line.split('|')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3072de79-cdef-45e3-b0ad-fc16e095d16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2772"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14ad09f1-e9b9-4a16-a6c6-51f880e6311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_folder = r'.\\output\\0703a\\result\\Ego4D_final_v6\\wav\\reconstructed'\n",
    "syn_folder = r'.\\output\\0703a\\result\\Ego4D_final_v6\\wav\\synthesized'\n",
    "gt_folder = r'.\\raw_data\\Ego4D_final_v6\\val\\Ego4D_final_v6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa35343b-2aed-42bd-b2d1-339ee1ce8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "melkwargs = {\n",
    "    \"n_fft\": int(0.05 * sr), \"win_length\": int(0.05 * sr),\n",
    "    \"hop_length\": int(0.0125 * sr), \"f_min\": 20,\n",
    "    \"n_mels\": 80, \"window_fn\": torch.hann_window\n",
    "}\n",
    "mfcc_fn = torchaudio.transforms.MFCC(\n",
    "    sr, n_mfcc=13, log_mels=True, melkwargs=melkwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156fb82e-c1c2-4e65-b0be-9e194b7bf7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef42b31cf384a0d9207a4ffe5604903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 5s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uids = []\n",
    "MCDs_recon = []\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(recon_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs_recon.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94af53d5-3897-4bce-a395-b9fb7eed8038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21104de03d746dc91bd3d9a7946ef0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 7s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "uids = []\n",
    "MCDs_gt = []\n",
    "for uid in tqdm(val_uids):\n",
    "    uids.append(uid)\n",
    "    wav_1, sr = torchaudio.load(op.join(gt_folder, f\"{uid}.wav\"))\n",
    "    wav_2, sr = torchaudio.load(op.join(syn_folder, f\"{uid}.wav\"))\n",
    "    wav_1 = wav_1.squeeze()\n",
    "    wav_2 = wav_2.squeeze()\n",
    "    mel_1 = mfcc_fn(wav_1).T.numpy()\n",
    "    mel_2 = mfcc_fn(wav_2).T.numpy()\n",
    "    # DTW\n",
    "    _, path = fastdtw(mel_2, mel_1, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    mel_2 = mel_2[twf[0]]\n",
    "    mel_1 = mel_1[twf[1]]\n",
    "    # We sum the squared differences over the first K MFCCs, skipping ct,0\n",
    "    mel_1 = mel_1[:, 1:]\n",
    "    mel_2 = mel_2[:, 1:]\n",
    "    result = (((mel_1 - mel_2) ** 2).sum(axis=1)**0.5).mean()\n",
    "    MCDs_gt.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b366e1b-05e0-4f95-b5e9-f973eecb81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "v6_mcd_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'MCD_recon': MCDs_recon,\n",
    "    'MCD_gt': MCDs_gt,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7ffa407-8dbc-4b40-8327-1909fc21fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "v6_mcd_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\MCD_0703a_1M2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45e09823-7b19-4775-bd0a-451e250455dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCDs_recon mean on 0703a_1M2: 12.918683052062988\n",
      "MCDs_recon std on 0703a_1M2: 2.4684879779815674\n",
      "MCDs_gt mean on 0703a_1M2: 13.95873737335205\n",
      "MCDs_gt std on 0703a_1M2: 2.4867985248565674\n"
     ]
    }
   ],
   "source": [
    "print(f\"MCDs_recon mean on 0703a_1M2: {torch.tensor(MCDs_recon).mean()}\")\n",
    "print(f\"MCDs_recon std on 0703a_1M2: {torch.tensor(MCDs_recon).std()}\")\n",
    "\n",
    "print(f\"MCDs_gt mean on 0703a_1M2: {torch.tensor(MCDs_gt).mean()}\")\n",
    "print(f\"MCDs_gt std on 0703a_1M2: {torch.tensor(MCDs_gt).std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725dd881-0910-4f3a-a702-fd80f9ef0df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbe3a4-68bf-483f-9749-aa0645ee17a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f41f43-5d32-46c9-907d-84761d46792e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6ab3a-8165-4d94-9bf0-79e4cdf22620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "813be1ad-71ed-42e4-9fd4-c199e9b11cf8",
   "metadata": {},
   "source": [
    "# log f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3c7b4a4-2319-4154-b76b-b3fc676788d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/espnet/espnet/blob/3e0dad524d62ccd45e067e9b36049f2214ea972a/egs2/TEMPLATE/asr1/pyscripts/utils/evaluate_f0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae4034fd-a193-41f4-9b42-a8a4952cd8dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def world_extract(\n",
    "    x: np.ndarray,\n",
    "    fs: int,\n",
    "    f0min: int = 40,\n",
    "    f0max: int = 800,\n",
    "    n_fft: int = 512,\n",
    "    n_shift: int = 256,\n",
    "    mcep_dim: int = 25,\n",
    "    mcep_alpha: float = 0.41,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract World-based acoustic features.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): 1D waveform array.\n",
    "        fs (int): Minimum f0 value (default=40).\n",
    "        f0 (int): Maximum f0 value (default=800).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        n_fft (int): FFT length in point (default=512).\n",
    "        n_shift (int): Shift length in point (default=256).\n",
    "        mcep_dim (int): Dimension of mel-cepstrum (default=25).\n",
    "        mcep_alpha (float): All pass filter coefficient (default=0.41).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Mel-cepstrum with the size (N, n_fft).\n",
    "        ndarray: F0 sequence (N,).\n",
    "\n",
    "    \"\"\"\n",
    "    # extract features\n",
    "    x = x.astype(np.float64)\n",
    "    f0, time_axis = pw.harvest(\n",
    "        x,\n",
    "        fs,\n",
    "        f0_floor=f0min,\n",
    "        f0_ceil=f0max,\n",
    "        frame_period=n_shift / fs * 1000,\n",
    "    )\n",
    "    sp = pw.cheaptrick(x, f0, time_axis, fs, fft_size=n_fft)\n",
    "    if mcep_dim is None or mcep_alpha is None:\n",
    "        mcep_dim, mcep_alpha = _get_best_mcep_params(fs)\n",
    "    mcep = pysptk.sp2mc(sp, mcep_dim, mcep_alpha)\n",
    "\n",
    "    return mcep, f0\n",
    "\n",
    "\n",
    "def _get_basename(path: str) -> str:\n",
    "    return os.path.splitext(os.path.split(path)[-1])[0]\n",
    "\n",
    "\n",
    "def _get_best_mcep_params(fs: int) -> Tuple[int, float]:\n",
    "    if fs == 16000:\n",
    "        return 23, 0.42\n",
    "    elif fs == 22050:\n",
    "        return 34, 0.45\n",
    "    elif fs == 24000:\n",
    "        return 34, 0.46\n",
    "    elif fs == 44100:\n",
    "        return 39, 0.53\n",
    "    elif fs == 48000:\n",
    "        return 39, 0.55\n",
    "    else:\n",
    "        raise ValueError(f\"Not found the setting for {fs}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dec73abb-e768-4449-8d1d-06a872deb8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a15f7e77934b0091f6573f649d0faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip uid 342edcf2-1150-42a0-af79-030571c0996d. len == 0.\n",
      "Skip uid 2ff974ac-b0f1-4611-8c9f-61c216a8ba96. len == 0.\n",
      "Skip uid 67ae347b-a4a6-4966-a736-c03a831d1a2a. len == 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2846: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2705: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip uid 959dcb2b-4291-45b4-aa24-dc144de6fc1a. len == 0.\n",
      "Skip uid 94ab3c42-86a9-427c-9dc7-9d3583836f89. len == 0.\n",
      "Skip uid 774fb2b7-15af-4028-bb0f-17725e2cfb7f. len == 0.\n",
      "Skip uid 8e2b989d-7499-4d4c-ac60-be9bebda99f7. len == 0.\n",
      "Skip uid 7444a0db-41ce-4779-a17c-1b5ec92b1a54. len == 0.\n",
      "Skip uid d7c942a6-bfd3-4ff2-9926-88fafe78781b. len == 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "D:\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip uid 9f56d2e9-8536-4ce4-878e-e31f5c64345a. len == 0.\n",
      "Skip uid 0d885f9d-8786-40d2-ab4b-d6874b5c6507. len == 0.\n",
      "Skip uid cff28f93-e826-46a8-b57b-4959b4f485ae. len == 0.\n"
     ]
    }
   ],
   "source": [
    "uids = []\n",
    "logf0_rmses_recon = []\n",
    "logf0_corrs_recon = []\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    \n",
    "    # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(recon_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    fs = gen_fs\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep, gen_f0 = world_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep, gt_f0 = world_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_f0_dtw = gen_f0[twf[0]]\n",
    "    gt_f0_dtw = gt_f0[twf[1]]\n",
    "    \n",
    "    # Get voiced part\n",
    "    nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "    eps = 1e-7\n",
    "    gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs] + eps)\n",
    "    gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs] + eps)\n",
    "\n",
    "    if len(gen_f0_dtw_voiced) == 0 or len(gt_f0_dtw_voiced) == 0:\n",
    "        print(f\"Skip uid {uid}. len == 0.\")\n",
    "        continue\n",
    "\n",
    "    # log F0 RMSE\n",
    "    log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "    # print(f\"{uid} {log_f0_rmse:.4f}\")\n",
    "\n",
    "    # log F0 corr\n",
    "    log_f0_corr = np.corrcoef(gen_f0_dtw_voiced, gt_f0_dtw_voiced)[0][1]\n",
    "    # print(f\"{uid} {log_f0_corr:.4f}\")\n",
    "    uids.append(uid)\n",
    "    logf0_rmses_recon.append(log_f0_rmse)\n",
    "    logf0_corrs_recon.append(log_f0_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf86a571-a6f1-477a-9fa3-47a08caac552",
   "metadata": {},
   "outputs": [],
   "source": [
    "v6_logf0_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    'logf0_rmse_recon': logf0_rmses_recon,\n",
    "    'logf0_corr_recon': logf0_corrs_recon,\n",
    "    # 'logf0_rmse_gt': logf0_rmses_gt,\n",
    "    # 'logf0_corr_gt': logf0_corrs_gt,\n",
    "})\n",
    "v6_logf0_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\logF0_0703a_1M2_recon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3083c44-9b7c-4f35-8e98-3837bbed2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "v6_logf0_df = v6_logf0_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00b992e8-36b2-4a0f-8b5d-a357984b96ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logf0_rmse_recon mean on 0703a_1M2: 0.27646684570654634\n",
      "logf0_rmse_recon std on 0703a_1M2: 0.20457826946634558\n",
      "logf0_corr_recon mean on 0703a_1M2: 0.4154188758649175\n",
      "logf0_corr_recon std on 0703a_1M2: 0.3870411664572518\n"
     ]
    }
   ],
   "source": [
    "print(f\"logf0_rmse_recon mean on 0703a_1M2: {v6_logf0_df['logf0_rmse_recon'].values.mean()}\")\n",
    "print(f\"logf0_rmse_recon std on 0703a_1M2: {v6_logf0_df['logf0_rmse_recon'].values.std()}\")\n",
    "print(f\"logf0_corr_recon mean on 0703a_1M2: {v6_logf0_df['logf0_corr_recon'].values.mean()}\")\n",
    "print(f\"logf0_corr_recon std on 0703a_1M2: {v6_logf0_df['logf0_corr_recon'].values.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df076e12-eec3-400c-a89f-abc87d6561aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0649965e10d84cc7b4100f872f614322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip uid f3a4e109-69d8-4641-995c-2da5df7a46e5. len == 0.\n",
      "Skip uid 7d6a297a-7137-468c-a332-000920392f49. len == 0.\n",
      "Skip uid 2ff974ac-b0f1-4611-8c9f-61c216a8ba96. len == 0.\n",
      "Skip uid 67ae347b-a4a6-4966-a736-c03a831d1a2a. len == 0.\n",
      "Skip uid 5ce954ac-b56b-4ef6-876c-81309d5c0db5. len == 0.\n",
      "Skip uid 8e2b989d-7499-4d4c-ac60-be9bebda99f7. len == 0.\n",
      "Skip uid 61a3c142-ed76-4a8d-afec-4e2c3bdf54a6. len == 0.\n",
      "Skip uid c40778c5-b585-4897-94ff-6a35edf19add. len == 0.\n",
      "Skip uid d7c942a6-bfd3-4ff2-9926-88fafe78781b. len == 0.\n",
      "Skip uid b13f3ab6-fb65-4ae2-9929-4f06d0d58951. len == 0.\n",
      "Skip uid 0ba7caf8-59b7-4cec-9560-caa6f0844e3e. len == 0.\n",
      "Skip uid cff28f93-e826-46a8-b57b-4959b4f485ae. len == 0.\n"
     ]
    }
   ],
   "source": [
    "uids = []\n",
    "logf0_rmses_gt = []\n",
    "logf0_corrs_gt = []\n",
    "\n",
    "for uid in tqdm(val_uids):\n",
    "    \n",
    "    # load wav file as int16\n",
    "    gen_x, gen_fs = sf.read(op.join(syn_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    gt_x, gt_fs = sf.read(op.join(gt_folder, f\"{uid}.wav\"), dtype=\"int16\")\n",
    "    fs = gen_fs\n",
    "    # extract ground truth and converted features\n",
    "    gen_mcep, gen_f0 = world_extract(\n",
    "        x=gen_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    gt_mcep, gt_f0 = world_extract(\n",
    "        x=gt_x,\n",
    "        fs=fs,\n",
    "        f0min=40,\n",
    "        f0max=800,\n",
    "        n_fft=1024,\n",
    "        n_shift=256,\n",
    "        mcep_dim=None,\n",
    "        mcep_alpha=None,\n",
    "    )\n",
    "    \n",
    "    # DTW\n",
    "    _, path = fastdtw(gen_mcep, gt_mcep, dist=spatial.distance.euclidean)\n",
    "    twf = np.array(path).T\n",
    "    gen_f0_dtw = gen_f0[twf[0]]\n",
    "    gt_f0_dtw = gt_f0[twf[1]]\n",
    "    \n",
    "    # Get voiced part\n",
    "    nonzero_idxs = np.where((gen_f0_dtw != 0) & (gt_f0_dtw != 0))[0]\n",
    "    eps = 1e-7\n",
    "    gen_f0_dtw_voiced = np.log(gen_f0_dtw[nonzero_idxs] + eps)\n",
    "    gt_f0_dtw_voiced = np.log(gt_f0_dtw[nonzero_idxs] + eps)\n",
    "\n",
    "    if len(gen_f0_dtw_voiced) == 0 or len(gt_f0_dtw_voiced) == 0:\n",
    "        print(f\"Skip uid {uid}. len == 0.\")\n",
    "        continue\n",
    "\n",
    "    # log F0 RMSE\n",
    "    log_f0_rmse = np.sqrt(np.mean((gen_f0_dtw_voiced - gt_f0_dtw_voiced) ** 2))\n",
    "    # print(f\"{uid} {log_f0_rmse:.4f}\")\n",
    "\n",
    "    # log F0 corr\n",
    "    log_f0_corr = np.corrcoef(gen_f0_dtw_voiced, gt_f0_dtw_voiced)[0][1]\n",
    "    # print(f\"{uid} {log_f0_corr:.4f}\")\n",
    "    uids.append(uid)\n",
    "    logf0_rmses_gt.append(log_f0_rmse)\n",
    "    logf0_corrs_gt.append(log_f0_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6871128e-9f7a-443b-bb7d-da3e0a67ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v6_logf0_df = pd.DataFrame({\n",
    "    'uid': uids,\n",
    "    # 'logf0_rmse_recon': logf0_rmses_recon,\n",
    "    # 'logf0_corr_recon': logf0_corrs_recon,\n",
    "    'logf0_rmse_gt': logf0_rmses_gt,\n",
    "    'logf0_corr_gt': logf0_corrs_gt,\n",
    "})\n",
    "v6_logf0_df.to_csv(r\".\\jupyter_walkthrough\\metrics\\logF0_0703a_1M2_gt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f97c53e2-e889-4487-9d9e-81e638d986bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logf0_rmse_recon mean on 0703a_1M2: 0.27740567782392284\n",
      "logf0_rmse_recon std on 0703a_1M2: 0.20721483481131892\n",
      "logf0_corr_recon mean on 0703a_1M2: 0.4154188758649175\n",
      "logf0_corr_recon std on 0703a_1M2: 0.3870411664572518\n",
      "logf0_rmse_gt mean on 0703a_1M2: 0.31706488680123224\n",
      "logf0_rmse_gt std on 0703a_1M2: 0.23586469383724842\n",
      "logf0_corr_gt mean on 0703a_1M2: 0.33125619636668713\n",
      "logf0_corr_gt std on 0703a_1M2: 0.39742992919195425\n"
     ]
    }
   ],
   "source": [
    "logf0_rmses_recon_array = np.array(logf0_rmses_recon)[~np.isnan(np.array(logf0_rmses_recon))]\n",
    "logf0_corrs_recon_array = np.array(logf0_corrs_recon)[~np.isnan(np.array(logf0_corrs_recon))]\n",
    "\n",
    "print(f\"logf0_rmse_recon mean on 0703a_1M2: {logf0_rmses_recon_array.mean()}\")\n",
    "print(f\"logf0_rmse_recon std on 0703a_1M2: {logf0_rmses_recon_array.std()}\")\n",
    "print(f\"logf0_corr_recon mean on 0703a_1M2: {logf0_corrs_recon_array.mean()}\")\n",
    "print(f\"logf0_corr_recon std on 0703a_1M2: {logf0_corrs_recon_array.std()}\")\n",
    "\n",
    "logf0_rmses_gt_array = np.array(logf0_rmses_gt)[~np.isnan(np.array(logf0_rmses_gt))]\n",
    "logf0_corrs_gt_array = np.array(logf0_corrs_gt)[~np.isnan(np.array(logf0_corrs_gt))]\n",
    "\n",
    "print(f\"logf0_rmse_gt mean on 0703a_1M2: {logf0_rmses_gt_array.mean()}\")\n",
    "print(f\"logf0_rmse_gt std on 0703a_1M2: {logf0_rmses_gt_array.std()}\")\n",
    "print(f\"logf0_corr_gt mean on 0703a_1M2: {logf0_corrs_gt_array.mean()}\")\n",
    "print(f\"logf0_corr_gt std on 0703a_1M2: {logf0_corrs_gt_array.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c45320-2f0b-4d02-b9e8-8f27774d1022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456f5e2-c25c-400c-9903-ac2fa286850d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab3f61-015a-4991-b106-12abcc9d4f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c9427-67a0-43e8-a700-dfa001b1fe07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e56ef5-6166-404d-a0f7-0e4a394809be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4003d63-0dae-4aac-8a00-d36d18a3fd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
